from typing import Dict, List, Tuple, Optional

from idea2paper.config import PipelineConfig
from idea2paper.infra.llm import call_llm, parse_json_from_llm


class PatternSelector:
    """Pattern é€‰æ‹©å™¨: é€‰æ‹©å¤šæ ·åŒ–çš„ Patternï¼ˆæ”¯æŒ LLM è¾…åŠ©åˆ†ç±»å’ŒåŠ¨æ€æ’åºï¼‰"""

    def __init__(self, recalled_patterns: List[Tuple[str, Dict, float]], user_idea: str = ""):
        """
        Args:
            recalled_patterns: [(pattern_id, pattern_info, score), ...]
            user_idea: ç”¨æˆ·çš„åŸå§‹ Ideaï¼ˆç”¨äº LLM åˆ¤æ–­é¢†åŸŸè·ç¦»ï¼‰
        """
        self.recalled_patterns = recalled_patterns
        self.user_idea = user_idea
        self.pattern_classifications = {}  # å­˜å‚¨ LLM åˆ†ç±»ç»“æœ

    def select(self) -> Dict[str, List[Tuple[str, Dict, Dict]]]:
        """é€‰æ‹©å¤šä¸ª Pattern å¹¶æŒ‰ä¸‰ä¸ªç»´åº¦ï¼ˆç¨³å¥åº¦ã€æ–°é¢–åº¦ã€è·¨åŸŸåº¦ï¼‰åˆ†åˆ«æ’åº

        Returns:
            {
                'stability': [(pattern_id, pattern_info, metadata), ...],   # æŒ‰ç¨³å®šæ€§(stability_score)é™åº
                'novelty': [(pattern_id, pattern_info, metadata), ...],     # æŒ‰æ–°é¢–æ€§(novelty_score)é™åº
                'domain_distance': [(pattern_id, pattern_info, metadata), ...]  # æŒ‰é¢†åŸŸè·ç¦»(domain_distance)å‡åº (è¶Šå°è¶Šå¥½)
            }
        """
        print("\n" + "=" * 80)
        print("ğŸ“‹ Phase 1: Pattern Selection (å¤šç»´åº¦è¯„åˆ†ä¸æ’åº)")
        print("=" * 80)

        # Step 1: ä¸ºæ‰€æœ‰ Pattern è®¡ç®—ä¸‰ä¸ªç»´åº¦çš„å¾—åˆ†
        print("\nğŸ¤– Step 1: å¤šç»´åº¦è¯„åˆ† (æ‰€æœ‰ Patterns)...")
        self._score_patterns_multidimensional()

        # Step 2: æŒ‰ä¸‰ä¸ªç»´åº¦åˆ†åˆ«æ’åº
        print("\nğŸ“Š Step 2: æŒ‰ç»´åº¦æ’åº...")
        ranked = self._rank_patterns_by_dimensions()

        # Step 3: æ‰“å°æ’åºç»“æœ
        print("\n" + "=" * 80)
        print("âœ… Pattern å¤šç»´åº¦æ’åºç»“æœ:")
        print("=" * 80)

        dimension_names = {
            'stability': 'ã€ç¨³å¥åº¦æ’åºã€‘',
            'novelty': 'ã€æ–°é¢–åº¦æ’åºã€‘',
            'domain_distance': 'ã€è·¨åŸŸåº¦æ’åº (ä½â†’é«˜è·ç¦»)ã€‘'
        }

        for dimension, patterns in ranked.items():
            if not patterns:
                continue

            print(f"\n{dimension_names.get(dimension, dimension)} å…± {len(patterns)} ä¸ª:")
            for i, (pid, pinfo, meta) in enumerate(patterns[:5], 1):  # æ˜¾ç¤ºå‰5ä¸ª
                print(f"  {i}. {pid}")
                print(f"     åç§°: {pinfo.get('name', 'N/A')}")
                print(f"     èšç±»: {pinfo.get('size', 0)} ç¯‡")
                if 'scores' in meta:
                    scores = meta['scores']
                    print(f"     å¾—åˆ†: ç¨³å¥={scores.get('stability_score', 0):.2f}, "
                          f"æ–°é¢–={scores.get('novelty_score', 0):.2f}, "
                          f"åŸŸè·={scores.get('domain_distance', 0):.2f}")

        print("\n" + "-" * 80)
        total = sum(len(patterns) for patterns in ranked.values())
        print(f"âœ… å…±è¯„åˆ† {total} ä¸ª Pattern")
        print("=" * 80)

        return ranked

    def _score_patterns_multidimensional(self):
        """ä¸ºæ‰€æœ‰ Pattern è®¡ç®—ä¸‰ä¸ªç»´åº¦çš„å¾—åˆ†ï¼ˆç¨³å¥åº¦ã€æ–°é¢–åº¦ã€è·¨åŸŸåº¦ï¼‰"""
        # å¯¹ Top-20 è¿›è¡Œ LLM è¯„åˆ†ï¼ˆå¹³è¡¡æ•ˆæœå’Œæˆæœ¬ï¼‰
        top_patterns = self.recalled_patterns[:20]

        for pattern_id, pattern_info, recall_score in top_patterns:
            # æ„å»º Pattern æ‘˜è¦ä¿¡æ¯
            pattern_name = pattern_info.get('name', '')
            pattern_size = pattern_info.get('size', 0)

            summary = pattern_info.get('summary', {})
            if isinstance(summary, dict):
                representative_ideas = summary.get('representative_ideas', [])[:3]
                common_problems = summary.get('common_problems', [])[:2]
            else:
                representative_ideas = []
                common_problems = []

            # è°ƒç”¨ LLM è¿›è¡Œå¤šç»´åº¦è¯„åˆ†
            scores = self._call_llm_for_multidim_scoring(
                pattern_id, pattern_name, pattern_size,
                representative_ideas, common_problems
            )

            if scores:
                self.pattern_classifications[pattern_id] = scores
                print(f"  âœ“ {pattern_id}: ç¨³å¥={scores.get('stability_score', 0):.2f}, "
                      f"æ–°é¢–={scores.get('novelty_score', 0):.2f}, "
                      f"åŸŸè·={scores.get('domain_distance', 0):.2f}")

    def _generate_reference_examples(self, current_pattern_id: str) -> str:
        """ç”Ÿæˆå‚è€ƒç¤ºä¾‹æ¥æ ¡å‡† LLM è¯„åˆ†ï¼ŒåŸºäºå·²è¯„åˆ†çš„ Pattern"""
        # å¦‚æœå·²æœ‰è¯„åˆ†ï¼Œä½¿ç”¨å®ƒä»¬ä½œä¸ºå‚è€ƒï¼›å¦åˆ™ç”Ÿæˆäººå·¥ç¤ºä¾‹
        if not self.pattern_classifications:
            # æ²¡æœ‰å·²è¯„åˆ†çš„Patternï¼Œä½¿ç”¨äººå·¥ç¤ºä¾‹
            return """
Example 1 - LOW novelty, HIGH stability (Size 150):
  "Attention Is All You Need" application - highly replicated, but well-known approach
  â†’ stability_score: 0.85, novelty_score: 0.15

Example 2 - HIGH novelty, MEDIUM stability (Size 25):
  "Reframing task as code generation problem" - novel angle, but niche community
  â†’ stability_score: 0.35, novelty_score: 0.75

Example 3 - MEDIUM novelty, MEDIUM stability (Size 60):
  "Combining RAG with multi-hop reasoning" - interesting combination, growing adoption
  â†’ stability_score: 0.60, novelty_score: 0.55
"""
        else:
            # ä»å·²è¯„åˆ†ä¸­æå–å‡ ä¸ªä»£è¡¨æ€§æ ·æœ¬
            samples = []
            for pid, scores in list(self.pattern_classifications.items())[:3]:
                pinfo = next((p[1] for p in self.recalled_patterns if p[0] == pid), {})
                samples.append(f"  {pid} (Size {pinfo.get('size', '?')}): "
                             f"stability={scores.get('stability_score', 0):.2f}, "
                             f"novelty={scores.get('novelty_score', 0):.2f}")
            return "Recent scoring calibration:\n" + "\n".join(samples) if samples else ""

    def _call_llm_for_multidim_scoring(self, pattern_id: str, pattern_name: str,
                                       pattern_size: int, ideas: List[str],
                                       problems: List[str]) -> Optional[Dict]:
        """è°ƒç”¨ LLM ä¸ºå•ä¸ª Pattern è®¡ç®—ä¸‰ä¸ªç»´åº¦çš„å¾—åˆ†ï¼ˆç¨³å¥åº¦ã€æ–°é¢–åº¦ã€è·¨åŸŸåº¦ï¼‰"""

        ideas_text = "\n".join(f"- {idea}" for idea in ideas[:3])

        # ç”Ÿæˆä¸€äº›å¯¹æ¯”å‚è€ƒï¼ˆä»å·²è¯„åˆ†çš„ Pattern ä¸­æŠ½å–ï¼‰
        reference_examples = self._generate_reference_examples(pattern_id)

        prompt = f"""
You are a **CRITICAL Multidimensional Pattern Scorer** for top-tier AI conferences (ICLR/NeurIPS).
Your task is to rigorously evaluate a research pattern across THREE independent dimensions.
âš ï¸  IMPORTANT: Avoid clustering scores in the middle range. Be discriminative!

ã€User's Research Ideaã€‘
"{self.user_idea}"

ã€Pattern Informationã€‘
Pattern ID: {pattern_id}
Name: {pattern_name}
Cluster Size: {pattern_size} papers
Representative Research Ideas:
{ideas_text if ideas_text else "N/A"}

ã€Reference Examples (for calibration)ã€‘
{reference_examples}

ã€Scoring Guidelines - Be CRITICAL and DISCRIMINATIVEã€‘

**Stability Score (0.0-1.0)** - How proven, mature, and widely-adopted?
Consider: Has this approach been replicated across many papers? Are there established benchmarks?
- 0.1-0.25: Highly experimental, niche idea, Size < 15, no standard benchmarks, high uncertainty
- 0.3-0.45: Early-stage research, Size 15-40, some implementations but inconsistent results
- 0.5-0.65: Maturing approach, Size 40-70, multiple independent implementations, emerging consensus
- 0.7-0.85: Well-established, Size 70-120, standard benchmarks, widely replicated with consistent gains
- 0.9-1.0: Foundational/canonical approach, Size > 120, ubiquitous, considered solved or foundational
ğŸ”´ RED FLAG: Avoid giving middle scores (0.4-0.6) to everything. Distinguish clearly.

**Novelty Score (0.0-1.0)** - How original, counter-intuitive, and fresh is this?
Consider: Is this a new perspective? Does it challenge existing assumptions? Or incremental variation?
- 0.1-0.25: Well-trodden path, combinations of existing techniques, straightforward application
- 0.3-0.45: Some novelty in execution or application domain, but builds on established ideas
- 0.5-0.65: Interesting recombination or new angle on known problems, moderate originality
- 0.7-0.85: Novel methodology, surprising insight, challenges conventional wisdom, fresh angle
- 0.9-1.0: Paradigm shift, highly counter-intuitive, fundamentally new problem formulation
ğŸ”´ RED FLAG: If pattern_name suggests "reframing" or "transforming", likely 0.6+. If it's optimization/tuning, likely 0.2-0.4.

**Domain Distance (0.0-1.0)** - How different from user's core idea?
Consider semantic and methodological distance, not just application domain.
- 0.0-0.15: Directly addresses same problem, highly relevant methodology
- 0.2-0.35: Related domain/approach, applicable with minor adaptation
- 0.4-0.55: Different domain but transferable insights, moderate adaptation needed
- 0.6-0.8: Orthogonal domain, interesting cross-domain inspirations
- 0.85-1.0: Completely different field, minimal direct relevance
ğŸ’¡ TIP: Compare pattern semantics to user idea content for distance.

ã€CRITICAL INSTRUCTIONSã€‘
1. DO NOT give all patterns middle-range scores (0.4-0.6). Spread the distribution.
2. DISTINGUISH between: optimization (low novelty), new methodology (medium), paradigm shift (high).
3. Large cluster size (>100) should NOT automatically mean high stability if methodology is flawed.
4. Small cluster size (<20) should NOT automatically mean low novelty; niche innovation exists.

ã€Output Format - JSON ONLYã€‘
{{
  "stability_score": 0.75,
  "novelty_score": 0.55,
  "domain_distance": 0.25,
  "reasoning": "Example: Established approach (Size 82), novel reframing angle, same-domain application"
}}
"""

        try:
            # ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´ï¼ˆ180 ç§’ï¼‰ä»¥åº”å¯¹ç½‘ç»œè¾ƒæ…¢çš„æƒ…å†µ
            response = call_llm(prompt, temperature=0.3, max_tokens=300, timeout=180)
            scores = parse_json_from_llm(response)
            if scores and all(k in scores for k in ['stability_score', 'novelty_score', 'domain_distance']):
                return scores
        except Exception as e:
            print(f"  âš ï¸  LLM è¯„åˆ†å¤±è´¥ ({pattern_id}): {e}")

        # Fallback: ä½¿ç”¨è§„åˆ™è®¡ç®—
        return self._fallback_multidim_scoring(pattern_size)

    def _rank_patterns_by_dimensions(self) -> Dict[str, List[Tuple[str, Dict, Dict]]]:
        """æŒ‰ä¸‰ä¸ªç»´åº¦ï¼ˆç¨³å¥åº¦ã€æ–°é¢–åº¦ã€è·¨åŸŸåº¦ï¼‰åˆ†åˆ«æ’åºæ‰€æœ‰ Pattern"""
        ranked = {
            'stability': [],       # æŒ‰ stability_score é™åº
            'novelty': [],         # æŒ‰ novelty_score é™åº
            'domain_distance': []  # æŒ‰ domain_distance å‡åºï¼ˆè¶Šå°è¶Šå¥½ï¼‰
        }

        for pattern_id, pattern_info, recall_score in self.recalled_patterns:
            # è·å– LLM è¯„åˆ†ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
            scores = self.pattern_classifications.get(pattern_id, None)

            # å¦‚æœ LLM è¯„åˆ†æˆåŠŸï¼Œä½¿ç”¨ LLM çš„è¯„åˆ†
            if scores:
                metadata = {
                    'recall_score': recall_score,
                    'scores': scores
                }
            else:
                # Fallback: ä½¿ç”¨è§„åˆ™è®¡ç®—ï¼ˆå…¼å®¹æ€§ï¼‰
                scores = self._fallback_multidim_scoring(pattern_info.get('size', 0))
                metadata = {
                    'recall_score': recall_score,
                    'scores': scores
                }

            # å°†è¯¥ pattern æ·»åŠ åˆ°æ‰€æœ‰ä¸‰ä¸ªç»´åº¦çš„åˆ—è¡¨ä¸­
            ranked['stability'].append((pattern_id, pattern_info, metadata))
            ranked['novelty'].append((pattern_id, pattern_info, metadata))
            ranked['domain_distance'].append((pattern_id, pattern_info, metadata))

        # æŒ‰ä¸‰ä¸ªç»´åº¦åˆ†åˆ«æ’åº
        ranked['stability'].sort(
            key=lambda x: x[2].get('scores', {}).get('stability_score', 0),
            reverse=True
        )
        ranked['novelty'].sort(
            key=lambda x: x[2].get('scores', {}).get('novelty_score', 0),
            reverse=True
        )
        # domain_distance è¶Šå°è¶Šå¥½ï¼ˆè¶Šæ¥è¿‘ç”¨æˆ·æƒ³æ³•ï¼‰ï¼Œæ‰€ä»¥å‡åº
        ranked['domain_distance'].sort(
            key=lambda x: x[2].get('scores', {}).get('domain_distance', 1.0),
            reverse=False
        )

        return ranked

    def _fallback_multidim_scoring(self, pattern_size: int) -> Dict:
        """Fallback è§„åˆ™è®¡ç®—ï¼ˆå½“ LLM è¯„åˆ†å¤±è´¥æ—¶ï¼‰

        åŸºäº pattern_size çš„å¤šç»´åº¦å¯å‘å¼è¯„åˆ†
        """
        # ã€è®¾è®¡æ€è·¯ã€‘æ ¹æ® size ä¼°è®¡ä¸‰ä¸ªç»´åº¦çš„å¾—åˆ†
        # Size è¶Šå¤§ â†’ Stability è¶Šé«˜
        # Size è¶Šå° â†’ Novelty è¶Šé«˜ï¼ˆå°ä¼—æ–¹å‘å¾€å¾€æ›´æ–°é¢–ï¼‰
        # Domain Distance éœ€è¦æ›´å¤šä¿¡æ¯æ‰èƒ½åˆ¤æ–­ï¼Œè¿™é‡Œè®¾ä¸ºä¸­ç­‰å€¼

        if pattern_size > 100:
            # å¤§å‹æˆç†Ÿç¤¾åŒº
            stability = 0.85
            novelty = 0.25
            domain_dist = 0.25
        elif pattern_size > 70:
            # æˆç†Ÿç¤¾åŒº
            stability = 0.80
            novelty = 0.30
            domain_dist = 0.30
        elif pattern_size > 40:
            # ä¸­ç­‰ç¤¾åŒº
            stability = 0.65
            novelty = 0.45
            domain_dist = 0.40
        elif pattern_size > 20:
            # å°å‹ä½†æœ‰ä¸€å®šåŸºç¡€
            stability = 0.50
            novelty = 0.60
            domain_dist = 0.35
        else:
            # éå¸¸å°çš„ç¤¾åŒºï¼Œé«˜åˆ›æ–°æ€§
            stability = 0.30
            novelty = 0.80
            domain_dist = 0.50

        return {
            'stability_score': stability,
            'novelty_score': novelty,
            'domain_distance': domain_dist,
            'reasoning': f'Rule-based: size={pattern_size}'
        }

    # ä¿ç•™æ—§æ–¹æ³•ä»¥å…¼å®¹æ—§ä»£ç ï¼ˆæ ‡è®°ä¸º deprecatedï¼‰
    def _select_conservative(self) -> Optional[Tuple[str, Dict]]:
        """ã€å·²å¼ƒç”¨ã€‘é€‰æ‹©ç¨³å¥å‹: Score æœ€é«˜"""
        if not self.recalled_patterns:
            return None

        # å·²ç»æŒ‰åˆ†æ•°æ’åºï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ª
        pattern_id, pattern_info, score = self.recalled_patterns[0]
        return (pattern_id, pattern_info)

    def _select_innovative(self, exclude: List[str]) -> Optional[Tuple[str, Dict]]:
        """é€‰æ‹©åˆ›æ–°å‹: Cluster Size æœ€å°"""
        candidates = [
            (pid, pinfo, score)
            for pid, pinfo, score in self.recalled_patterns
            if pid not in exclude and
               pinfo.get('size', 999) < PipelineConfig.INNOVATIVE_CLUSTER_SIZE_THRESHOLD
        ]

        if not candidates:
            # å¦‚æœæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„ï¼Œé€‰æ‹© Cluster Size æœ€å°çš„
            candidates = [
                (pid, pinfo, score)
                for pid, pinfo, score in self.recalled_patterns
                if pid not in exclude
            ]
            candidates.sort(key=lambda x: x[1].get('size', 999))

        if candidates:
            return (candidates[0][0], candidates[0][1])
        return None

    def _select_cross_domain(self, exclude: List[str]) -> Optional[Tuple[str, Dict]]:
        """é€‰æ‹©è·¨åŸŸå‹: ä»å‰©ä½™çš„ä¸­é€‰æ‹©"""
        candidates = [
            (pid, pinfo, score)
            for pid, pinfo, score in self.recalled_patterns
            if pid not in exclude
        ]

        if candidates:
            # é€‰æ‹©å¾—åˆ†ç¬¬äºŒé«˜çš„ï¼ˆä¸åŒäº conservativeï¼‰
            return (candidates[0][0], candidates[0][1])
        return None
