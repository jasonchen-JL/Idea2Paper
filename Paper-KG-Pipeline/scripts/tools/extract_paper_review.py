"""
è®ºæ–‡ä¸å®¡ç¨¿æ„è§ç»¼åˆæŠ½å–è„šæœ¬
æ•´åˆ paper å’Œ review æ•°æ®ï¼Œè¾“å‡ºé€‚åˆçŸ¥è¯†å›¾è°±çš„èŠ‚ç‚¹æ ¼å¼
"""

import json
import re
import os
import requests
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict, field
from datetime import datetime


# ============================================================
# é…ç½® - è¯·é…ç½®ç¯å¢ƒå˜é‡æˆ–ä¿®æ”¹æ­¤å¤„
# ============================================================
API_URL = os.environ.get("LLM_API_URL", "https://api.openai.com/v1/chat/completions")
AUTH_TOKEN = os.environ.get("LLM_AUTH_TOKEN", "")
MODEL = os.environ.get("LLM_MODEL", "gpt-4")

if not AUTH_TOKEN:
    print("âš ï¸  è­¦å‘Š: æœªè®¾ç½® LLM_AUTH_TOKEN ç¯å¢ƒå˜é‡ï¼ŒæŠ½å–åŠŸèƒ½å°†ä¸å¯ç”¨")
    print("   è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export LLM_AUTH_TOKEN='Bearer your_token_here'")


# ============================================================
# æ•°æ®ç»“æ„ - è®ºæ–‡èŠ‚ç‚¹
# ============================================================
@dataclass
class PaperSections:
    """è®ºæ–‡åˆ†æ®µç»“æœ"""
    title: str
    abstract: str
    keywords: List[str]
    introduction: str
    related_work: str
    method: str
    experiments: str
    conclusion: str


@dataclass
class ReviewNode:
    """å•æ¡å®¡ç¨¿çŸ¥è¯†å›¾è°±èŠ‚ç‚¹"""
    review_id: str
    paper_id: str
    reviewer: Optional[str]
    paper_summary: str
    strengths: str
    weaknesses: str
    comments: str
    overall_score: str
    confidence: str


@dataclass
class PaperNode:
    """è®ºæ–‡çŸ¥è¯†å›¾è°±èŠ‚ç‚¹"""
    # åŸºç¡€ä¿¡æ¯
    paper_id: str
    title: str
    conference: str
    
    # æŠ½å–çš„å››ç±»ä¿¡æ¯
    domain: Dict
    ideal: Dict
    skeleton: Dict
    tricks: List[Dict]


# ============================================================
# JSON è§£æ
# ============================================================
def parse_json_paper(json_path: str) -> PaperSections:
    """ä» JSON æ–‡ä»¶è§£æè®ºæ–‡ç« èŠ‚"""
    
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    metadata = data.get('metadata', {})
    title = metadata.get('title', '')
    sections_data = metadata.get('sections', [])
    
    abstract = ""
    keywords = []
    introduction = ""
    related_work = ""
    method = ""
    experiments = ""
    conclusion = ""
    
    for section in sections_data:
        heading = section.get('heading', '').lower()
        text = section.get('text', '')
        
        if not text:
            continue
        
        if 'abstract' in heading:
            abstract = text
        elif 'introduction' in heading or heading.startswith('1 '):
            introduction += text + " "
        elif any(kw in heading for kw in ['related', 'background', 'preliminar']):
            related_work += text + " "
        elif any(kw in heading for kw in ['method', 'approach', 'model', 'proposed', 'framework']):
            method += text + " "
        elif any(kw in heading for kw in ['experiment', 'evaluation', 'result', 'empirical']):
            experiments += text + " "
        elif any(kw in heading for kw in ['conclusion', 'discussion', 'future']):
            conclusion += text + " "
    
    if not abstract and sections_data:
        first_text = sections_data[0].get('text', '')
        if len(first_text) < 2000:
            abstract = first_text
    
    # æ¸…ç†æ–‡æœ¬ä½†ä¸æˆªæ–­
    def clean_text(text):
        text = text.strip()
        text = re.sub(r'\s+', ' ', text)
        return text
    
    return PaperSections(
        title=title,
        abstract=clean_text(abstract),
        keywords=keywords,
        introduction=clean_text(introduction),
        related_work=clean_text(related_work),
        method=clean_text(method),
        experiments=clean_text(experiments),
        conclusion=clean_text(conclusion)
    )


def parse_reviews(review_path: str, paper_id: str) -> List[ReviewNode]:
    """ä» JSON æ–‡ä»¶è§£æå®¡ç¨¿æ„è§ï¼Œè½¬ä¸º ReviewNode"""
    
    if not os.path.exists(review_path):
        return []
    
    with open(review_path, 'r', encoding='utf-8') as f:
        reviews_data = json.load(f)
    
    reviews = []
    for idx, review in enumerate(reviews_data):
        report = review.get('report', {})
        scores = review.get('scores', {})
        meta = review.get('meta', {})
        
        rid = review.get('rid', '')[:16] if review.get('rid') else f"{paper_id}_review_{idx}"
        
        reviews.append(ReviewNode(
            review_id=rid,
            paper_id=paper_id,
            reviewer=review.get('reviewer'),
            paper_summary=report.get('paper_summary', ''),
            strengths=report.get('summary_of_strengths', ''),
            weaknesses=report.get('summary_of_weaknesses', ''),
            comments=report.get('comments,_suggestions_and_typos', ''),
            overall_score=str(scores.get('overall', '')),
            confidence=str(meta.get('confidence', ''))
        ))
    
    return reviews


def parse_metadata(metadata_path: str) -> Dict:
    """è§£æå…ƒæ•°æ®æ–‡ä»¶"""
    if not os.path.exists(metadata_path):
        return {}
    
    with open(metadata_path, 'r', encoding='utf-8') as f:
        return json.load(f)


# ============================================================
# LLM è°ƒç”¨
# ============================================================
def call_llm(prompt: str, system_prompt: str = "", temperature: float = 0.3, max_retries: int = 3) -> str:
    """è°ƒç”¨ LLM API"""
    headers = {
        "Authorization": AUTH_TOKEN,
        "Content-Type": "application/json"
    }
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})
    
    payload = {
        "model": MODEL,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": 2000
    }
    
    for attempt in range(max_retries):
        try:
            print(f"  [API] æ­£åœ¨è°ƒç”¨ LLM... (å°è¯• {attempt + 1}/{max_retries})")
            response = requests.post(API_URL, headers=headers, json=payload, timeout=120)
            response.raise_for_status()
            result = response.json()
            return result['choices'][0]['message']['content'].strip()
        except Exception as e:
            print(f"  [API] è°ƒç”¨å¤±è´¥: {e}")
            if attempt < max_retries - 1:
                import time
                time.sleep(3)
            else:
                raise
    return ""


def parse_json_response(response: str) -> Dict:
    """è§£æ LLM è¿”å›çš„ JSON"""
    json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response, re.DOTALL)
    if json_match:
        json_str = json_match.group(1)
    else:
        json_str = response
    
    json_str = json_str.strip()
    
    try:
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        print(f"  [è§£æ] JSON è§£æå¤±è´¥: {e}")
        return {}


# ============================================================
# Paper æŠ½å–ä»»åŠ¡
# ============================================================
def extract_domain(title: str, keywords: List[str], abstract: str) -> Dict:
    """ä»»åŠ¡ 1ï¼šæŠ½å– domain"""
    print("\nğŸ“Œ ä»»åŠ¡ 1ï¼šæŠ½å– domain")
    
    prompt = f"""ã€è¾“å…¥ã€‘
- è®ºæ–‡æ ‡é¢˜: {title}
- å…³é”®è¯: {', '.join(keywords) if keywords else 'æ— '}
- æ‘˜è¦: {abstract}

ã€ä»»åŠ¡ã€‘
è¯·ä»ä»¥ä¸‹ä¸‰ä¸ªç»´åº¦åˆ†æè¿™ç¯‡è®ºæ–‡çš„ç ”ç©¶é¢†åŸŸï¼š

1. ç ”ç©¶å¯¹è±¡ï¼šè¿™ç¯‡è®ºæ–‡ä¸»è¦ç ”ç©¶ä»€ä¹ˆç±»å‹çš„æ•°æ®æˆ–é—®é¢˜ï¼Ÿï¼ˆå¦‚å›¾åƒã€æ–‡æœ¬ã€å›¾ç»“æ„ã€æ—¶åºæ•°æ®ã€å¤šæ¨¡æ€ç­‰ï¼‰
2. æ ¸å¿ƒæŠ€æœ¯ï¼šè®ºæ–‡ä½¿ç”¨æˆ–æ”¹è¿›äº†ä»€ä¹ˆç±»å‹çš„æŠ€æœ¯æ–¹æ³•ï¼Ÿï¼ˆå¦‚ Transformerã€æ‰©æ•£æ¨¡å‹ã€å¼ºåŒ–å­¦ä¹ ã€å›¾ç¥ç»ç½‘ç»œç­‰ï¼‰
3. åº”ç”¨åœºæ™¯ï¼šè®ºæ–‡çš„æˆæœå¯ä»¥åº”ç”¨äºä»€ä¹ˆå®é™…åœºæ™¯ï¼Ÿï¼ˆå¦‚æœºå™¨ç¿»è¯‘ã€ç›®æ ‡æ£€æµ‹ã€æ¨èç³»ç»Ÿã€å¯¹è¯ç³»ç»Ÿç­‰ï¼‰

åŸºäºä»¥ä¸Šåˆ†æï¼Œå½’çº³å‡ºè¿™ç¯‡è®ºæ–‡æ‰€å±çš„ç ”ç©¶é¢†åŸŸï¼ˆå¯ä»¥æœ‰å¤šä¸ªï¼‰ã€‚

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·ä¸¥æ ¼è¾“å‡º JSON æ ¼å¼ï¼š
{{
  "research_object": "ç ”ç©¶å¯¹è±¡æè¿°",
  "core_technique": "æ ¸å¿ƒæŠ€æœ¯æè¿°", 
  "application": "åº”ç”¨åœºæ™¯æè¿°",
  "domains": ["é¢†åŸŸ1", "é¢†åŸŸ2"]
}}"""

    system_prompt = "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å­¦æœ¯è®ºæ–‡åˆ†æä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„æ ¼å¼è¾“å‡º JSONã€‚"
    response = call_llm(prompt, system_prompt)
    result = parse_json_response(response)
    print(f"  âœ… domain æŠ½å–å®Œæˆ: {result.get('domains', [])}")
    return result


def extract_ideal(abstract: str, keywords: List[str], introduction: str, method_first_para: str) -> Dict:
    """ä»»åŠ¡ 2ï¼šæŠ½å– ideal"""
    print("\nğŸ“Œ ä»»åŠ¡ 2ï¼šæŠ½å– ideal")
    
    # æå– method ç¬¬ä¸€æ®µ
    method_intro = method_first_para[:1500] if method_first_para else ""
    
    prompt = f"""ã€è¾“å…¥ã€‘
- æ‘˜è¦: {abstract}
- å…³é”®è¯: {', '.join(keywords) if keywords else 'æ— '}
- å¼•è¨€: {introduction[:3000]}
- æ–¹æ³•éƒ¨åˆ†å¼€å¤´: {method_intro}

ã€ä»»åŠ¡ã€‘
æå–è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ä¿¡æ¯ã€‚

è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š
1. core_ideaï¼šè¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿç”¨ä¸€å¥è¯æè¿°ï¼ˆä¸è¶…è¿‡50å­—ï¼‰
2. tech_stackï¼šè®ºæ–‡ä½¿ç”¨äº†å“ªäº›å…³é”®æŠ€æœ¯ï¼Ÿåˆ—å‡ºæŠ€æœ¯åè¯
3. input_typeï¼šè®ºæ–‡æ–¹æ³•çš„è¾“å…¥æ˜¯ä»€ä¹ˆç±»å‹çš„æ•°æ®æˆ–é—®é¢˜ï¼Ÿ
4. output_typeï¼šè®ºæ–‡æ–¹æ³•çš„è¾“å‡ºæ˜¯ä»€ä¹ˆç±»å‹çš„ç»“æœï¼Ÿ

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·ä¸¥æ ¼è¾“å‡º JSON æ ¼å¼ï¼š
{{
  "core_idea": "ä¸€å¥è¯æè¿°æ ¸å¿ƒåˆ›æ–°ç‚¹",
  "tech_stack": ["æŠ€æœ¯1", "æŠ€æœ¯2"],
  "input_type": "è¾“å…¥ç±»å‹æè¿°",
  "output_type": "è¾“å‡ºç±»å‹æè¿°"
}}"""

    system_prompt = "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å­¦æœ¯è®ºæ–‡åˆ†æä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„æ ¼å¼è¾“å‡º JSONã€‚"
    response = call_llm(prompt, system_prompt)
    result = parse_json_response(response)
    print(f"  âœ… ideal æŠ½å–å®Œæˆ: {result.get('core_idea', '')[:50]}...")
    return result


def extract_skeleton(introduction: str, related_work: str, method: str, experiments: str) -> Dict:
    """ä»»åŠ¡ 3ï¼šæŠ½å– skeleton"""
    print("\nğŸ“Œ ä»»åŠ¡ 3ï¼šæŠ½å– skeleton")
    
    # æå–å„éƒ¨åˆ†çš„ç»“æ„æ€§ä¿¡æ¯
    intro_summary = introduction[:3000] if introduction else ""
    related_summary = related_work[:2000] if related_work else ""
    method_summary = method[:3000] if method else ""
    exp_summary = experiments[:3000] if experiments else ""
    
    prompt = f"""ã€è¾“å…¥ã€‘
- å¼•è¨€: {intro_summary}
- ç›¸å…³å·¥ä½œ: {related_summary}
- æ–¹æ³•: {method_summary}
- å®éªŒ: {exp_summary}

ã€ä»»åŠ¡ã€‘
åˆ†æè¿™ç¯‡è®ºæ–‡çš„å™äº‹ç»“æ„ç­–ç•¥ï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

1. problem_framingï¼šè®ºæ–‡å¦‚ä½•å¼•å‡ºé—®é¢˜ï¼Ÿç”¨äº†ä»€ä¹ˆå¼€ç¯‡ç­–ç•¥ï¼Ÿï¼ˆå¦‚ï¼šä»å®é™…ç—›ç‚¹å‡ºå‘ã€ä»å­¦æœ¯gapå‡ºå‘ã€ä»åº”ç”¨éœ€æ±‚å‡ºå‘ç­‰ï¼‰
2. gap_patternï¼šè®ºæ–‡å¦‚ä½•æ‰¹è¯„ç°æœ‰æ–¹æ³•ï¼Ÿç”¨äº†ä»€ä¹ˆé€»è¾‘æˆ–å¥å¼ï¼Ÿï¼ˆå¦‚ï¼šç°æœ‰æ–¹æ³•å¿½è§†äº†Xã€ç°æœ‰æ–¹æ³•åœ¨Yåœºæ™¯ä¸‹å¤±æ•ˆç­‰ï¼‰
3. method_storyï¼šæ–¹æ³•éƒ¨åˆ†çš„å™è¿°é¡ºåºå’Œç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆå¦‚ï¼šå…ˆæ•´ä½“åå±€éƒ¨ã€åˆ†æ¨¡å—ä»‹ç»ã€ä»ç®€å•åˆ°å¤æ‚ç­‰ï¼‰
4. experiments_storyï¼šå®éªŒéƒ¨åˆ†çš„å™è¿°ç­–ç•¥æ˜¯ä»€ä¹ˆï¼ŸåŒ…å«å“ªäº›ç±»å‹çš„å®éªŒï¼Ÿï¼ˆå¦‚ï¼šä¸»å®éªŒ+æ¶ˆè+å¯è§†åŒ–ã€å¤šæ•°æ®é›†éªŒè¯ç­‰ï¼‰

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·ä¸¥æ ¼è¾“å‡º JSON æ ¼å¼ï¼š
{{
  "problem_framing": "é—®é¢˜å¼•å‡ºç­–ç•¥æè¿°",
  "gap_pattern": "Gap æ‰¹è¯„ç­–ç•¥æè¿°",
  "method_story": "æ–¹æ³•å™è¿°ç­–ç•¥æè¿°",
  "experiments_story": "å®éªŒå™è¿°ç­–ç•¥æè¿°"
}}"""

    system_prompt = "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å­¦æœ¯è®ºæ–‡åˆ†æä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„æ ¼å¼è¾“å‡º JSONã€‚"
    response = call_llm(prompt, system_prompt)
    result = parse_json_response(response)
    print(f"  âœ… skeleton æŠ½å–å®Œæˆ")
    return result


def extract_tricks(introduction: str, method: str, experiments: str) -> List[Dict]:
    """ä»»åŠ¡ 4ï¼šæŠ½å– tricks"""
    print("\nğŸ“Œ ä»»åŠ¡ 4ï¼šæŠ½å– tricks")
    
    intro_text = introduction[:3000] if introduction else ""
    method_text = method[:4000] if method else ""
    exp_text = experiments[:4000] if experiments else ""
    
    prompt = f"""ã€è¾“å…¥ã€‘
- å¼•è¨€éƒ¨åˆ†: {intro_text}
- æ–¹æ³•éƒ¨åˆ†: {method_text}
- å®éªŒéƒ¨åˆ†: {exp_text}

ã€ä»»åŠ¡ã€‘
è¯·åƒä¸€ä½ç»éªŒä¸°å¯Œçš„å®¡ç¨¿äººä¸€æ ·ï¼Œåˆ†æè¿™ç¯‡è®ºæ–‡åœ¨â€œå¼•è¨€â€ã€â€œæ–¹æ³•æè¿°â€å’Œâ€œå®éªŒè®¾è®¡â€ä¸­ä½¿ç”¨äº†å“ªäº›**å†™ä½œæŠ€å·§æˆ–åŒ…è£…ç­–ç•¥**ã€‚

ä»ä»¥ä¸‹å‡ ä¸ªç»´åº¦æ€è€ƒï¼š
1. è¯´æœåŠ›ï¼šä½œè€…ç”¨äº†ä»€ä¹ˆæ–¹æ³•è®©è¯»è€…ç›¸ä¿¡è¿™ä¸ªæ–¹æ³•æœ‰æ•ˆï¼Ÿ
2. æ–°é¢–æ€§ï¼šä½œè€…å¦‚ä½•å±•ç¤ºè¿™ä¸ªå·¥ä½œçš„åˆ›æ–°ç‚¹ï¼Ÿ
3. å¯è§£é‡Šæ€§ï¼šä½œè€…å¦‚ä½•å¸®åŠ©è¯»è€…ç†è§£æ–¹æ³•çš„åŸç†ï¼Ÿ
4. å®Œå¤‡æ€§ï¼šä½œè€…å¦‚ä½•è¯æ˜å®éªŒæ˜¯å……åˆ†çš„ã€ç»“è®ºæ˜¯å¯é çš„ï¼Ÿ
5. å¯¹æ¯”æ€§ï¼šä½œè€…å¦‚ä½•ä¸ç°æœ‰æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Ÿ
6. å™äº‹ç»“æ„ï¼šä½œè€…å¦‚ä½•ç»„ç»‡å…¨æ–‡çš„é€»è¾‘æµï¼Ÿå¦‚ä½•å¼•å…¥é—®é¢˜ã€é“ºå«æ–¹æ³•ã€å‘¼åº”ç»“è®ºï¼Ÿ

è¯·è¯†åˆ«å‡ºè®ºæ–‡ä¸­ä½¿ç”¨çš„å…·ä½“æŠ€å·§ï¼Œæ¯ä¸ªæŠ€å·§éœ€è¦è¯´æ˜ï¼š
- nameï¼šæŠ€å·§åç§°ï¼ˆç”¨ç®€æ´çš„è¯è¯­å‘½åï¼‰
- typeï¼šæ‰€å±ç±»å‹ï¼ˆmethod-level / experiment-level / writing-levelï¼‰
- purposeï¼šè¾¾æˆç›®çš„ï¼ˆè¿™ä¸ªæŠ€å·§å¸®åŠ©ä½œè€…è¾¾æˆä»€ä¹ˆæ•ˆæœï¼‰
- locationï¼šå…·ä½“ä½ç½®ï¼ˆå‡ºç°åœ¨è®ºæ–‡çš„å“ªä¸ªéƒ¨åˆ†ï¼‰
- descriptionï¼šç®€è¦æè¿°ï¼ˆç”¨ä¸€å¥è¯æè¿°ä½œè€…æ˜¯æ€ä¹ˆåšçš„ï¼‰

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·ä¸¥æ ¼è¾“å‡º JSON æ•°ç»„æ ¼å¼ï¼š
[
  {{
    "name": "æŠ€å·§åç§°",
    "type": "method-level / experiment-level / writing-level",
    "purpose": "ç›®çš„æè¿°",
    "location": "introduction / method / experiments / å…¶ä»–",
    "description": "ç®€è¦æè¿°"
  }}
]"""

    system_prompt = "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å­¦æœ¯è®ºæ–‡åˆ†æä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„æ ¼å¼è¾“å‡º JSON æ•°ç»„ã€‚"
    response = call_llm(prompt, system_prompt)
    result = parse_json_response(response)
    
    if isinstance(result, list):
        print(f"  âœ… tricks æŠ½å–å®Œæˆ: å‘ç° {len(result)} ä¸ªæŠ€å·§")
        return result
    else:
        print(f"  âš ï¸ tricks è§£æå¼‚å¸¸ï¼Œè¿”å›ç©ºåˆ—è¡¨")
        return []



# ============================================================
# ä¸»æµç¨‹
# ============================================================
def extract_paper_with_reviews(
    paper_path: str, 
    review_path: str, 
    metadata_path: str = None
):
    """æŠ½å–å•ç¯‡è®ºæ–‡åŠå…¶å®¡ç¨¿ä¿¡æ¯ï¼Œè¿”å› (PaperNode, List[ReviewNode])"""
    
    # è·å– paper_id
    filename = os.path.basename(paper_path)
    paper_id = filename.replace('_paper.json', '')
    
    print("=" * 60)
    print(f"ğŸ“„ å¼€å§‹æŠ½å–: {paper_id}")
    print("=" * 60)
    
    # Step 1: è§£æè®ºæ–‡
    print("\nğŸ” Step 1: è§£æè®ºæ–‡ JSON")
    sections = parse_json_paper(paper_path)
    print(f"  æ ‡é¢˜: {sections.title[:60]}...")
    
    # Step 2: è§£æå®¡ç¨¿
    print("\nğŸ” Step 2: è§£æå®¡ç¨¿æ„è§")
    review_nodes = parse_reviews(review_path, paper_id)
    print(f"  å®¡ç¨¿æ•°é‡: {len(review_nodes)} æ¡")
    
    # Step 3: è§£æå…ƒæ•°æ®
    metadata = {}
    if metadata_path:
        metadata = parse_metadata(metadata_path)
    conference = metadata.get('conference', 'ARR')
    
    # Step 4: LLM æŠ½å–è®ºæ–‡ä¿¡æ¯
    print("\nğŸ¤– Step 3: LLM æŠ½å–è®ºæ–‡ä¿¡æ¯")
    
    domain_result = extract_domain(
        sections.title,
        sections.keywords,
        sections.abstract
    )
    
    ideal_result = extract_ideal(
        sections.abstract,
        sections.keywords,
        sections.introduction,
        sections.method
    )
    
    skeleton_result = extract_skeleton(
        sections.introduction,
        sections.related_work,
        sections.method,
        sections.experiments
    )
    
    tricks_result = extract_tricks(
        sections.introduction,
        sections.method,
        sections.experiments
    )
    
    # Step 5: æ„å»ºè®ºæ–‡èŠ‚ç‚¹
    print("\nğŸ“¦ Step 4: æ„å»ºçŸ¥è¯†å›¾è°±èŠ‚ç‚¹")
    
    paper_node = PaperNode(
        paper_id=paper_id,
        title=sections.title,
        conference=conference,
        domain=domain_result,
        ideal=ideal_result,
        skeleton=skeleton_result,
        tricks=tricks_result
    )
    
    print(f"\nâœ… æŠ½å–å®Œæˆ")
    print(f"  è®ºæ–‡èŠ‚ç‚¹: {paper_id}")
    print(f"  å®¡ç¨¿èŠ‚ç‚¹: {len(review_nodes)} ä¸ª")
    
    return paper_node, review_nodes


def batch_extract(
    paper_dir: str, 
    review_dir: str, 
    metadata_dir: str,
    output_dir: str, 
    limit: int = None
):
    """æ‰¹é‡æŠ½å–ï¼Œç”Ÿæˆ paper èŠ‚ç‚¹å’Œ review èŠ‚ç‚¹"""
    
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰ paper æ–‡ä»¶
    paper_files = [f for f in os.listdir(paper_dir) if f.endswith('_paper.json')]
    paper_files.sort()
    
    if limit:
        paper_files = paper_files[:limit]
    
    print(f"\nğŸš€ æ‰¹é‡æŠ½å–å¼€å§‹")
    print(f"   Paper ç›®å½•: {paper_dir}")
    print(f"   Review ç›®å½•: {review_dir}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   å¾…å¤„ç†: {len(paper_files)} ç¯‡")
    print("=" * 60)
    
    success_count = 0
    fail_count = 0
    all_paper_nodes = []
    all_review_nodes = []
    
    for i, filename in enumerate(paper_files, 1):
        paper_id = filename.replace('_paper.json', '')
        
        paper_path = os.path.join(paper_dir, filename)
        review_path = os.path.join(review_dir, f"{paper_id}_review.json")
        metadata_path = os.path.join(metadata_dir, f"{paper_id}_metadata.json")
        
        # è¾“å‡ºè·¯å¾„
        paper_output_file = os.path.join(output_dir, f"{paper_id}_paper_node.json")
        
        # è·³è¿‡å·²å¤„ç†
        if os.path.exists(paper_output_file):
            print(f"\n[{i}/{len(paper_files)}] â­ï¸ è·³è¿‡å·²å¤„ç†: {paper_id}")
            continue
        
        print(f"\n[{i}/{len(paper_files)}] å¤„ç†ä¸­...")
        
        try:
            paper_node, review_nodes = extract_paper_with_reviews(
                paper_path, review_path, metadata_path
            )
            
            # ä¿å­˜ paper èŠ‚ç‚¹
            with open(paper_output_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(paper_node), f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å·²ä¿å­˜ paper èŠ‚ç‚¹: {paper_output_file}")
            
            # ä¿å­˜æ‰€æœ‰ review èŠ‚ç‚¹åˆ°ä¸€ä¸ªæ–‡ä»¶
            if review_nodes:
                review_output_file = os.path.join(output_dir, f"{paper_id}_reviews.json")
                with open(review_output_file, 'w', encoding='utf-8') as f:
                    json.dump([asdict(r) for r in review_nodes], f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ å·²ä¿å­˜ {len(review_nodes)} ä¸ª review èŠ‚ç‚¹: {review_output_file}")
            
            all_paper_nodes.append(asdict(paper_node))
            all_review_nodes.extend([asdict(r) for r in review_nodes])
            success_count += 1
            
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {paper_id}, é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            fail_count += 1
            continue
    
    # ä¿å­˜æ±‡æ€»æ–‡ä»¶
    if all_paper_nodes:
        paper_summary_file = os.path.join(output_dir, "_all_paper_nodes.json")
        with open(paper_summary_file, 'w', encoding='utf-8') as f:
            json.dump(all_paper_nodes, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ Paper æ±‡æ€»æ–‡ä»¶: {paper_summary_file}")
    
    if all_review_nodes:
        review_summary_file = os.path.join(output_dir, "_all_review_nodes.json")
        with open(review_summary_file, 'w', encoding='utf-8') as f:
            json.dump(all_review_nodes, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ Review æ±‡æ€»æ–‡ä»¶: {review_summary_file}")
    
    print("\n" + "=" * 60)
    print(f"ğŸ‰ æ‰¹é‡æŠ½å–å®Œæˆï¼")
    print(f"   æˆåŠŸ: {success_count} ç¯‡")
    print(f"   å¤±è´¥: {fail_count} ç¯‡")
    print(f"   Paper èŠ‚ç‚¹: {len(all_paper_nodes)} ä¸ª")
    print(f"   Review èŠ‚ç‚¹: {len(all_review_nodes)} ä¸ª")
    print("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    # é»˜è®¤é…ç½® - ARR_2022
    base_dir = "ARR/ARR_2022"
    paper_dir = f"{base_dir}/ARR_2022_paper"
    review_dir = f"{base_dir}/ARR_2022_review"
    metadata_dir = f"{base_dir}/ARR_2022_metadata"
    output_dir = "extract_result/ARR_2022"  # ç»Ÿä¸€è¾“å‡ºåˆ° extract_result ç›®å½•
    limit = 3  # é»˜è®¤åªå¤„ç†å‰3ç¯‡æµ‹è¯•
    
    # å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        base_dir = sys.argv[1]
        conf_name = os.path.basename(base_dir)
        paper_dir = f"{base_dir}/{conf_name}_paper"
        review_dir = f"{base_dir}/{conf_name}_review"
        metadata_dir = f"{base_dir}/{conf_name}_metadata"
        output_dir = f"extract_result/{conf_name}"  # ç»Ÿä¸€è¾“å‡ºåˆ° extract_result ç›®å½•
    
    if len(sys.argv) > 2:
        limit = int(sys.argv[2]) if sys.argv[2] != 'all' else None
    
    # å•æ–‡ä»¶æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1].endswith('.json'):
        paper_path = sys.argv[1]
        paper_id = os.path.basename(paper_path).replace('_paper.json', '')
        
        # æ„å»º review å’Œ metadata è·¯å¾„
        paper_dir = os.path.dirname(paper_path)
        base_dir = os.path.dirname(paper_dir)
        review_path = os.path.join(base_dir, os.path.basename(paper_dir).replace('paper', 'review'), f"{paper_id}_review.json")
        metadata_path = os.path.join(base_dir, os.path.basename(paper_dir).replace('paper', 'metadata'), f"{paper_id}_metadata.json")
        
        paper_node, review_nodes = extract_paper_with_reviews(paper_path, review_path, metadata_path)
        
        # ç»Ÿä¸€è¾“å‡ºåˆ° extract_result ç›®å½•
        os.makedirs('extract_result', exist_ok=True)
        
        # ä¿å­˜ paper èŠ‚ç‚¹
        paper_output_file = os.path.join('extract_result', f"{paper_id}_paper_node.json")
        with open(paper_output_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(paper_node), f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ Paper èŠ‚ç‚¹å·²ä¿å­˜åˆ°: {paper_output_file}")
        
        # ä¿å­˜ review èŠ‚ç‚¹åˆ°ä¸€ä¸ªæ–‡ä»¶
        if review_nodes:
            review_output_file = os.path.join('extract_result', f"{paper_id}_reviews.json")
            with open(review_output_file, 'w', encoding='utf-8') as f:
                json.dump([asdict(r) for r in review_nodes], f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ {len(review_nodes)} ä¸ª Review èŠ‚ç‚¹å·²ä¿å­˜åˆ°: {review_output_file}")
    else:
        # æ‰¹é‡æ¨¡å¼
        batch_extract(paper_dir, review_dir, metadata_dir, output_dir, limit)


if __name__ == "__main__":
    main()
