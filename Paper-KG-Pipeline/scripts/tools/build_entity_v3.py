"""
çŸ¥è¯†å›¾è°±æ„å»ºè„šæœ¬ V3 - åŸºäºICLRæ•°æ®æº
ä»assignmentså’Œcluster_libraryæ„å»ºçŸ¥è¯†å›¾è°±

æ•°æ®æºï¼š
  - data/ICLR_25/assignments.jsonl: Paperåˆ°Patternçš„åˆ†é…å…³ç³»
  - data/ICLR_25/cluster_library_sorted.jsonl: Pattern Clusterä¿¡æ¯
  - data/ICLR_25/iclr_patterns_full_cn_912.jsonl: Patternè¯¦ç»†å±æ€§

èŠ‚ç‚¹ç±»å‹ï¼š
  - Idea: æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼ˆä»patternçš„ideaå­—æ®µæå–ï¼‰
  - Pattern: å†™ä½œå¥—è·¯ï¼ˆä»clusterä¿¡æ¯æ„å»ºï¼‰
  - Domain: ç ”ç©¶é¢†åŸŸï¼ˆä»domainå­—æ®µèšåˆï¼‰
  - Paper: è®ºæ–‡ï¼ˆä»assignmentsæ„å»ºï¼‰

è¾“å‡º:
  - output/nodes_idea.json
  - output/nodes_pattern.json
  - output/nodes_domain.json
  - output/nodes_paper.json
  - output/knowledge_graph_stats.json
"""

import hashlib
import json
import os
import sys
from collections import defaultdict
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Dict, List

# å¯¼å…¥pipelineå·¥å…·å‡½æ•°
SCRIPT_DIR = Path(__file__).resolve().parent
SCRIPTS_DIR = SCRIPT_DIR.parent
sys.path.insert(0, str(SCRIPTS_DIR))
from pipeline.utils import call_llm, parse_json_from_llm

# ===================== é…ç½® =====================

PROJECT_ROOT = SCRIPTS_DIR.parent

# è¾“å…¥è·¯å¾„
DATA_DIR = PROJECT_ROOT / "data" / "ICLR_25"
ASSIGNMENTS_FILE = DATA_DIR / "assignments.jsonl"
CLUSTER_LIBRARY_FILE = DATA_DIR / "cluster_library_sorted.jsonl"
PATTERN_DETAILS_FILE = DATA_DIR / "iclr_patterns_full.jsonl"  # ä½¿ç”¨å®Œæ•´çš„è‹±æ–‡ç‰ˆæœ¬
REVIEWS_FILE = DATA_DIR / "paper_reviews_dataset_iclr_reviews_filtered.jsonl"  # Review æ•°æ®

# è¾“å‡ºè·¯å¾„
OUTPUT_DIR = PROJECT_ROOT / "output"
NODES_IDEA = OUTPUT_DIR / "nodes_idea.json"
NODES_PATTERN = OUTPUT_DIR / "nodes_pattern.json"
NODES_DOMAIN = OUTPUT_DIR / "nodes_domain.json"
NODES_PAPER = OUTPUT_DIR / "nodes_paper.json"
NODES_REVIEW = OUTPUT_DIR / "nodes_review.json"  # æ–°å¢ï¼šReview èŠ‚ç‚¹
STATS_FILE = OUTPUT_DIR / "knowledge_graph_stats.json"

# LLM API é…ç½®
SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY", "")
LLM_API_URL = os.getenv("LLM_API_URL", "https://api.siliconflow.cn/v1/chat/completions")
LLM_MODEL = os.getenv("LLM_MODEL", "Qwen/Qwen3-14B")


# ===================== æ•°æ®ç±» =====================

@dataclass
class GraphStats:
    """å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
    total_nodes: int = 0
    ideas: int = 0
    patterns: int = 0
    domains: int = 0
    papers: int = 0
    reviews: int = 0


# ===================== èŠ‚ç‚¹æ„å»ºå™¨ =====================

class KnowledgeGraphBuilderV3:
    """çŸ¥è¯†å›¾è°±æ„å»ºå™¨ V3 - åŸºäºICLRæ•°æ®æº"""

    def __init__(self):
        self.stats = GraphStats()

        # èŠ‚ç‚¹å­˜å‚¨
        self.idea_nodes: List[Dict] = []
        self.pattern_nodes: List[Dict] = []
        self.domain_nodes: List[Dict] = []
        self.paper_nodes: List[Dict] = []
        self.review_nodes: List[Dict] = []  # æ–°å¢ï¼šReview èŠ‚ç‚¹

        # å»é‡æ˜ å°„
        self.idea_map: Dict[str, str] = {}           # idea_hash -> idea_id
        self.domain_map: Dict[str, str] = {}         # domain_name -> domain_id
        self.pattern_map: Dict[int, str] = {}        # cluster_id -> pattern_id
        self.global_pattern_map: Dict[str, str] = {} # global_pattern_id -> idea_id
        self.paper_map: Dict[str, str] = {}          # paper_id -> paper_id
        self.review_map: Dict[str, str] = {}         # æ–°å¢ï¼šreview_id -> review_id

        # ä¸­é—´æ•°æ®
        self.paper_details: Dict[str, Dict] = {}     # paper_id -> pattern details
        self.paper_reviews_map: Dict[str, List[Dict]] = {}  # æ–°å¢ï¼špaper_id -> list of reviews

    def build(self):
        """æ„å»ºå®Œæ•´çš„çŸ¥è¯†å›¾è°±"""
        print("=" * 60)
        print("ğŸš€ å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°± V3 (ICLRæ•°æ®æº)")
        print("=" * 60)

        # Step 1: åŠ è½½æ•°æ®
        print("\nã€Step 1ã€‘åŠ è½½æ•°æ®")
        assignments = self._load_assignments()
        clusters = self._load_clusters()
        pattern_details = self._load_pattern_details()
        reviews_data = self._load_reviews()  # æ–°å¢ï¼šåŠ è½½ Review æ•°æ®
        print(f"âœ… åŠ è½½ {len(assignments)} ç¯‡è®ºæ–‡åˆ†é…")
        print(f"âœ… åŠ è½½ {len(clusters)} ä¸ª Pattern Clusters")
        print(f"âœ… åŠ è½½ {len(pattern_details)} ç¯‡è®ºæ–‡çš„è¯¦ç»†Pattern")
        print(f"âœ… åŠ è½½ {len(reviews_data)} ç¯‡è®ºæ–‡çš„ Review æ•°æ®")  # æ–°å¢

        # Step 2: æ„å»ºèŠ‚ç‚¹
        print("\nã€Step 2ã€‘æ„å»ºèŠ‚ç‚¹")
        self._build_pattern_nodes(clusters)
        self._enhance_patterns_with_llm(clusters)  # LLMå¢å¼ºPatternèŠ‚ç‚¹
        self._build_idea_nodes(pattern_details)
        self._build_domain_nodes(assignments, clusters)
        self._build_paper_nodes(assignments, pattern_details)
        self._build_review_nodes(reviews_data)  # æ–°å¢ï¼šæ„å»º Review èŠ‚ç‚¹

        # Step 3: å»ºç«‹å…³è”
        print("\nã€Step 3ã€‘å»ºç«‹èŠ‚ç‚¹å…³è”")
        self._link_paper_to_pattern(assignments)
        self._link_paper_to_idea()
        self._link_paper_to_domain()
        self._link_paper_to_review()  # æ–°å¢ï¼šå…³è” Paper å’Œ Review
        self._link_idea_to_pattern()

        # Step 4: ä¿å­˜èŠ‚ç‚¹
        print("\nã€Step 4ã€‘ä¿å­˜èŠ‚ç‚¹")
        self._save_nodes()

        # Step 5: ç»Ÿè®¡
        print("\nã€Step 5ã€‘ç»Ÿè®¡ä¿¡æ¯")
        self._update_stats()
        self._save_stats()
        self._print_stats()

        print("\n" + "=" * 60)
        print("âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ!")
        print("=" * 60)

    # ===================== æ•°æ®åŠ è½½ =====================

    def _load_assignments(self) -> List[Dict]:
        """åŠ è½½è®ºæ–‡åˆ†é…æ•°æ® (assignments.jsonl)"""
        assignments = []
        if not ASSIGNMENTS_FILE.exists():
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {ASSIGNMENTS_FILE}")
            return []

        with open(ASSIGNMENTS_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    assignments.append(json.loads(line))
        return assignments

    def _load_clusters(self) -> List[Dict]:
        """åŠ è½½Pattern Clusteræ•°æ® (cluster_library_sorted.jsonl)"""
        clusters = []
        if not CLUSTER_LIBRARY_FILE.exists():
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {CLUSTER_LIBRARY_FILE}")
            return []

        with open(CLUSTER_LIBRARY_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    clusters.append(json.loads(line))
        return clusters

    def _load_pattern_details(self) -> Dict[str, Dict]:
        """åŠ è½½Patternè¯¦ç»†å±æ€§ (iclr_patterns_full.jsonl)"""
        details = {}
        if not PATTERN_DETAILS_FILE.exists():
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {PATTERN_DETAILS_FILE}")
            return {}

        with open(PATTERN_DETAILS_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    item = json.loads(line)
                    paper_id = item.get('paper_id')
                    if paper_id:
                        details[paper_id] = item

        self.paper_details = details
        return details

    def _load_reviews(self) -> Dict[str, List[Dict]]:
        """åŠ è½½Reviewæ•°æ® (paper_reviews_dataset_iclr_reviews_filtered.jsonl)"""
        reviews_data = {}
        if not REVIEWS_FILE.exists():
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {REVIEWS_FILE}")
            return {}

        with open(REVIEWS_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    item = json.loads(line)
                    paper_id = item.get('id')  # è®ºæ–‡IDæ¥è‡ª 'id' å­—æ®µ
                    related_notes = item.get('related_notes', '[]')

                    # è§£æ related_notesï¼ˆæ˜¯ä¸€ä¸ª JSON å­—ç¬¦ä¸²ï¼‰
                    if isinstance(related_notes, str):
                        try:
                            reviews_list = json.loads(related_notes)
                        except:
                            reviews_list = []
                    else:
                        reviews_list = related_notes if isinstance(related_notes, list) else []

                    if paper_id and reviews_list:
                        reviews_data[paper_id] = reviews_list
                        self.paper_reviews_map[paper_id] = reviews_list

        return reviews_data

    # ===================== æ„å»ºèŠ‚ç‚¹ =====================

    def _build_pattern_nodes(self, clusters: List[Dict]):
        """æ„å»º Pattern èŠ‚ç‚¹ï¼ˆåŸºäºclusterä¿¡æ¯ï¼‰+ LLMå¢å¼º"""
        print("\nğŸ“‹ æ„å»º Pattern èŠ‚ç‚¹...")

        for cluster in clusters:
            cluster_id = cluster.get('cluster_id')
            if cluster_id == -1:  # è·³è¿‡æœªåˆ†é…çš„cluster
                continue

            pattern_id = f"pattern_{cluster_id}"
            self.pattern_map[cluster_id] = pattern_id

            # æå–ä»£è¡¨æ€§è®ºæ–‡çš„patternä¿¡æ¯
            exemplars = cluster.get('exemplars', [])
            exemplar_ideas = []
            exemplar_problems = []
            exemplar_solutions = []
            exemplar_stories = []  # æ–°å¢ï¼šæå–storyä¿¡æ¯

            for ex in exemplars[:3]:  # å–å‰3ä¸ªä»£è¡¨æ€§è®ºæ–‡
                if ex.get('idea'):
                    exemplar_ideas.append(ex['idea'])
                if ex.get('base_problem'):
                    exemplar_problems.append(ex['base_problem'])
                if ex.get('solution_pattern'):
                    exemplar_solutions.append(ex['solution_pattern'])
                if ex.get('story'):  # æ–°å¢ï¼šæå–story
                    exemplar_stories.append(ex['story'])

            # æ„å»ºPatternèŠ‚ç‚¹
            pattern_node = {
                'pattern_id': pattern_id,
                'cluster_id': cluster_id,
                'name': cluster.get('cluster_name', ''),
                'size': cluster.get('size', 0),

                # é¢†åŸŸä¿¡æ¯
                'domain': cluster.get('retrieval_facets', {}).get('domain', ''),
                'sub_domains': cluster.get('retrieval_facets', {}).get('sub_domains', []),

                # èšç±»è´¨é‡æŒ‡æ ‡
                'coherence': {
                    'centroid_mean': cluster.get('coherence', {}).get('centroid_mean', 0),
                    'centroid_p50': cluster.get('coherence', {}).get('centroid_p50', 0),
                    'pairwise_sample_mean': cluster.get('coherence', {}).get('pairwise_sample_mean', 0),
                    'pairwise_sample_p50': cluster.get('coherence', {}).get('pairwise_sample_p50', 0)
                },

                # ä»£è¡¨æ€§ä¿¡æ¯ï¼ˆä»exemplarsæå–ï¼ŒåŒ…å«storyï¼‰
                'summary': {
                    'representative_ideas': exemplar_ideas,
                    'common_problems': exemplar_problems,
                    'solution_approaches': exemplar_solutions,
                    'story': exemplar_stories  # æ–°å¢ï¼šstoryç»´åº¦
                },

                # å…ƒæ•°æ®
                'exemplar_count': len(exemplars),
                'exemplar_paper_ids': [ex.get('paper_id') for ex in exemplars if ex.get('paper_id')]
            }

            self.pattern_nodes.append(pattern_node)

        print(f"  âœ“ åˆ›å»º {len(self.pattern_nodes)} ä¸ª Pattern èŠ‚ç‚¹")

    def _enhance_patterns_with_llm(self, clusters: List[Dict]):
        """ä½¿ç”¨LLMå¢å¼ºPatternèŠ‚ç‚¹çš„å½’çº³æ€§æè¿°"""
        print("\nğŸ¤– ä½¿ç”¨LLMå¢å¼ºPatternèŠ‚ç‚¹...")

        # æ£€æŸ¥API Key
        if not SILICONFLOW_API_KEY:
            print("  âš ï¸  è­¦å‘Š: SILICONFLOW_API_KEY æœªè®¾ç½®ï¼Œè·³è¿‡LLMå¢å¼º")
            return

        enhanced_count = 0

        for idx, pattern_node in enumerate(self.pattern_nodes):
            cluster_id = pattern_node['cluster_id']

            # æ‰¾åˆ°å¯¹åº”çš„cluster
            cluster = next((c for c in clusters if c.get('cluster_id') == cluster_id), None)
            if not cluster:
                continue

            # æ”¶é›†è¯¥clusterä¸­æ‰€æœ‰è®ºæ–‡çš„Patternä¿¡æ¯
            exemplars = cluster.get('exemplars', [])

            # æ„å»ºPrompt
            prompt = self._build_llm_prompt_for_pattern(pattern_node, exemplars)

            # è°ƒç”¨LLM
            print(f"  å¤„ç† {idx+1}/{len(self.pattern_nodes)}: {pattern_node['name']} (cluster_id={cluster_id}, size={pattern_node['size']})")

            llm_response = call_llm(prompt, temperature=0.3, max_tokens=4096)

            if llm_response:
                # è§£æLLMå“åº”
                parsed_summary = parse_json_from_llm(llm_response)

                if parsed_summary and all(k in parsed_summary for k in ['representative_ideas', 'common_problems', 'solution_approaches', 'story']):
                    # æ·»åŠ LLMç”Ÿæˆçš„å½’çº³æ€§æè¿°
                    pattern_node['llm_enhanced_summary'] = {
                        'representative_ideas': parsed_summary.get('representative_ideas', ''),
                        'common_problems': parsed_summary.get('common_problems', ''),
                        'solution_approaches': parsed_summary.get('solution_approaches', ''),
                        'story': parsed_summary.get('story', '')
                    }
                    pattern_node['llm_enhanced'] = True
                    enhanced_count += 1
                    print(f"    âœ“ LLMå¢å¼ºæˆåŠŸ")
                else:
                    print(f"    âš ï¸  LLMå“åº”æ ¼å¼é”™è¯¯ï¼Œè·³è¿‡")
                    pattern_node['llm_enhanced'] = False
            else:
                print(f"    âŒ LLMè°ƒç”¨å¤±è´¥ï¼Œè·³è¿‡")
                pattern_node['llm_enhanced'] = False

        print(f"\n  âœ“ æˆåŠŸå¢å¼º {enhanced_count}/{len(self.pattern_nodes)} ä¸ª Pattern èŠ‚ç‚¹")

    def _build_llm_prompt_for_pattern(self, pattern_node: Dict, exemplars: List[Dict]) -> str:
        """ä¸ºPatternç”ŸæˆLLM Prompt"""

        cluster_name = pattern_node.get('name', '')
        cluster_size = pattern_node.get('size', 0)
        domain = pattern_node.get('domain', '')

        # æ”¶é›†æ‰€æœ‰exemplarçš„ä¿¡æ¯
        all_ideas = []
        all_problems = []
        all_solutions = []
        all_stories = []

        for ex in exemplars:
            if ex.get('idea'):
                all_ideas.append(ex['idea'])
            if ex.get('base_problem'):
                all_problems.append(ex['base_problem'])
            if ex.get('solution_pattern'):
                all_solutions.append(ex['solution_pattern'])
            if ex.get('story'):
                all_stories.append(ex['story'])

        # æ„å»ºPrompt
        prompt = f"""You are a research pattern analyst. Given a cluster of {cluster_size} research papers in the "{domain}" domain, your task is to analyze the common patterns and generate a comprehensive summary.

**Cluster Name**: {cluster_name}

**Representative Ideas from papers**:
{self._format_list(all_ideas[:20])}

**Common Problems from papers**:
{self._format_list(all_problems[:20])}

**Solution Approaches from papers**:
{self._format_list(all_solutions[:20])}

**Story/Reframing from papers**:
{self._format_list(all_stories[:20])}

---

Based on the above information, please generate an **inductive summary** for this pattern cluster. For each category, provide **ONE comprehensive sentence** that captures the essence of all papers in this cluster.

Return your response in the following JSON format:

{{
  "representative_ideas": "A single comprehensive sentence summarizing the core innovative ideas across all papers in this cluster.",
  "common_problems": "A single comprehensive sentence describing the common challenges and problems addressed by papers in this cluster.",
  "solution_approaches": "A single comprehensive sentence outlining the general solution strategies and methodologies employed across this cluster.",
  "story": "A single comprehensive sentence that reframes the research narrative and explains the transformative perspective this pattern brings to the field."
}}

Make sure each sentence is detailed, informative, and captures the diversity of approaches within the cluster. Keep the response in JSON format only."""

        return prompt

    def _format_list(self, items: List[str]) -> str:
        """æ ¼å¼åŒ–åˆ—è¡¨ä¸ºå¸¦åºå·çš„å­—ç¬¦ä¸²"""
        if not items:
            return "(No data available)"

        return "\n".join([f"{i+1}. {item[:300]}..." if len(item) > 300 else f"{i+1}. {item}"
                         for i, item in enumerate(items[:20])])

    def _build_idea_nodes(self, pattern_details: Dict[str, Dict]):
        """æ„å»º Idea èŠ‚ç‚¹ï¼ˆä»pattern_detailsçš„ideaå­—æ®µæå–ï¼‰"""
        print("\nğŸ’¡ æ„å»º Idea èŠ‚ç‚¹...")

        for paper_id, details in pattern_details.items():
            idea_text = details.get('idea', '')
            if not idea_text:
                continue

            # ç”¨hashå»é‡
            idea_hash = hashlib.md5(idea_text.encode()).hexdigest()[:16]

            if idea_hash not in self.idea_map:
                idea_id = f"idea_{len(self.idea_nodes)}"
                self.idea_map[idea_hash] = idea_id

                # æå–research_patternsä¿¡æ¯
                patterns = details.get('research_patterns', [])
                first_pattern = patterns[0] if patterns else {}

                self.idea_nodes.append({
                    'idea_id': idea_id,
                    'description': idea_text,

                    # ä»research_patternsæå–
                    'base_problem': first_pattern.get('base_problem', ''),
                    'solution_pattern': first_pattern.get('solution_pattern', ''),
                    'story': first_pattern.get('story', ''),
                    'application': first_pattern.get('application', ''),

                    # é¢†åŸŸä¿¡æ¯
                    'domain': details.get('domain', ''),
                    'sub_domains': details.get('sub_domains', []),

                    # æ¥æºä¿¡æ¯
                    'source_paper_ids': [paper_id],
                    'pattern_ids': []  # ç¨åå¡«å……
                })
            else:
                # å·²å­˜åœ¨çš„Ideaï¼Œè¿½åŠ paper_id
                idea_id = self.idea_map[idea_hash]
                for idea_node in self.idea_nodes:
                    if idea_node['idea_id'] == idea_id:
                        if paper_id not in idea_node['source_paper_ids']:
                            idea_node['source_paper_ids'].append(paper_id)
                        break

        print(f"  âœ“ åˆ›å»º {len(self.idea_nodes)} ä¸ª Idea èŠ‚ç‚¹")

    def _build_domain_nodes(self, assignments: List[Dict], clusters: List[Dict]):
        """æ„å»º Domain èŠ‚ç‚¹ï¼ˆèšåˆdomainå’Œsub_domainsä¿¡æ¯ï¼‰"""
        print("\nğŸŒ æ„å»º Domain èŠ‚ç‚¹...")

        domain_stats = defaultdict(lambda: {
            'paper_count': 0,
            'sub_domains': set(),
            'related_patterns': set(),
            'paper_ids': []
        })

        # ä»assignmentsèšåˆ
        for assignment in assignments:
            domain = assignment.get('domain', '')
            if not domain:
                continue

            paper_id = assignment.get('paper_id', '')
            cluster_id = assignment.get('cluster_id', -1)

            domain_stats[domain]['paper_count'] += 1
            domain_stats[domain]['paper_ids'].append(paper_id)

            # èšåˆsub_domains
            for sub_domain in assignment.get('sub_domains', []):
                domain_stats[domain]['sub_domains'].add(sub_domain)

            # å…³è”pattern
            if cluster_id != -1:
                domain_stats[domain]['related_patterns'].add(f"pattern_{cluster_id}")

        # ç”ŸæˆDomainèŠ‚ç‚¹
        for domain_name, stats in domain_stats.items():
            domain_id = f"domain_{len(self.domain_nodes)}"
            self.domain_map[domain_name] = domain_id

            self.domain_nodes.append({
                'domain_id': domain_id,
                'name': domain_name,
                'paper_count': stats['paper_count'],
                'sub_domains': sorted(list(stats['sub_domains'])),
                'related_pattern_ids': sorted(list(stats['related_patterns'])),
                'sample_paper_ids': stats['paper_ids'][:10]  # ä¿ç•™å‰10ä¸ªæ ·ä¾‹
            })

        print(f"  âœ“ åˆ›å»º {len(self.domain_nodes)} ä¸ª Domain èŠ‚ç‚¹")

    def _build_paper_nodes(self, assignments: List[Dict], pattern_details: Dict[str, Dict]):
        """æ„å»º Paper èŠ‚ç‚¹ï¼ˆåˆå¹¶assignmentså’Œpattern_detailsä¿¡æ¯ï¼‰"""
        print("\nğŸ“„ æ„å»º Paper èŠ‚ç‚¹...")

        for assignment in assignments:
            paper_id = assignment.get('paper_id', '')
            if not paper_id:
                continue

            self.paper_map[paper_id] = paper_id

            # ä»pattern_detailsè·å–è¯¦ç»†ä¿¡æ¯
            details = pattern_details.get(paper_id, {})
            patterns = details.get('research_patterns', [])
            first_pattern = patterns[0] if patterns else {}

            # æ„å»ºPaperèŠ‚ç‚¹
            self.paper_nodes.append({
                'paper_id': paper_id,
                'title': assignment.get('paper_title', ''),

                # Patternå…³è”
                'global_pattern_id': assignment.get('global_pattern_id', ''),
                'cluster_id': assignment.get('cluster_id', -1),
                'cluster_prob': assignment.get('cluster_prob', 0),

                # é¢†åŸŸä¿¡æ¯
                'domain': assignment.get('domain', ''),
                'sub_domains': assignment.get('sub_domains', []),

                # Ideaä¿¡æ¯ï¼ˆæ¥è‡ªpattern_detailsï¼‰
                'idea': details.get('idea', ''),

                # Patternè¯¦ç»†ä¿¡æ¯
                'pattern_details': {
                    'base_problem': first_pattern.get('base_problem', ''),
                    'solution_pattern': first_pattern.get('solution_pattern', ''),
                    'story': first_pattern.get('story', ''),
                    'application': first_pattern.get('application', '')
                },

                # å…³è”IDï¼ˆç¨åå¡«å……ï¼‰
                'pattern_id': '',
                'idea_id': '',
                'domain_id': ''
            })

        print(f"  âœ“ åˆ›å»º {len(self.paper_nodes)} ä¸ª Paper èŠ‚ç‚¹")

    def _build_review_nodes(self, reviews_data: Dict[str, List[Dict]]):
        """æ„å»º Review èŠ‚ç‚¹"""
        print("\nâ­ æ„å»º Review èŠ‚ç‚¹...")

        for paper_id, reviews_list in reviews_data.items():
            for review in reviews_list:
                review_id = review.get('review_id', '')
                if not review_id:
                    continue

                self.review_map[review_id] = review_id

                # è§£æ overall_score
                overall_score = review.get('overall_score', '')
                score_value = self._parse_overall_score(overall_score)

                # æå–å…³é”®ä¿¡æ¯
                self.review_nodes.append({
                    'review_id': review_id,
                    'paper_id': review.get('paper_id', ''),

                    # è¯„åˆ†ä¿¡æ¯
                    'overall_score': overall_score,
                    'overall_score_value': score_value,  # æ•°å€¼åŒ–çš„åˆ†æ•°
                    'confidence': review.get('confidence'),  # å¯èƒ½ä¸º None

                    # è¯„ä»·å†…å®¹
                    'paper_summary': review.get('paper_summary', '')[:500],  # é™åˆ¶é•¿åº¦
                    'strengths': review.get('strengths', ''),
                    'weaknesses': review.get('weaknesses', ''),
                    'comments': review.get('comments', ''),
                    'tldr': review.get('tldr', ''),

                    # è¯„ä»·ç»´åº¦
                    'contribution': review.get('contribution'),
                    'correctness': review.get('correctness', ''),
                    'clarity_quality_novelty_reproducibility': review.get('clarity_quality_novelty_reproducibility', ''),
                    'recommendation': review.get('recommendation', ''),

                    # å…ƒæ•°æ®
                    'reviewer': review.get('reviewer'),
                })

        print(f"  âœ“ åˆ›å»º {len(self.review_nodes)} ä¸ª Review èŠ‚ç‚¹")

    def _parse_overall_score(self, score_str: str) -> float:
        """å°†æ•´ä½“è¯„åˆ†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°å€¼ (0-1)

        åŸºç¡€æ˜ å°„ï¼Œåç»­åœ¨ review_stats ä¸­ä¼šç»¼åˆå¤šä¸ªç»´åº¦é‡æ–°è®¡ç®—
        """
        if not score_str:
            return 0.5

        score_str = score_str.lower().strip()

        # æŒ‰æ•°å­—å¤§å°æ˜ å°„ (1-10 çš„è¯„åˆ†)
        score_mapping = {
            '10': 1.0,
            '9': 0.95,
            '8': 0.85,
            '7': 0.7,
            '6': 0.6,
            '5': 0.5,
            '4': 0.4,
            '3': 0.3,
            '2': 0.2,
            '1': 0.1,
        }

        # å…ˆå°è¯•æå–æ•°å­—ï¼ˆä¾‹å¦‚ "6: marginally above..." -> 0.6ï¼‰
        import re
        numbers = re.findall(r'\d+', score_str)
        if numbers:
            for num_str in numbers:
                if num_str in score_mapping:
                    return score_mapping[num_str]

        # æ–‡æœ¬æ˜ å°„
        text_mapping = {
            'accept': 0.8,
            'reject': 0.2,
            'borderline': 0.5,
        }

        for key, value in text_mapping.items():
            if key in score_str:
                return value

        return 0.5  # é»˜è®¤ä¸­ç­‰è¯„åˆ†

    # ===================== å»ºç«‹å…³è” =====================

    def _link_paper_to_pattern(self, assignments: List[Dict]):
        """å»ºç«‹ Paper -> Pattern å…³è”"""
        print("\nğŸ”— å»ºç«‹ Paper -> Pattern å…³è”...")

        for paper_node in self.paper_nodes:
            cluster_id = paper_node.get('cluster_id', -1)
            if cluster_id != -1 and cluster_id in self.pattern_map:
                paper_node['pattern_id'] = self.pattern_map[cluster_id]

        linked_count = sum(1 for p in self.paper_nodes if p['pattern_id'])
        print(f"  âœ“ {linked_count}/{len(self.paper_nodes)} ç¯‡è®ºæ–‡å…³è”åˆ°Pattern")

    def _link_paper_to_idea(self):
        """å»ºç«‹ Paper -> Idea å…³è”"""
        print("\nğŸ”— å»ºç«‹ Paper -> Idea å…³è”...")

        for paper_node in self.paper_nodes:
            paper_id = paper_node['paper_id']
            idea_text = paper_node.get('idea', '')

            if idea_text:
                idea_hash = hashlib.md5(idea_text.encode()).hexdigest()[:16]
                if idea_hash in self.idea_map:
                    paper_node['idea_id'] = self.idea_map[idea_hash]

        linked_count = sum(1 for p in self.paper_nodes if p['idea_id'])
        print(f"  âœ“ {linked_count}/{len(self.paper_nodes)} ç¯‡è®ºæ–‡å…³è”åˆ°Idea")

    def _link_paper_to_domain(self):
        """å»ºç«‹ Paper -> Domain å…³è”"""
        print("\nğŸ”— å»ºç«‹ Paper -> Domain å…³è”...")

        for paper_node in self.paper_nodes:
            domain_name = paper_node.get('domain', '')

            if domain_name and domain_name in self.domain_map:
                paper_node['domain_id'] = self.domain_map[domain_name]

        linked_count = sum(1 for p in self.paper_nodes if p['domain_id'])
        print(f"  âœ“ {linked_count}/{len(self.paper_nodes)} ç¯‡è®ºæ–‡å…³è”åˆ°Domain")

    def _link_paper_to_review(self):
        """å»ºç«‹ Paper -> Review å…³è”å¹¶è¡¥å…… Paper çš„ Review è´¨é‡ä¿¡æ¯"""
        print("\nğŸ”— å»ºç«‹ Paper -> Review å…³è”...")

        # ä¸ºæ¯ä¸ª Paper æ”¶é›†å…³è”çš„ Review
        paper_review_stats = defaultdict(lambda: {
            'review_ids': [],
            'avg_score': 0.0,
            'review_count': 0,
            'highest_score': 0.0,
            'lowest_score': 1.0
        })

        for review_node in self.review_nodes:
            paper_id = review_node.get('paper_id', '')
            if not paper_id:
                continue

            score_value = review_node.get('overall_score_value', 0.5)
            paper_review_stats[paper_id]['review_ids'].append(review_node['review_id'])
            paper_review_stats[paper_id]['review_count'] += 1
            paper_review_stats[paper_id]['highest_score'] = max(
                paper_review_stats[paper_id]['highest_score'], score_value
            )
            paper_review_stats[paper_id]['lowest_score'] = min(
                paper_review_stats[paper_id]['lowest_score'], score_value
            )

        # è®¡ç®—å¹³å‡åˆ†å¹¶æ›´æ–° Paper èŠ‚ç‚¹
        for paper_id, stats in paper_review_stats.items():
            if stats['review_count'] > 0:
                review_ids = stats['review_ids']
                scores = [
                    next((r['overall_score_value'] for r in self.review_nodes if r['review_id'] == rid), 0.5)
                    for rid in review_ids
                ]
                stats['avg_score'] = sum(scores) / len(scores)

            # æ‰¾åˆ°å¯¹åº”çš„ Paper èŠ‚ç‚¹å¹¶è¡¥å……ä¿¡æ¯
            for paper_node in self.paper_nodes:
                if paper_node['paper_id'] == paper_id:
                    paper_node['review_ids'] = stats['review_ids']
                    paper_node['review_stats'] = {
                        'review_count': stats['review_count'],
                        'avg_score': stats['avg_score'],
                        'highest_score': stats['highest_score'],
                        'lowest_score': stats['lowest_score']
                    }
                    break

        linked_count = sum(1 for p in self.paper_nodes if p.get('review_ids'))
        print(f"  âœ“ {linked_count}/{len(self.paper_nodes)} ç¯‡è®ºæ–‡å…³è”åˆ°Review")
        print(f"  âœ“ å…±å…³è” {len(self.review_nodes)} æ¡Review")

    def _link_idea_to_pattern(self):
        """å»ºç«‹ Idea -> Pattern å…³è”ï¼ˆé€šè¿‡Paperä¸­è½¬ï¼‰"""
        print("\nğŸ”— å»ºç«‹ Idea -> Pattern å…³è”...")

        # ä¸ºæ¯ä¸ªIdeaæ”¶é›†pattern_ids
        idea_to_patterns = defaultdict(set)

        for paper_node in self.paper_nodes:
            idea_id = paper_node.get('idea_id', '')
            pattern_id = paper_node.get('pattern_id', '')

            if idea_id and pattern_id:
                idea_to_patterns[idea_id].add(pattern_id)

        # æ›´æ–°IdeaèŠ‚ç‚¹
        for idea_node in self.idea_nodes:
            idea_id = idea_node['idea_id']
            idea_node['pattern_ids'] = sorted(list(idea_to_patterns[idea_id]))

        total_links = sum(len(patterns) for patterns in idea_to_patterns.values())
        print(f"  âœ“ å…±å»ºç«‹ {total_links} ä¸ª Idea->Pattern è¿æ¥")

        if len(self.idea_nodes) > 0:
            avg_patterns = total_links / len(self.idea_nodes)
            print(f"  âœ“ å¹³å‡æ¯ä¸ª Idea å…³è” {avg_patterns:.1f} ä¸ª Pattern")

    # ===================== ä¿å­˜å’Œç»Ÿè®¡ =====================

    def _save_nodes(self):
        """ä¿å­˜æ‰€æœ‰èŠ‚ç‚¹åˆ°ç‹¬ç«‹æ–‡ä»¶"""
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

        with open(NODES_IDEA, 'w', encoding='utf-8') as f:
            json.dump(self.idea_nodes, f, ensure_ascii=False, indent=2)
        print(f"  âœ“ {NODES_IDEA}")

        with open(NODES_PATTERN, 'w', encoding='utf-8') as f:
            json.dump(self.pattern_nodes, f, ensure_ascii=False, indent=2)
        print(f"  âœ“ {NODES_PATTERN}")

        with open(NODES_DOMAIN, 'w', encoding='utf-8') as f:
            json.dump(self.domain_nodes, f, ensure_ascii=False, indent=2)
        print(f"  âœ“ {NODES_DOMAIN}")

        with open(NODES_PAPER, 'w', encoding='utf-8') as f:
            json.dump(self.paper_nodes, f, ensure_ascii=False, indent=2)
        print(f"  âœ“ {NODES_PAPER}")

        with open(NODES_REVIEW, 'w', encoding='utf-8') as f:
            json.dump(self.review_nodes, f, ensure_ascii=False, indent=2)
        print(f"  âœ“ {NODES_REVIEW}")

    def _update_stats(self):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats.ideas = len(self.idea_nodes)
        self.stats.patterns = len(self.pattern_nodes)
        self.stats.domains = len(self.domain_nodes)
        self.stats.papers = len(self.paper_nodes)
        self.stats.reviews = len(self.review_nodes)
        self.stats.total_nodes = (
            self.stats.ideas +
            self.stats.patterns +
            self.stats.domains +
            self.stats.papers +
            self.stats.reviews
        )

    def _save_stats(self):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with open(STATS_FILE, 'w', encoding='utf-8') as f:
            json.dump(asdict(self.stats), f, ensure_ascii=False, indent=2)
        print(f"  âœ“ {STATS_FILE}")

    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“Š çŸ¥è¯†å›¾è°±ç»Ÿè®¡ (V3 - ICLR):")
        print("-" * 40)
        print(f"  æ€»èŠ‚ç‚¹æ•°:  {self.stats.total_nodes}")
        print(f"  Idea:      {self.stats.ideas}")
        print(f"  Pattern:   {self.stats.patterns}")
        print(f"  Domain:    {self.stats.domains}")
        print(f"  Paper:     {self.stats.papers}")
        print("-" * 40)


def main():
    """ä¸»å‡½æ•°"""
    builder = KnowledgeGraphBuilderV3()
    builder.build()


if __name__ == '__main__':
    main()
