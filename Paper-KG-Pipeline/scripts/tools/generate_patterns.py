"""
åŸºäºskeleton+tricksèšç±»ç”Ÿæˆpatterns
è¾“å‡ºï¼š
1. patterns_structured.json - ç»“æ„åŒ–æ•°æ®
2. patterns_guide.txt - ç”¨æˆ·æŒ‡å¯¼æ–‡æ¡£
3. patterns_statistics.json - ç»Ÿè®¡æŠ¥å‘Š
"""

import glob
import json
import os
import time
from pathlib import Path
from collections import Counter, defaultdict
from typing import Dict, List, Tuple

import numpy as np
import requests
from sklearn.cluster import AgglomerativeClustering

SCRIPT_DIR = Path(__file__).resolve().parent
SCRIPTS_DIR = SCRIPT_DIR.parent
PROJECT_ROOT = SCRIPTS_DIR.parent

# LLMé…ç½® - åˆ‡æ¢è‡³ SiliconFlow (å‚è€ƒ 01_RAG_minimal_DEMO)
LLM_CONFIG = {
    "api_url": os.environ.get("LLM_API_URL", "https://api.siliconflow.cn/v1/chat/completions"),
    "auth_token": os.environ.get("SILICONFLOW_API_KEY", "sk-your-api-key-here"),
    "model": os.environ.get("LLM_MODEL", "Qwen/Qwen2.5-7B-Instruct")
}

# Embeddingé…ç½®
EMBED_CONFIG = {
    "api_url": os.environ.get("EMBED_API_URL", "https://api.siliconflow.cn/v1/embeddings"),
    "auth_token": os.environ.get("SILICONFLOW_API_KEY", "sk-your-api-key-here"),
    "model": os.environ.get("EMBED_MODEL", "Qwen/Qwen3-Embedding-4B")
}

print(f"ğŸš€ æ­£åœ¨å¯åŠ¨ Pattern ç”Ÿæˆ (ä½¿ç”¨ SiliconFlow)...")
print(f"   - LLM æ¨¡å‹: {LLM_CONFIG['model']}")
print(f"   - Embedding æ¨¡å‹: {EMBED_CONFIG['model']}")

if not LLM_CONFIG["auth_token"]:
    print("âš ï¸  è­¦å‘Š: æœªè®¾ç½® LLM_AUTH_TOKEN ç¯å¢ƒå˜é‡")
    print("   Patternç”ŸæˆåŠŸèƒ½å°†ä¸å¯ç”¨ï¼Œä½†å¯ä»¥ç›´æ¥ä½¿ç”¨å·²ç”Ÿæˆçš„ patterns_structured.json")

# èšç±»å‚æ•°
CLUSTER_PARAMS = {
    "distance_threshold": 0.35,  # è·ç¦»é˜ˆå€¼
    "min_cluster_size": 5,       # æœ€å°clusterå¤§å°
    "skeleton_weight": 0.4,      # skeletonæƒé‡
    "tricks_weight": 0.6,        # tricksæƒé‡
}


def get_embedding(text: str, max_retries: int = 3) -> List[float]:
    """è·å–æ–‡æœ¬çš„embeddingå‘é‡"""
    url = EMBED_CONFIG["api_url"]
    headers = {
        "Authorization": f"Bearer {EMBED_CONFIG['auth_token']}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": EMBED_CONFIG["model"],
        "input": text[:8000]
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            result = response.json()
            return result['data'][0]['embedding']
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
            else:
                print(f"  âŒ Embeddingå¤±è´¥: {e}")
                return [0.0] * 4096
    
    return [0.0] * 4096


def call_llm(prompt: str, max_retries: int = 3) -> str:
    """è°ƒç”¨LLM API"""
    url = LLM_CONFIG["api_url"]
    headers = {
        "Authorization": f"Bearer {LLM_CONFIG['auth_token']}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": LLM_CONFIG["model"],
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            result = response.json()
            return result['choices'][0]['message']['content'].strip()
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
            else:
                print(f"  âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
                return ""
    
    return ""


def load_all_papers(base_dir: str = None) -> List[Dict]:
    """åŠ è½½æ‰€æœ‰è®ºæ–‡æ•°æ®"""
    if base_dir is None:
        # é»˜è®¤ä½¿ç”¨Pipelineçš„dataç›®å½•
        base_dir = str(PROJECT_ROOT / "data")
    
    all_papers = []
    
    # éå†æ‰€æœ‰ä¼šè®®ç›®å½•
    for conf_dir in glob.glob(os.path.join(base_dir, "*")):
        if not os.path.isdir(conf_dir):
            continue
        
        conf_name = os.path.basename(conf_dir)
        files = glob.glob(os.path.join(conf_dir, "*_paper_node.json"))
        
        if not files:
            continue
        
        print(f"ğŸ“ åŠ è½½ {conf_name}: {len(files)} ç¯‡è®ºæ–‡")
        
        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    paper = json.load(f)
                    paper['conference'] = conf_name
                    paper['file_path'] = file_path
                    
                    # éªŒè¯å¿…è¦å­—æ®µ
                    if 'skeleton' in paper and 'tricks' in paper:
                        all_papers.append(paper)
            except Exception as e:
                print(f"  âš ï¸  è¯»å–å¤±è´¥ {file_path}: {e}")
    
    return all_papers


def build_pattern_embeddings(papers: List[Dict]) -> Tuple[np.ndarray, List[Dict]]:
    """æ„å»ºpatternçš„embeddingè¡¨ç¤ºï¼ˆskeleton + tricksèåˆï¼‰"""
    print(f"\nğŸ”¢ æ„å»ºpattern embeddings...")
    
    embeddings = []
    pattern_data = []
    
    for i, paper in enumerate(papers):
        if (i + 1) % 50 == 0:
            print(f"  è¿›åº¦: {i+1}/{len(papers)}")
        
        # 1. Skeletonæ–‡æœ¬
        skeleton = paper.get('skeleton', {})
        skeleton_text = " ".join([
            skeleton.get('problem_framing', ''),
            skeleton.get('gap_pattern', ''),
            skeleton.get('method_story', ''),
            skeleton.get('experiments_story', '')
        ])
        
        # 2. Tricksæ–‡æœ¬
        tricks = paper.get('tricks', [])
        tricks_text = " ".join([
            f"{t.get('name', '')}: {t.get('description', '')}"
            for t in tricks
        ])
        
        # 3. åˆ†åˆ«è®¡ç®—embedding
        skeleton_emb = get_embedding(skeleton_text.strip())
        time.sleep(0.1)
        tricks_emb = get_embedding(tricks_text.strip())
        time.sleep(0.1)
        
        # 4. åŠ æƒèåˆ
        skeleton_emb = np.array(skeleton_emb)
        tricks_emb = np.array(tricks_emb)
        pattern_emb = (CLUSTER_PARAMS['skeleton_weight'] * skeleton_emb + 
                      CLUSTER_PARAMS['tricks_weight'] * tricks_emb)
        
        embeddings.append(pattern_emb)
        pattern_data.append({
            'paper_id': paper.get('paper_id', ''),
            'title': paper.get('title', ''),
            'conference': paper.get('conference', ''),
            'skeleton': skeleton,
            'tricks': tricks,
            'skeleton_text': skeleton_text[:500],
            'tricks_text': tricks_text[:500]
        })
    
    return np.array(embeddings), pattern_data


def cluster_patterns(embeddings: np.ndarray) -> np.ndarray:
    """å¯¹patternsè¿›è¡Œå±‚æ¬¡èšç±»ï¼ˆè‡ªé€‚åº”è·ç¦»é˜ˆå€¼æ–¹å¼ï¼‰"""
    print(f"\nğŸ”„ å¼€å§‹èšç±»...")

    distance_threshold = CLUSTER_PARAMS['distance_threshold']
    print(f"  è·ç¦»é˜ˆå€¼: {distance_threshold}")

    clusterer = AgglomerativeClustering(
        n_clusters=None,                           # æ”¹ä¸ºåŠ¨æ€
        distance_threshold=distance_threshold,     # ä½¿ç”¨é˜ˆå€¼
        metric='cosine',
        linkage='average'
    )

    labels = clusterer.fit_predict(embeddings)

    # å¤„ç†å¯èƒ½çš„ -1 æ ‡ç­¾ï¼ˆæœªèšç±»çš„è®ºæ–‡ï¼‰
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    print(f"  âœ“ ç”Ÿæˆ {n_clusters} ä¸ª clusters")

    cluster_sizes = Counter(labels)
    print(f"  ğŸ“Š Cluster å¤§å°åˆ†å¸ƒ:")
    for cluster_id in sorted(cluster_sizes.keys()):
        if cluster_id != -1:
            print(f"    Cluster {cluster_id}: {cluster_sizes[cluster_id]} ç¯‡")

    if -1 in cluster_sizes:
        print(f"    æœªèšç±»: {cluster_sizes[-1]} ç¯‡")

    
    return labels


def analyze_cluster(cluster_papers: List[Dict], cluster_id: int) -> Dict:
    """åˆ†æå•ä¸ªclusterçš„ç‰¹å¾"""
    print(f"\n  ğŸ“Š åˆ†æ Cluster {cluster_id} ({len(cluster_papers)} ç¯‡)...")
    
    # 1. ç»Ÿè®¡é«˜é¢‘tricks
    trick_counter = Counter()
    trick_examples = defaultdict(list)
    
    for paper in cluster_papers:
        for trick in paper['tricks']:
            trick_name = trick.get('name', '')
            if not trick_name:
                continue
            
            trick_counter[trick_name] += 1
            trick_examples[trick_name].append({
                'paper_id': paper['paper_id'],
                'title': paper['title'],
                'description': trick.get('description', ''),
                'type': trick.get('type', ''),
                'purpose': trick.get('purpose', '')
            })
    
    # 2. é€‰æ‹©ä»£è¡¨æ€§skeletonä¾‹å­ï¼ˆå–æœ€å‰é¢3ä¸ªï¼‰
    skeleton_examples = []
    for paper in cluster_papers[:3]:
        skeleton_examples.append({
            'paper_id': paper['paper_id'],
            'title': paper['title'],
            'skeleton': paper['skeleton']
        })
    
    # 3. è®¡ç®—coherenceï¼ˆç±»å†…å¹³å‡ç›¸ä¼¼åº¦ï¼‰
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥é‡æ–°è®¡ç®—
    coherence = 0.75  # å ä½å€¼
    
    return {
        'cluster_id': cluster_id,
        'size': len(cluster_papers),
        'skeleton_examples': skeleton_examples,
        'trick_frequency': trick_counter.most_common(20),
        'trick_examples': trick_examples,
        'coherence': coherence,
        'all_papers': cluster_papers
    }


def generate_pattern_summary(cluster_analysis: Dict) -> str:
    """ç”Ÿæˆpatternæ€»ç»“ï¼ˆLLMï¼‰- ä½¿ç”¨clusterå®Œæ•´ä¿¡æ¯"""
    
    all_papers = cluster_analysis['all_papers']
    cluster_size = cluster_analysis['size']
    
    # ============================================================
    # 1. æ„å»ºå®Œæ•´çš„ skeleton ä¿¡æ¯ï¼ˆæ‰€æœ‰è®ºæ–‡ï¼Œå®Œæ•´å››ä¸ªç»´åº¦ï¼‰
    # ============================================================
    skeleton_info_list = []
    for i, paper in enumerate(all_papers[:8]):  # æœ€å¤šå–8ç¯‡é¿å…tokenè¿‡é•¿
        if not paper:
            continue
        skeleton = paper.get('skeleton') or {}
        title = paper.get('title') or 'Unknown Title'
        skeleton_info_list.append(f"""
è®ºæ–‡{i+1}ï¼šã€Š{str(title)[:60]}ã€‹
  - é—®é¢˜å®šä½ï¼š{skeleton.get('problem_framing', '')}
  - ç ”ç©¶ç¼ºå£ï¼š{skeleton.get('gap_pattern', '')}
  - æ–¹æ³•å™è¿°ï¼š{skeleton.get('method_story', '')}
  - å®éªŒè®¾è®¡ï¼š{skeleton.get('experiments_story', '')}""")
    
    skeleton_full_text = "\n".join(skeleton_info_list)
    
    # ============================================================
    # 2. æ„å»ºå®Œæ•´çš„ tricks ä¿¡æ¯ï¼ˆåŒ…å« name + description + purposeï¼‰
    # ============================================================
    tricks_info_list = []
    seen_tricks = set()  # å»é‡
    for paper in all_papers:
        if not paper:
            continue
        for trick in (paper.get('tricks') or []):
            if not trick:
                continue
            trick_name = trick.get('name', '')
            if trick_name and trick_name not in seen_tricks:
                seen_tricks.add(trick_name)
                tricks_info_list.append({
                    'name': trick_name,
                    'type': trick.get('type', ''),
                    'description': trick.get('description', ''),
                    'purpose': trick.get('purpose', ''),
                    'location': trick.get('location', '')
                })
    
    # æŒ‰é¢‘ç‡ç»Ÿè®¡ï¼Œå–å‰15ä¸ªé«˜é¢‘trickçš„å®Œæ•´ä¿¡æ¯
    trick_freq = cluster_analysis.get('trick_frequency', [])
    if not trick_freq:
        trick_freq = []
    
    top_trick_names = [item[0] for item in trick_freq[:15] if item and isinstance(item, (list, tuple))]

    tricks_full_list = []
    for trick_info in tricks_info_list:
        if trick_info['name'] in top_trick_names:
            tricks_full_list.append(
                f"- {trick_info['name']} [{trick_info['type']}]\n"
                f"    æè¿°ï¼š{trick_info['description']}\n"
                f"    ç›®çš„ï¼š{trick_info['purpose']}\n"
                f"    ä½ç½®ï¼š{trick_info['location']}"
            )
    
    tricks_full_text = "\n".join(tricks_full_list[:10])  # æœ€å¤š10ä¸ªå®Œæ•´trick
    
    # ============================================================
    # 3. ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºpromptå‚è€ƒï¼‰
    # ============================================================
    trick_stats = ", ".join([f"{name}({count}æ¬¡)" for name, count in trick_freq[:10]])
    
    # ============================================================
    # 4. æ„å»ºå®Œæ•´çš„ prompt
    # ============================================================
    prompt = f"""
ä½ æ˜¯NLPç ”ç©¶ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹clusterçš„å®Œæ•´ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªæŠ€æœ¯æ€§æ€»ç»“ã€‚

ã€Clusteræ¦‚è§ˆã€‘
- åŒ…å« {cluster_size} ç¯‡è®ºæ–‡
- é«˜é¢‘Tricksç»Ÿè®¡ï¼š{trick_stats}

ã€æ‰€æœ‰è®ºæ–‡çš„Skeletonä¿¡æ¯ã€‘
{skeleton_full_text}

ã€é«˜é¢‘Tricksè¯¦ç»†ä¿¡æ¯ã€‘
{tricks_full_text}

ã€ä»»åŠ¡è¦æ±‚ã€‘
è¯·åˆ†æä¸Šè¿°è®ºæ–‡çš„å…±åŒç‰¹å¾ï¼Œç”Ÿæˆä¸€ä¸ª 150-200 å­—çš„æŠ€æœ¯æ€§æ€»ç»“ã€‚

è¦æ±‚ï¼š
1. æ‰¾å‡ºè¿™äº›è®ºæ–‡åœ¨ç ”ç©¶é—®é¢˜ã€æ–¹æ³•è®¾è®¡ã€å®éªŒç­–ç•¥ä¸Šçš„å…±æ€§
2. ä¿ç•™å…·ä½“æŠ€æœ¯è¯ï¼ˆæ¨¡å‹åã€æ–¹æ³•åã€æ•°æ®é›†åï¼‰
3. çªå‡ºè¿™ç±»è®ºæ–‡çš„æ ¸å¿ƒå†™ä½œå¥—è·¯å’ŒæŠ€æœ¯ç‰¹å¾
4. é¿å…ç©ºæ³›çš„æè¿°ï¼Œè¦æœ‰å¯æ“ä½œçš„å…·ä½“ä¿¡æ¯

ã€è¾“å‡ºæ ¼å¼ã€‘ï¼ˆåˆ†ä¸‰æ®µï¼‰:
ç¬¬1æ®µï¼ˆ60å­—ï¼‰ï¼šæ ¸å¿ƒç ”ç©¶é—®é¢˜ä¸æŠ€æœ¯è·¯çº¿ - è¿™ç±»è®ºæ–‡ä¸»è¦è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Œç”¨ä»€ä¹ˆæ–¹æ³•
ç¬¬2æ®µï¼ˆ60å­—ï¼‰ï¼šå…³é”®æŠ€æœ¯ç»„åˆä¸å†™ä½œç­–ç•¥ - skeletonç‰¹ç‚¹ + å¸¸ç”¨tricksç»„åˆ
ç¬¬3æ®µï¼ˆ60å­—ï¼‰ï¼šé€‚ç”¨åœºæ™¯ä¸é¢„æœŸæ•ˆæœ - ä»€ä¹ˆä»»åŠ¡/æ•°æ®/ç›®æ ‡é€‚åˆè¿™ä¸ªå¥—è·¯

ã€ç¤ºä¾‹å¯¹æ¯”ã€‘:
âœ… å¥½: "é’ˆå¯¹è·¨è¯­è¨€ç†è§£ä»»åŠ¡ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œé‡‡ç”¨å¤šè¯­è¨€é¢„è®­ç»ƒ+é›¶æ ·æœ¬è¿ç§»çš„æŠ€æœ¯è·¯çº¿ã€‚
      skeletoné€šå¸¸ä»¥'ä½èµ„æºè¯­è¨€å›°å¢ƒ'å¼€ç¯‡ï¼Œé€šè¿‡'å¤šè¯­è¨€å¯¹é½ä¸è¶³'æŒ‡å‡ºgapï¼Œ
      æ–¹æ³•éƒ¨åˆ†é‡‡ç”¨'å¯¹æ¯”å­¦ä¹ +è¯­è¨€æ ‡ç­¾'ç»„åˆã€‚é«˜é¢‘ä½¿ç”¨æ¶ˆèå®éªŒéªŒè¯å„ç»„ä»¶è´¡çŒ®ï¼Œ
      é…åˆå¤šæ•°æ®é›†éªŒè¯å¢å¼ºæ³›åŒ–æ€§ã€‚é€‚ç”¨äºä½èµ„æºNLPä»»åŠ¡ï¼Œé¢„æœŸæå‡5-10%é›¶æ ·æœ¬æ€§èƒ½ã€‚"

âŒ å·®: "è¿™äº›è®ºæ–‡é‡‡ç”¨é—®é¢˜å¯¼å‘çš„å™äº‹ç»“æ„ï¼Œé¦–å…ˆæŒ‡å‡ºç°æœ‰æ–¹æ³•ä¸è¶³ï¼Œç„¶åæå‡ºåˆ›æ–°æ–¹æ³•..."

ç›´æ¥è¾“å‡ºæ€»ç»“:
"""
    
    summary = call_llm(prompt)
    
    # éªŒè¯è´¨é‡
    bad_patterns = ['å™äº‹ç»“æ„', 'å†™ä½œç»“æ„', 'é¦–å…ˆ...æ¥ç€', 'è¿™äº›è®ºæ–‡é‡‡ç”¨']
    if len(summary) < 100 or any(bad in summary for bad in bad_patterns):
        print(f"    âš ï¸  Summaryè´¨é‡ä¸ä½³ï¼Œå°è¯•é‡æ–°ç”Ÿæˆ...")
        summary = call_llm(prompt)  # å†è¯•ä¸€æ¬¡
    
    return summary


def extract_pattern_name(summary: str) -> str:
    """ä»summaryæå–ç®€çŸ­åç§°"""
    prompt = f"""
ä»ä»¥ä¸‹patternæ€»ç»“ä¸­æå–ä¸€ä¸ªç®€çŸ­çš„åç§°ï¼ˆä¸è¶…è¿‡12ä¸ªå­—ï¼‰ã€‚

æ€»ç»“ï¼š{summary}

è¦æ±‚ï¼š
- çªå‡ºæ ¸å¿ƒæŠ€æœ¯ç‰¹å¾
- ç®€æ´ã€ä¸“ä¸š
- ä¸è¦"XXç ”ç©¶"ã€"XXè®ºæ–‡"ç­‰åç¼€

ç›´æ¥è¾“å‡ºåç§°:
"""
    
    name = call_llm(prompt)
    return name.strip()


def generate_writing_guide_text(pattern_name: str, summary: str, skeleton_examples: List[Dict], 
                                common_tricks: List[Dict], cluster_size: int) -> str:
    """ç”Ÿæˆpatternçš„å†™ä½œæŒ‡å¯¼æ–‡æœ¬ï¼ˆç»™æ™ºèƒ½ä½“ç”¨ï¼‰"""
    
    guide_lines = []
    
    # 1. æ¨¡æ¿èšç„¦
    guide_lines.extend([
        f"å†™ä½œæ¨¡æ¿ï¼š{pattern_name}",
        "",
        "ã€æ¨¡æ¿èšç„¦ã€‘",
        summary,
        "",
    ])
    
    # 2. éª¨æ¶ç¤ºä¾‹
    guide_lines.extend([
        "ã€ä»£è¡¨æ€§è®ºæ–‡éª¨æ¶ç¤ºä¾‹ã€‘",
        f"è¯¥å¥—è·¯åŒ…å« {len(skeleton_examples)} ä¸ªä»£è¡¨æ€§è®ºæ–‡çš„éª¨æ¶ç¤ºä¾‹ï¼Œå¯ç›´è§‚ä½“ç°è¯¥æ¨¡å¼çš„è®ºæ–‡æ’°å†™æ¡†æ¶ï¼š",
        ""
    ])
    
    for i, sk in enumerate(skeleton_examples):
        guide_lines.extend([
            f"ç¤ºä¾‹ {i+1}ï¼šã€Š{sk['title']}ã€‹",
            f"  â€¢ é—®é¢˜å®šä½ï¼š{compress_text(sk['problem_framing'], 150)}",
            f"  â€¢ ç°æœ‰ç ”ç©¶ç¼ºå£ï¼š{compress_text(sk['gap_pattern'], 150)}",
            f"  â€¢ æ ¸å¿ƒæ–¹æ³•ï¼š{compress_text(sk['method_story'], 150)}",
            f"  â€¢ å®éªŒè®¾è®¡ï¼š{compress_text(sk['experiments_story'], 150)}",
            ""
        ])
    
    # 3. é«˜é¢‘æŠ€å·§
    guide_lines.extend([
        "ã€é«˜é¢‘ç ”ç©¶æŠ€å·§ã€‘",
        f"è¯¥æ¨¡å¼ä¸‹æœ‰ä»¥ä¸‹ {len(common_tricks)} ä¸ªé«˜é¢‘ä½¿ç”¨çš„ç ”ç©¶æŠ€å·§ï¼š",
        ""
    ])
    
    for i, trick in enumerate(common_tricks[:10]):
        example = trick['examples'][0] if trick['examples'] else {}
        guide_lines.extend([
            f"{i+1}. {trick['trick_name']}ï¼ˆä½¿ç”¨é¢‘ç‡ {trick['frequency']} æ¬¡ï¼Œå æ¯” {trick['percentage']}ï¼‰",
            f"   ç±»å‹ï¼š{example.get('type', 'é€šç”¨æŠ€å·§')}",
            f"   åº”ç”¨ï¼š{compress_text(example.get('description', ''), 150)}",
            ""
        ])
    
    return "\n".join(guide_lines)


def assemble_pattern(cluster_analysis: Dict, summary: str) -> Dict:
    """ç»„è£…æœ€ç»ˆçš„patternç»“æ„"""
    
    pattern_name = extract_pattern_name(summary)
    
    # Skeletonä¾‹å­
    skeleton_examples = [
        {
            'paper_id': sk['paper_id'],
            'title': sk['title'],
            'problem_framing': sk['skeleton'].get('problem_framing', ''),
            'gap_pattern': sk['skeleton'].get('gap_pattern', ''),
            'method_story': sk['skeleton'].get('method_story', ''),
            'experiments_story': sk['skeleton'].get('experiments_story', '')
        }
        for sk in cluster_analysis['skeleton_examples']
    ]
    
    # é«˜é¢‘Tricks
    common_tricks = [
        {
            'trick_name': name,
            'frequency': count,
            'percentage': f"{count/cluster_analysis['size']*100:.1f}%",
            'examples': cluster_analysis['trick_examples'][name][:3]
        }
        for name, count in cluster_analysis['trick_frequency'][:15]
    ]
    
    # ç”Ÿæˆå†™ä½œæŒ‡å¯¼æ–‡æœ¬
    writing_guide = generate_writing_guide_text(
        pattern_name, summary, skeleton_examples, common_tricks, 
        cluster_analysis['size']
    )
    
    return {
        'pattern_id': cluster_analysis['cluster_id'],
        'pattern_name': pattern_name,
        'pattern_summary': summary,
        
        # æ–°å¢ï¼šå®Œæ•´çš„å†™ä½œæŒ‡å¯¼æ–‡æœ¬ï¼ˆç»™æ™ºèƒ½ä½“ç”¨ï¼‰
        'writing_guide': writing_guide,
        
        # Skeletonä¾‹å­
        'skeleton_examples': skeleton_examples,
        
        # é«˜é¢‘Tricks
        'common_tricks': common_tricks,
        
        # å…ƒæ•°æ®
        'metadata': {
            'cluster_size': cluster_analysis['size'],
            'coherence_score': cluster_analysis['coherence'],
            'all_paper_ids': [p['paper_id'] for p in cluster_analysis['all_papers']]
        }
    }


def compress_text(text: str, max_len: int = 100) -> str:
    """å‹ç¼©æ–‡æœ¬åˆ°æŒ‡å®šé•¿åº¦"""
    if len(text) <= max_len:
        return text
    
    sentences = text.split('ã€‚')
    compressed = ""
    for sent in sentences:
        if len(compressed) + len(sent) + 1 <= max_len - 3:
            compressed += sent + "ã€‚"
        else:
            break
    
    return compressed if compressed else text[:max_len-3] + "..."


def generate_user_guide(patterns: List[Dict]) -> str:
    """ç”Ÿæˆç”¨æˆ·æŒ‡å¯¼æ–‡æ¡£"""
    print(f"\nğŸ“ ç”Ÿæˆç”¨æˆ·æŒ‡å¯¼æ–‡æ¡£...")
    
    # 1. æ•´ä½“ä»‹ç»
    total_papers = sum(p['metadata']['cluster_size'] for p in patterns)
    
    guide_lines = [
        "="*80,
        "NLP è®ºæ–‡å†™ä½œæ¨¡å¼ï¼ˆPatternsï¼‰æŒ‡å—",
        "="*80,
        "",
        "ã€æ•´ä½“ä»‹ç»ã€‘",
        "",
        f"æœ¬æŒ‡å—åŸºäº {total_papers} ç¯‡ NLP é¡¶ä¼šè®ºæ–‡çš„æ·±åº¦åˆ†æï¼Œé€šè¿‡å¯¹è®ºæ–‡éª¨æ¶ï¼ˆskeletonï¼‰å’Œ",
        f"ç ”ç©¶æŠ€å·§ï¼ˆtricksï¼‰çš„èšç±»ï¼ŒæŠ½è±¡å‡º {len(patterns)} ä¸ªå¯å¤ç”¨çš„å†™ä½œæ¨¡å¼ï¼ˆpatternsï¼‰ã€‚",
        "",
        "æ¯ä¸ª pattern åŒ…å«ï¼š",
        "  â€¢ æ¨¡å¼æ€»ç»“ï¼šè¯¥ç±»è®ºæ–‡çš„æ ¸å¿ƒæŠ€æœ¯è·¯çº¿å’Œå†™ä½œç‰¹ç‚¹",
        "  â€¢ éª¨æ¶ç¤ºä¾‹ï¼š2-3 ç¯‡ä»£è¡¨æ€§è®ºæ–‡çš„å®Œæ•´ç»“æ„æ¡†æ¶",
        "  â€¢ é«˜é¢‘æŠ€å·§ï¼šç»Ÿè®¡æ’åºçš„å¸¸ç”¨ç ”ç©¶æŠ€å·§åŠä½¿ç”¨é¢‘ç‡",
        "  â€¢ ä½¿ç”¨å»ºè®®ï¼šé’ˆå¯¹æ€§çš„å†™ä½œå’Œç ”ç©¶å»ºè®®",
        "",
        "ã€å¦‚ä½•ä½¿ç”¨æœ¬æŒ‡å—ã€‘",
        "",
        "1ï¸âƒ£  å®šä½ä½ çš„ç ”ç©¶ç±»å‹",
        "   - æµè§ˆå„ä¸ª pattern çš„ã€æ¨¡æ¿èšç„¦ã€‘éƒ¨åˆ†",
        "   - æ‰¾åˆ°ä¸ä½ ç ”ç©¶æœ€ç›¸å…³çš„ 1-2 ä¸ª patterns",
        "",
        "2ï¸âƒ£  å­¦ä¹ è®ºæ–‡ç»“æ„",
        "   - å‚è€ƒã€ä»£è¡¨æ€§è®ºæ–‡éª¨æ¶ç¤ºä¾‹ã€‘",
        "   - ç†è§£é—®é¢˜å®šä½ã€ç¼ºå£åˆ†æã€æ–¹æ³•å™è¿°ã€å®éªŒè®¾è®¡çš„é€»è¾‘",
        "",
        "3ï¸âƒ£  é€‰æ‹©åˆé€‚æŠ€å·§",
        "   - æŸ¥çœ‹ã€é«˜é¢‘ç ”ç©¶æŠ€å·§ã€‘åˆ—è¡¨",
        "   - æ ¹æ®ä½¿ç”¨é¢‘ç‡å’Œé€‚ç”¨åœºæ™¯ï¼Œé€‰æ‹© 3-5 ä¸ªæŠ€å·§åº”ç”¨åˆ°ä½ çš„è®ºæ–‡",
        "",
        "4ï¸âƒ£  è¿½æº¯å…·ä½“è®ºæ–‡",
        "   - é€šè¿‡ã€ç›¸å…³è®ºæ–‡ã€‘åˆ—è¡¨ï¼Œæ‰¾åˆ°å…·ä½“è®ºæ–‡æ·±åº¦å­¦ä¹ ",
        "",
        "ã€Pattern åˆ—è¡¨ã€‘",
        ""
    ]
    
    # 2. Patternç›®å½•
    for p in patterns:
        guide_lines.append(
            f"  Pattern #{p['pattern_id']:02d} - {p['pattern_name']} "
            f"({p['metadata']['cluster_size']}ç¯‡è®ºæ–‡)"
        )
    
    guide_lines.extend(["", "="*80, ""])
    
    # 3. æ¯ä¸ªPatternçš„è¯¦ç»†ä¿¡æ¯
    for pattern in patterns:
        guide_lines.extend([
            "="*80,
            f"å†™ä½œæ¨¡æ¿ #{pattern['pattern_id']}ï¼š{pattern['pattern_name']}",
            "="*80,
            "",
            "ã€æ¨¡æ¿èšç„¦ã€‘",
            pattern['pattern_summary'],
            "",
            "-"*80,
            "ã€ä»£è¡¨æ€§è®ºæ–‡éª¨æ¶ç¤ºä¾‹ã€‘",
            "-"*80,
            "",
            f"è¯¥å¥—è·¯åŒ…å« {len(pattern['skeleton_examples'])} ä¸ªä»£è¡¨æ€§è®ºæ–‡çš„éª¨æ¶ç¤ºä¾‹ï¼Œå¯ç›´è§‚ä½“ç°è¯¥æ¨¡å¼çš„è®ºæ–‡æ’°å†™æ¡†æ¶ï¼š",
            ""
        ])
        
        # Skeletonä¾‹å­
        for sk in pattern['skeleton_examples']:
            guide_lines.extend([
                f"ğŸ“„ è®ºæ–‡æ ‡é¢˜ï¼šã€Š{sk['title']}ã€‹",
                "",
                f"   â€¢ é—®é¢˜å®šä½ï¼š{compress_text(sk['problem_framing'], 120)}",
                "",
                f"   â€¢ ç°æœ‰ç ”ç©¶ç¼ºå£ï¼š{compress_text(sk['gap_pattern'], 120)}",
                "",
                f"   â€¢ æ ¸å¿ƒæ–¹æ³•ï¼š{compress_text(sk['method_story'], 120)}",
                "",
                f"   â€¢ å®éªŒè®¾è®¡ï¼š{compress_text(sk['experiments_story'], 120)}",
                ""
            ])
        
        # Tricks
        guide_lines.extend([
            "-"*80,
            "ã€é«˜é¢‘ç ”ç©¶æŠ€å·§ã€‘",
            "-"*80,
            "",
            f"è¯¥æ¨¡å¼ä¸‹æ¢³ç†å‡ºä»¥ä¸‹ {len(pattern['common_tricks'])} ä¸ªé«˜é¢‘ä½¿ç”¨çš„ç ”ç©¶æŠ€å·§ï¼Œå«ä½¿ç”¨é¢‘ç‡ã€å æ¯”åŠå…·ä½“ç¤ºä¾‹ï¼š",
            ""
        ])
        
        for i, trick in enumerate(pattern['common_tricks'][:10]):
            example = trick['examples'][0] if trick['examples'] else {}
            guide_lines.extend([
                f"{i+1}. {trick['trick_name']}",
                f"   - ä½¿ç”¨é¢‘ç‡ï¼š{trick['frequency']} æ¬¡ï¼ˆå æ¯” {trick['percentage']}ï¼‰",
                f"   - æŠ€å·§ç±»å‹ï¼š{example.get('type', 'é€šç”¨æŠ€å·§')}",
                f"   - å…¸å‹åº”ç”¨ï¼š{compress_text(example.get('description', ''), 150)}",
                ""
            ])
        
        # ç›¸å…³è®ºæ–‡
        paper_ids = pattern['metadata']['all_paper_ids']
        guide_lines.extend([
            "-"*80,
            f"ã€ç›¸å…³è®ºæ–‡ã€‘ï¼ˆå…± {len(paper_ids)} ç¯‡ï¼‰",
            "-"*80
        ])
        
        for i, paper_id in enumerate(paper_ids[:15]):
            guide_lines.append(f"  [{i+1}] {paper_id}")
        
        if len(paper_ids) > 15:
            guide_lines.append(f"  ... åŠå…¶ä»– {len(paper_ids) - 15} ç¯‡")
        
        guide_lines.extend(["", "="*80, ""])
    
    return "\n".join(guide_lines)


def generate_statistics(patterns: List[Dict]) -> Dict:
    """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
    print(f"\nğŸ“Š ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
    
    # å…¨å±€trickç»Ÿè®¡
    all_tricks = Counter()
    for pattern in patterns:
        for trick in pattern['common_tricks']:
            all_tricks[trick['trick_name']] += trick['frequency']
    
    # èšç±»è´¨é‡ç»Ÿè®¡
    cluster_sizes = [p['metadata']['cluster_size'] for p in patterns]
    
    return {
        'total_patterns': len(patterns),
        'total_papers': sum(cluster_sizes),
        'average_cluster_size': float(np.mean(cluster_sizes)),
        'median_cluster_size': float(np.median(cluster_sizes)),
        'cluster_size_distribution': {
            'min': min(cluster_sizes),
            'max': max(cluster_sizes),
            'std': float(np.std(cluster_sizes))
        },
        'top_global_tricks': [
            {'name': name, 'total_count': count}
            for name, count in all_tricks.most_common(20)
        ],
        'pattern_size_distribution': {
            'small (<10)': len([s for s in cluster_sizes if s < 10]),
            'medium (10-20)': len([s for s in cluster_sizes if 10 <= s < 20]),
            'large (20-30)': len([s for s in cluster_sizes if 20 <= s < 30]),
            'xlarge (>=30)': len([s for s in cluster_sizes if s >= 30])
        }
    }


def main():
    """ä¸»æµç¨‹"""
    print("="*80)
    print("åŸºäº Skeleton + Tricks èšç±»ç”Ÿæˆ Patterns")
    print("="*80)
    
    # 1. åŠ è½½è®ºæ–‡
    print("\nã€Step 1ã€‘åŠ è½½è®ºæ–‡æ•°æ®")
    papers = load_all_papers()
    print(f"âœ… å…±åŠ è½½ {len(papers)} ç¯‡è®ºæ–‡")
    
    # 2. æ„å»ºpattern embeddings
    print("\nã€Step 2ã€‘æ„å»ºpattern embeddings")
    embeddings, pattern_data = build_pattern_embeddings(papers)
    print(f"âœ… å®Œæˆ {len(embeddings)} ä¸ªpatternçš„embedding")
    
    # 3. èšç±»
    print("\nã€Step 3ã€‘èšç±»")
    labels = cluster_patterns(embeddings)
    
    # 4. åˆ†ææ¯ä¸ªclusterå¹¶ç”Ÿæˆpattern
    print("\nã€Step 4ã€‘ç”Ÿæˆpatterns")
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    patterns = []
    
    # åˆ›å»ºåŸå§‹ cluster_id -> æ–° pattern_id çš„æ˜ å°„
    old_to_new_id = {}  # old_cluster_id -> new_pattern_id (1-based)
    new_pattern_id = 1

    # åˆ›å»º paper_id -> pattern_id çš„æ˜ å°„
    paper_to_pattern = {}  # paper_id -> new_pattern_id

    for cluster_id in sorted(set(labels)):
        if cluster_id == -1:  # è·³è¿‡æœªèšç±»çš„è®ºæ–‡
            print(f"  âš ï¸  Cluster -1: {(labels == -1).sum()}ç¯‡ (æœªèšç±»ï¼Œå·²è·³è¿‡)")
            continue

        cluster_indices = [i for i in range(len(labels)) if labels[i] == cluster_id]

        if len(cluster_indices) < CLUSTER_PARAMS['min_cluster_size']:
            print(f"  âš ï¸  Cluster {cluster_id}: {len(cluster_indices)}ç¯‡ (è¿‡å°ï¼Œè·³è¿‡)")
            continue
        
        cluster_papers = [pattern_data[i] for i in cluster_indices]
        
        # å»ºç«‹æ—§ ID åˆ°æ–° ID çš„æ˜ å°„
        old_to_new_id[cluster_id] = new_pattern_id
        
        # è®°å½•è¿™ä¸ª cluster ä¸­æ‰€æœ‰ paper çš„æ˜ å°„
        for idx in cluster_indices:
            paper_id = pattern_data[idx]['paper_id']
            paper_to_pattern[paper_id] = new_pattern_id

        # åˆ†æclusterï¼ˆä½¿ç”¨æ–°çš„ pattern_idï¼‰
        cluster_analysis = analyze_cluster(cluster_papers, new_pattern_id)

        # ç”Ÿæˆsummary
        summary = generate_pattern_summary(cluster_analysis)
        print(f"    Pattern {new_pattern_id}: {summary[:80]}...")
        
        # ç»„è£…pattern
        pattern = assemble_pattern(cluster_analysis, summary)
        patterns.append(pattern)
    
        new_pattern_id += 1

    print(f"\nâœ… å…±ç”Ÿæˆ {len(patterns)} ä¸ªpatterns")
    
    # 5. ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
    print("\nã€Step 5ã€‘ç”Ÿæˆè¾“å‡ºæ–‡ä»¶")
    
    # è·å–è¾“å‡ºç›®å½•
    output_dir = str(PROJECT_ROOT / "output")
    os.makedirs(output_dir, exist_ok=True)
    
    # 5.1 ç»“æ„åŒ–JSON
    with open(os.path.join(output_dir, 'patterns_structured.json'), 'w', encoding='utf-8') as f:
        json.dump(patterns, f, ensure_ascii=False, indent=2)
    print("  âœ… patterns_structured.json")
    
    # 5.2 ä¿å­˜ Paper â†’ Pattern æ˜ å°„ï¼ˆä¾›åç»­ä½¿ç”¨ï¼‰
    with open(os.path.join(output_dir, 'paper_to_pattern.json'), 'w', encoding='utf-8') as f:
        json.dump(paper_to_pattern, f, ensure_ascii=False, indent=2)
    print(f"  âœ… paper_to_pattern.json ({len(paper_to_pattern)} ç¯‡è®ºæ–‡)")

    # 5.2 ç”¨æˆ·æŒ‡å¯¼
    guide_text = generate_user_guide(patterns)
    with open(os.path.join(output_dir, 'patterns_guide.txt'), 'w', encoding='utf-8') as f:
        f.write(guide_text)
    print("  âœ… patterns_guide.txt")
    
    # 5.3 ç»Ÿè®¡æŠ¥å‘Š
    statistics = generate_statistics(patterns)
    with open(os.path.join(output_dir, 'patterns_statistics.json'), 'w', encoding='utf-8') as f:
        json.dump(statistics, f, ensure_ascii=False, indent=2)
    print("  âœ… patterns_statistics.json")
    
    print("\n" + "="*80)
    print("ğŸ‰ å®Œæˆï¼")
    print("="*80)
    print(f"\nç”Ÿæˆäº† {len(patterns)} ä¸ªpatternsï¼Œè¦†ç›– {statistics['total_papers']} ç¯‡è®ºæ–‡")
    print(f"å¹³å‡æ¯ä¸ªpatternåŒ…å« {statistics['average_cluster_size']:.1f} ç¯‡è®ºæ–‡")
    print(f"\nè¾“å‡ºæ–‡ä»¶ï¼š")
    print(f"  1. patterns_structured.json  - ç»“æ„åŒ–æ•°æ®ï¼ˆç»™ç¨‹åºç”¨ï¼‰")
    print(f"  2. patterns_guide.txt        - ç”¨æˆ·æŒ‡å¯¼æ–‡æ¡£ï¼ˆç»™äººçœ‹ï¼‰")
    print(f"  3. patterns_statistics.json  - ç»Ÿè®¡æŠ¥å‘Š")


if __name__ == '__main__':
    main()
