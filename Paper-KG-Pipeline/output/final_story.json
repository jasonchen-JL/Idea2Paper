{
  "title": "Modality Reliability Routing: Overcoming Text-Dominant Bias in Audio-Text Large Models via Dynamic Composition",
  "abstract": "Audio-text large models frequently suffer from text-dominant bias, where the model excessively relies on textual modality, leading to critical errors in emotion recognition when text is misleading or conflicting. We reframe robust multimodal emotion recognition as a dynamic selective reasoning challenge, shifting from static fusion to adaptive composition based on modality trustworthiness. We propose a modular routing framework that quantifies 'text interference' using energy and temporal evidence to assess modality reliability. By training lightweight adapters on these interference signals, our system dynamically routes inputs to either a Joint (Audio+Text) expert or an Audio-only expert during inference. This approach effectively mitigates the impact of adversarial and irrelevant text, significantly enhancing robustness and generalization across faithful, adversarial, and irrelevant settings without the prohibitive cost of retraining monolithic architectures.",
  "problem_framing": "We reframe multimodal emotion recognition from a static fusion problem to a dynamic selective reasoning challenge under modality conflict. Traditional approaches assume consistent alignment between audio and text, yet real-world scenarios are fraught with misleading, adversarial, or irrelevant textual information that exposes text-dominant bias. By shifting the focus to the inherent reliability of each modality, we aim to construct a system that actively distrusts text when evidence suggests interference, rather than passively aggregating conflicting signals. This perspective transforms the objective from simply maximizing joint accuracy to optimizing trustworthiness-aware decision-making.",
  "gap_pattern": "Current audio-text large models fail to address text-dominant bias because they rely on rigid, monolithic fusion architectures that implicitly assume modality harmony. These methods lack the mechanism to discern 'text interference' or quantify modality trustworthiness, leading them to blindly follow misleading text in adversarial settings. This gap stems from treating fusion as a fixed parameter update process rather than a reliability-based decision, resulting in fragile systems that collapse when cross-modal consistency is broken. Existing solutions often ignore the temporal and energy signatures that signal conflicts, missing the opportunity for dynamic correction.",
  "solution": "Our solution introduces a modular routing and fusion framework grounded in modality reliability. We operationalize the detection of text-dominant bias by constructing a 'text interference' representation characterized by energy and temporal evidence, transforming abstract conflict into quantifiable signals. Lightweight gating mechanisms and adapters are trained to interpret these signals, functioning as gatekeepers that assess the trustworthiness of the textual modality. During inference, this mechanism enables dynamic expert selection, routing the input to a Joint expert when modalities align, or an Audio-only expert when text interference is detected. This transforms the fusion strategy from static joint processing to adaptive, selective composition, effectively mitigating bias while maintaining computational efficiency.",
  "method_skeleton": "Step 1: Construct a 'text interference' representation by extracting energy and temporal evidence features from the audio-text input to quantify potential conflicts; Step 2: Train lightweight gating mechanisms and adapters using these interference features to predict modality reliability scores; Step 3: Implement a dynamic routing module that selects between a Joint (Audio+Text) expert and an Audio-only expert based on the predicted reliability scores; Step 4: Optimize the framework using a loss function that reinforces correct expert selection in faithful, adversarial, and irrelevant settings.",
  "innovation_claims": [
    "Transforms the understanding of text-dominant bias from a static data imbalance issue to a dynamic inference-time reliability assessment problem by introducing 'text interference' quantification via energy and temporal evidence.",
    "Reframes multimodal fusion from monolithic joint processing to modular selective composition by implementing a reliability-based routing mechanism that adapts to modality conflict.",
    "Shifts robust emotion recognition from expensive full-model fine-tuning to efficient expert selection by utilizing lightweight adapters, ensuring stability against adversarial text without retraining the entire backbone."
  ],
  "experiments_plan": "Evaluation uses IEMOCAP and MELD datasets augmented with adversarial/irrelevant samples to simulate conflict. Metrics include Accuracy, Weighted Accuracy (WA), Unweighted Accuracy (UA), and F1-score. We compare against standard audio-text LMMs and static fusion baselines. Ablation studies validate the impact of the 'text interference' representation, the gating mechanism, and the dynamic routing strategy."
}