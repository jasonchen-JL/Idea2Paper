{
  "title": "Overcoming Text-Dominant Bias via Modality Reliability-based Dynamic Routing",
  "abstract": "Audio-text large models frequently exhibit text-dominant bias, where textual semantics override acoustic evidence, leading to failures in adversarial or irrelevant settings. We propose a novel framework centered on Modality Reliability to mitigate this bias and achieve robust sentiment recognition. By constructing fine-grained Text Interference representations that quantify energy and temporal mismatches, we train lightweight gating adapters to assess modality trustworthiness. This mechanism enables Dynamic Routing, automatically selecting either a Joint or Audio expert during inference. Our approach transforms the fusion process from static aggregation to adaptive reliability assessment, ensuring the model remains faithful to acoustic cues when text is misleading.",
  "problem_framing": "We reframe the challenge of multimodal sentiment analysis from static feature fusion to a dynamic reliability assessment problem. Current models struggle because they implicitly assume text is the dominant truth-teller, leading to systematic failures in adversarial settings where textual content contradicts acoustic reality. Our approach transforms this perspective by treating modality selection as a function of measured reliability, rather than relying on fixed architectural priors that inevitably favor text.",
  "gap_pattern": "Existing fusion methods for audio-text models fail to address text-dominant bias because they lack fine-grained mechanisms to detect 'text interference.' They rely on indiscriminate attention or concatenation, assuming uniform reliability across faithful, adversarial, and irrelevant contexts. This paradigm fails to distinguish between helpful text and misleading signals, resulting in models that are easily hijacked by textual content, rendering the audio modality effectively dormant in critical inference scenarios.",
  "solution": "We introduce a Modality Reliability-based Routing framework that shifts the paradigm from passive fusion to active expert selection. By implementing fine-grained Text Interference signals—derived from energy and temporal evidence—we calculate a dynamic reliability score for the textual modality. This score drives a constrained gating mechanism that routes inputs to a Joint expert when modalities are consistent, or an Audio expert when text interference is detected. This ensures that acoustic evidence acts as a robust anchor, preventing the model from being misled by adversarial text.",
  "method_skeleton": "Construct Text Interference Representations by calculating energy and temporal evidence mismatches between audio and text streams to quantify local modality reliability; Train a Lightweight Gating Adapter using a constrained optimization objective that learns to map interference signals to routing weights; Implement a Dynamic Routing Mechanism at inference that selects the Joint or Audio expert based on the thresholded reliability score from the adapter.",
  "innovation_claims": [
    "Transform multimodal fusion from static aggregation to adaptive routing by quantifying text interference, enabling the model to dynamically suppress misleading text in favor of audio evidence.",
    "Shift sentiment recognition from text-reliant to reliability-aware processing by introducing energy-based modality scoring, ensuring robustness across faithful, adversarial, and irrelevant settings.",
    "Reframe the handling of irrelevant text from noise filtering to expert-based selection, utilizing lightweight adapters to switch between integrated and audio-only processing without sacrificing model capacity."
  ],
  "experiments_plan": "We evaluate on standard multimodal sentiment datasets (e.g., MOSI, IEMOCAP) restructured into faithful, adversarial, and irrelevant splits. We compare accuracy and robustness against state-of-the-art fusion baselines to demonstrate the effectiveness of our reliability-based routing."
}