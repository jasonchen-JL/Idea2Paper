{
  "title": "Mitigating Text-Dominance Bias in Audio-Text Large Models via Modality Reliability and Dynamic Routing",
  "abstract": "Audio-Text Large Models (ATLMs) often suffer from text-dominance bias, over-relying on textual cues at the expense of acoustic information. This limits robustness in real-world scenarios where text may be adversarial or irrelevant. We propose a reliability-based routing framework to address this. By constructing a 'text interference' representation using energy and temporal evidence, our model quantifies the trustworthiness of textual input. We then train lightweight gating mechanisms to dynamically route inputs to either a Joint expert or an Audio expert during inference. This modular approach ensures robust emotion recognition by suppressing misleading text and prioritizing reliable modalities, significantly improving performance across faithful, adversarial, and irrelevant settings.",
  "problem_framing": "We reframe the challenge of multimodal emotion recognition from a simple feature aggregation problem to a conflict resolution problem characterized by text-dominance bias. While existing models assume consistent and cooperative inputs, real-world data often contains adversarial or irrelevant text that misleads the model. This static view of multimodal integration fails to account for the varying reliability of modalities, causing models to ignore critical acoustic cues when text is deceptive.",
  "gap_pattern": "Current ATLMs fail to handle text-dominance bias because they rely on static fusion strategies that indiscriminately combine audio and text features. These approaches lack the mechanism to detect when text is interfering with the true emotional signal. Consequently, they suffer from significant performance degradation in adversarial settings, as they cannot dynamically adapt their reasoning process to exclude unreliable textual information.",
  "solution": "We propose a modular composition strategy that transforms multimodal fusion into a dynamic reasoning step based on modality reliability. Drawing on principles of modular composition, we introduce a 'text interference' representation that captures energy and temporal evidence of conflict. This intermediate representation guides lightweight gating mechanisms, effectively training the model to assess modality trustworthiness. During inference, the framework dynamically routes the input to the most appropriate expert—either a Joint expert for consistent inputs or an Audio expert when text interference is detected—thereby ensuring robust emotion recognition.",
  "method_skeleton": "Construct a 'text interference' representation utilizing energy and temporal evidence to quantify multimodal conflict; train lightweight gating mechanisms and adapters to learn modality trustworthiness without full retraining; implement dynamic routing logic to select between Joint or Audio experts based on the interference representation during inference.",
  "innovation_claims": [
    "Transform multimodal fusion from static aggregation to dynamic reliability-based routing by introducing modality trustworthiness as a decision criterion, enabling adaptive expert selection for robust reasoning.",
    "Reframe text-dominance bias mitigation as a quantifiable interference problem by modeling 'text interference' through energy and temporal evidence, providing an explicit signal for conflict detection.",
    "Enhance the generalization capacity of Audio-Text Large Models by integrating lightweight adapters that route to audio-only experts under adversarial conditions, effectively neutralizing misleading textual cues."
  ],
  "experiments_plan": "Evaluation on CMU-MOSEI and IEMOCAP datasets using faithful, adversarial, and irrelevant text splits. Metrics include Accuracy and F1-score. Baselines comprise vanilla ATLMs and static fusion models. Ablation studies analyze the impact of the interference representation and routing logic, demonstrating robustness improvements specifically in adversarial settings."
}