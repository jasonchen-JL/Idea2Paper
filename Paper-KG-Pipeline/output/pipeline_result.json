{
  "user_idea": "我们研究音频-文本大模型的文本主导偏见：在 faithful/adversarial/irrelevant 三类设定下，提出基于模态可信度的路由/融合框架。通过构造“文本干扰”表示能量与时序证据，训练轻量门控/适配器，在推理时选择 Joint 或 Audio 专家，实现对抗文本误导下的稳健情绪识别",
  "success": true,
  "iterations": 1,
  "selected_patterns": {
    "stability": [
      "pattern_74",
      "pattern_6",
      "pattern_112",
      "pattern_51",
      "pattern_115"
    ],
    "novelty": [
      "pattern_113",
      "pattern_7",
      "pattern_89",
      "pattern_106",
      "pattern_112"
    ],
    "domain_distance": [
      "pattern_112",
      "pattern_113",
      "pattern_106",
      "pattern_51",
      "pattern_89"
    ]
  },
  "final_story": {
    "title": "Modality Reliability Routing: Overcoming Text-Dominant Bias in Audio-Text Large Models via Dynamic Composition",
    "abstract": "Audio-text large models frequently suffer from text-dominant bias, where the model excessively relies on textual modality, leading to critical errors in emotion recognition when text is misleading or conflicting. We reframe robust multimodal emotion recognition as a dynamic selective reasoning challenge, shifting from static fusion to adaptive composition based on modality trustworthiness. We propose a modular routing framework that quantifies 'text interference' using energy and temporal evidence to assess modality reliability. By training lightweight adapters on these interference signals, our system dynamically routes inputs to either a Joint (Audio+Text) expert or an Audio-only expert during inference. This approach effectively mitigates the impact of adversarial and irrelevant text, significantly enhancing robustness and generalization across faithful, adversarial, and irrelevant settings without the prohibitive cost of retraining monolithic architectures.",
    "problem_framing": "We reframe multimodal emotion recognition from a static fusion problem to a dynamic selective reasoning challenge under modality conflict. Traditional approaches assume consistent alignment between audio and text, yet real-world scenarios are fraught with misleading, adversarial, or irrelevant textual information that exposes text-dominant bias. By shifting the focus to the inherent reliability of each modality, we aim to construct a system that actively distrusts text when evidence suggests interference, rather than passively aggregating conflicting signals. This perspective transforms the objective from simply maximizing joint accuracy to optimizing trustworthiness-aware decision-making.",
    "gap_pattern": "Current audio-text large models fail to address text-dominant bias because they rely on rigid, monolithic fusion architectures that implicitly assume modality harmony. These methods lack the mechanism to discern 'text interference' or quantify modality trustworthiness, leading them to blindly follow misleading text in adversarial settings. This gap stems from treating fusion as a fixed parameter update process rather than a reliability-based decision, resulting in fragile systems that collapse when cross-modal consistency is broken. Existing solutions often ignore the temporal and energy signatures that signal conflicts, missing the opportunity for dynamic correction.",
    "solution": "Our solution introduces a modular routing and fusion framework grounded in modality reliability. We operationalize the detection of text-dominant bias by constructing a 'text interference' representation characterized by energy and temporal evidence, transforming abstract conflict into quantifiable signals. Lightweight gating mechanisms and adapters are trained to interpret these signals, functioning as gatekeepers that assess the trustworthiness of the textual modality. During inference, this mechanism enables dynamic expert selection, routing the input to a Joint expert when modalities align, or an Audio-only expert when text interference is detected. This transforms the fusion strategy from static joint processing to adaptive, selective composition, effectively mitigating bias while maintaining computational efficiency.",
    "method_skeleton": "Step 1: Construct a 'text interference' representation by extracting energy and temporal evidence features from the audio-text input to quantify potential conflicts; Step 2: Train lightweight gating mechanisms and adapters using these interference features to predict modality reliability scores; Step 3: Implement a dynamic routing module that selects between a Joint (Audio+Text) expert and an Audio-only expert based on the predicted reliability scores; Step 4: Optimize the framework using a loss function that reinforces correct expert selection in faithful, adversarial, and irrelevant settings.",
    "innovation_claims": [
      "Transforms the understanding of text-dominant bias from a static data imbalance issue to a dynamic inference-time reliability assessment problem by introducing 'text interference' quantification via energy and temporal evidence.",
      "Reframes multimodal fusion from monolithic joint processing to modular selective composition by implementing a reliability-based routing mechanism that adapts to modality conflict.",
      "Shifts robust emotion recognition from expensive full-model fine-tuning to efficient expert selection by utilizing lightweight adapters, ensuring stability against adversarial text without retraining the entire backbone."
    ],
    "experiments_plan": "Evaluation uses IEMOCAP and MELD datasets augmented with adversarial/irrelevant samples to simulate conflict. Metrics include Accuracy, Weighted Accuracy (WA), Unweighted Accuracy (UA), and F1-score. We compare against standard audio-text LMMs and static fusion baselines. Ablation studies validate the impact of the 'text interference' representation, the gating mechanism, and the dynamic routing strategy."
  },
  "review_history": [
    {
      "pass": true,
      "avg_score": 9.999999999999831,
      "reviews": [
        {
          "reviewer": "Reviewer A",
          "role": "Methodology",
          "score": 9.999999999999831,
          "feedback": "Blind comparisons vs 13 anchors. Loss=1.0787, AvgStrength=3.00. CoachPriority: innovation_claims, method_skeleton, experiments_plan."
        },
        {
          "reviewer": "Reviewer B",
          "role": "Novelty",
          "score": 9.999999999999831,
          "feedback": "Blind comparisons vs 13 anchors. Loss=2.6654, AvgStrength=3.00. CoachPriority: innovation_claims, method_skeleton, experiments_plan."
        },
        {
          "reviewer": "Reviewer C",
          "role": "Storyteller",
          "score": 9.999999999999831,
          "feedback": "Blind comparisons vs 13 anchors. Loss=1.0787, AvgStrength=3.00. CoachPriority: innovation_claims, method_skeleton, experiments_plan."
        }
      ],
      "main_issue": "stability",
      "suggestions": [
        "从stability维度选择稳健Pattern",
        "注入成熟方法增强鲁棒性"
      ],
      "audit": {
        "pattern_id": "pattern_74",
        "anchors": [
          {
            "anchor_id": "A1",
            "paper_id": "79ZkWgY2FI",
            "score10": 5.787999999999999,
            "weight": 0.6399140961528769
          },
          {
            "anchor_id": "A2",
            "paper_id": "4es2oO9tw1",
            "score10": 5.967999999999999,
            "weight": 0.7928139244371923
          },
          {
            "anchor_id": "A3",
            "paper_id": "i0zzO7Hslk",
            "score10": 6.004,
            "weight": 1.0417206216442176
          },
          {
            "anchor_id": "A4",
            "paper_id": "gU4ZgQNsOC",
            "score10": 6.039999999999999,
            "weight": 0.9357197165314532
          },
          {
            "anchor_id": "A5",
            "paper_id": "BTKAeLqLMw",
            "score10": 6.175,
            "weight": 0.7121406692186284
          },
          {
            "anchor_id": "A6",
            "paper_id": "OQqNieeivq",
            "score10": 6.22,
            "weight": 0.8610221898474837
          },
          {
            "anchor_id": "A7",
            "paper_id": "3E8YNv1HjU",
            "score10": 6.292000000000001,
            "weight": 0.7928139244371923
          },
          {
            "anchor_id": "A8",
            "paper_id": "csbf1p8xUq",
            "score10": 6.49,
            "weight": 0.8470725854916313
          },
          {
            "anchor_id": "A9",
            "paper_id": "wg1PCg3CUP",
            "score10": 6.724,
            "weight": 1.5184402281593683
          },
          {
            "anchor_id": "A10",
            "paper_id": "-Aw0rrrPUF",
            "score10": 7.624,
            "weight": 0.6503664135129055
          },
          {
            "anchor_id": "A11",
            "paper_id": "gUL6zYN4Uaf",
            "score10": 5.788,
            "weight": 0.5752036819351701
          },
          {
            "anchor_id": "A12",
            "paper_id": "9y0HFvaAYD6",
            "score10": 7.489,
            "weight": 0.7082053238055552
          },
          {
            "anchor_id": "A13",
            "paper_id": "lq62uWRJjiY",
            "score10": 7.0878571428571435,
            "weight": 0.8075501132737226
          }
        ],
        "anchors_rounds": [
          [
            {
              "anchor_id": "A1",
              "paper_id": "79ZkWgY2FI",
              "score10": 5.787999999999999,
              "weight": 0.6399140961528769
            },
            {
              "anchor_id": "A2",
              "paper_id": "4es2oO9tw1",
              "score10": 5.967999999999999,
              "weight": 0.7928139244371923
            },
            {
              "anchor_id": "A3",
              "paper_id": "i0zzO7Hslk",
              "score10": 6.004,
              "weight": 1.0417206216442176
            },
            {
              "anchor_id": "A4",
              "paper_id": "gU4ZgQNsOC",
              "score10": 6.039999999999999,
              "weight": 0.9357197165314532
            },
            {
              "anchor_id": "A5",
              "paper_id": "BTKAeLqLMw",
              "score10": 6.175,
              "weight": 0.7121406692186284
            },
            {
              "anchor_id": "A6",
              "paper_id": "OQqNieeivq",
              "score10": 6.22,
              "weight": 0.8610221898474837
            },
            {
              "anchor_id": "A7",
              "paper_id": "3E8YNv1HjU",
              "score10": 6.292000000000001,
              "weight": 0.7928139244371923
            },
            {
              "anchor_id": "A8",
              "paper_id": "csbf1p8xUq",
              "score10": 6.49,
              "weight": 0.8470725854916313
            },
            {
              "anchor_id": "A9",
              "paper_id": "wg1PCg3CUP",
              "score10": 6.724,
              "weight": 1.5184402281593683
            },
            {
              "anchor_id": "A10",
              "paper_id": "-Aw0rrrPUF",
              "score10": 7.624,
              "weight": 0.6503664135129055
            },
            {
              "anchor_id": "A11",
              "paper_id": "gUL6zYN4Uaf",
              "score10": 5.788,
              "weight": 0.5752036819351701
            }
          ],
          [
            {
              "paper_id": "9y0HFvaAYD6",
              "pattern_id": "pattern_74",
              "score10": 7.489,
              "review_count": 5,
              "dispersion10": 1.5300000000000002,
              "weight": 0.7082053238055552
            },
            {
              "paper_id": "lq62uWRJjiY",
              "pattern_id": "pattern_74",
              "score10": 7.0878571428571435,
              "review_count": 7,
              "dispersion10": 1.5750000000000004,
              "weight": 0.8075501132737226
            }
          ]
        ],
        "role_details": {
          "Methodology": {
            "comparisons": [
              {
                "anchor_id": "A1",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story provides detailed 4-step methodology with concrete experimental plan, while A1 lacks experimental details."
              },
              {
                "anchor_id": "A2",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story has specific technical implementation steps and evaluation metrics, while A2's experiments plan is unknown."
              },
              {
                "anchor_id": "A3",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story outlines clear methodology with ablation studies, while A3 provides no experimental details."
              },
              {
                "anchor_id": "A4",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story has concrete implementation steps and evaluation framework, while A4's experiments plan is unknown."
              },
              {
                "anchor_id": "A5",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story provides detailed methodology with specific datasets and metrics, while A5 lacks experimental details."
              },
              {
                "anchor_id": "A6",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story has comprehensive methodology with evaluation plan, while A6's experiments plan is unknown."
              },
              {
                "anchor_id": "A7",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story provides concrete implementation steps and evaluation metrics, while A7 lacks experimental details."
              },
              {
                "anchor_id": "A8",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story has detailed methodology with specific evaluation plan, while A8's experiments plan is unknown."
              },
              {
                "anchor_id": "A9",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story provides concrete implementation steps and evaluation framework, while A9 lacks experimental details."
              },
              {
                "anchor_id": "A10",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story has detailed methodology with specific evaluation metrics, while A10's experiments plan is unknown."
              },
              {
                "anchor_id": "A11",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story provides concrete implementation steps and evaluation plan, while A11 lacks experimental details."
              },
              {
                "anchor_id": "A12",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story has comprehensive methodology with specific evaluation metrics, while A12's experiments plan is unknown."
              },
              {
                "anchor_id": "A13",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story provides detailed methodology with concrete experimental plan, while A13 lacks experimental details."
              }
            ],
            "loss": 1.0786604099064554,
            "avg_strength": 3.0,
            "monotonic_violations": 0,
            "ci_low": 8.939999999999854,
            "ci_high": 9.999999999999831,
            "tau": 1.0
          },
          "Novelty": {
            "comparisons": [
              {
                "anchor_id": "A1",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story introduces novel dynamic selective reasoning for modality conflict, while A1 analyzes training data distribution effects across scales."
              },
              {
                "anchor_id": "A2",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story's dynamic routing for modality conflict is more novel than A2's cost-aware utility function for data selection."
              },
              {
                "anchor_id": "A3",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story's text interference quantification and dynamic routing is more innovative than A3's low-rank Riemannian optimization."
              },
              {
                "anchor_id": "A4",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story's reliability-based expert selection for modality conflict is more novel than A4's instance-level data reweighting."
              },
              {
                "anchor_id": "A5",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story's dynamic selective reasoning under modality conflict is more innovative than A5's measurement-driven data selection strategy."
              },
              {
                "anchor_id": "A6",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story's text interference representation and dynamic routing is more novel than A6's SVD-based knowledge activation."
              },
              {
                "anchor_id": "A7",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story's dynamic selective reasoning for modality conflict is more innovative than A7's taxonomy of memorization."
              },
              {
                "anchor_id": "A8",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story's reliability-based routing for modality conflict is more novel than A8's language-specific modules for translation."
              },
              {
                "anchor_id": "A9",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story's dynamic selective reasoning for modality conflict is more innovative than A9's precision-aware scaling laws."
              },
              {
                "anchor_id": "A10",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story's text interference quantification and dynamic routing is more novel than A10's development of another large-scale model."
              },
              {
                "anchor_id": "A11",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story's dynamic selective reasoning for modality conflict is more innovative than A11's optimization of pretraining pipeline."
              },
              {
                "anchor_id": "A12",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story's reliability-based routing for modality conflict is more novel than A12's Hidden Markov Transformer for translation timing."
              },
              {
                "anchor_id": "A13",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story's text interference quantification and dynamic routing is more innovative than A13's adaptive parameter budget allocation."
              }
            ],
            "loss": 2.6654326797674113,
            "avg_strength": 3.0,
            "monotonic_violations": 0,
            "ci_low": 9.199999999999848,
            "ci_high": 9.999999999999831,
            "tau": 1.4
          },
          "Storyteller": {
            "comparisons": [
              {
                "anchor_id": "A1",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story details specific method steps and evaluation metrics; Anchor lacks experimental plan and method specifics."
              },
              {
                "anchor_id": "A2",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story provides concrete method steps and evaluation plan; Anchor lacks experimental details and specific implementation."
              },
              {
                "anchor_id": "A3",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story outlines clear method steps and evaluation metrics; Anchor lacks experimental plan and specific implementation details."
              },
              {
                "anchor_id": "A4",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story details specific method steps and evaluation plan; Anchor lacks experimental details and concrete implementation."
              },
              {
                "anchor_id": "A5",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story provides clear method steps and evaluation metrics; Anchor lacks experimental plan and specific method details."
              },
              {
                "anchor_id": "A6",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story details specific method steps and evaluation plan; Anchor lacks experimental details and concrete implementation."
              },
              {
                "anchor_id": "A7",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story outlines clear method steps and evaluation metrics; Anchor lacks experimental plan and specific implementation details."
              },
              {
                "anchor_id": "A8",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story provides concrete method steps and evaluation plan; Anchor lacks experimental details and specific implementation."
              },
              {
                "anchor_id": "A9",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story details specific method steps and evaluation metrics; Anchor lacks experimental plan and concrete implementation."
              },
              {
                "anchor_id": "A10",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story outlines clear method steps and evaluation plan; Anchor lacks experimental details and specific implementation."
              },
              {
                "anchor_id": "A11",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story provides concrete method steps and evaluation metrics; Anchor lacks experimental plan and specific implementation."
              },
              {
                "anchor_id": "A12",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story details specific method steps and evaluation plan; Anchor lacks experimental details and concrete implementation."
              },
              {
                "anchor_id": "A13",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Story outlines clear method steps and evaluation metrics; Anchor lacks experimental plan and specific implementation."
              }
            ],
            "loss": 1.0786604099064554,
            "avg_strength": 3.0,
            "monotonic_violations": 0,
            "ci_low": 8.939999999999854,
            "ci_high": 9.999999999999831,
            "tau": 1.0
          }
        },
        "pass": {
          "mode": "two_of_three_q75_and_avg_ge_q50",
          "used_distribution": "pattern",
          "pattern_paper_count": 156,
          "q50": 6.175,
          "q75": 6.292000000000001,
          "count_roles_ge_q75": 3,
          "roles_ge_q75": {
            "Methodology": true,
            "Novelty": true,
            "Storyteller": true
          },
          "avg_ge_q50": true,
          "avg_score": 9.999999999999831
        },
        "rubric_version": "rubric_v1",
        "card_version": "blind_card_v1"
      },
      "field_feedback": {
        "title": {
          "issue": "The phrase 'Dynamic Composition' may trigger skepticism regarding training convergence and inference consistency, which directly contradicts the main issue of 'stability'.",
          "edit_instruction": "Modify the title to emphasize 'Reliable' or 'Stable' routing, or ensure the abstract immediately qualifies 'Dynamic' with stability guarantees.",
          "expected_effect": "Preempts reviewer concerns about the inherent instability of dynamic routing mechanisms."
        },
        "abstract": {
          "issue": "The abstract focuses on 'robustness' and 'generalization' but fails to explicitly address 'stability', despite it being the main issue.",
          "edit_instruction": "Insert a phrase stating that the method 'stabilizes inference' or 'ensures consistent decision-making' when describing the routing mechanism.",
          "expected_effect": "Aligns the abstract's promises with the paper's core technical contribution."
        },
        "problem_framing": {
          "issue": "The problem is framed as a 'bias' issue leading to errors, but not explicitly as a 'stability' issue where model performance fluctuates wildly under conflict.",
          "edit_instruction": "Reframe text-dominant bias as a source of 'prediction instability' in adversarial scenarios, rather than just an accuracy drop.",
          "expected_effect": "Elevates the problem definition to match the high-stakes nature of stability research."
        },
        "method_skeleton": {
          "issue": "The skeleton describes the components but lacks a mechanism to enforce stability in the gating/routing logic, which is prone to oscillation.",
          "edit_instruction": "In Step 2 or Step 4, explicitly mention a regularization technique (e.g., 'consistency loss' or 'entropy regularization') to stabilize the router's training.",
          "expected_effect": "Demonstrates technical depth and provides a concrete solution to the stability problem."
        },
        "innovation_claims": {
          "issue": "Claim 3 conflates 'efficiency' (lightweight adapters) with 'stability'. Efficiency does not inherently guarantee stable performance.",
          "edit_instruction": "Rewrite Claim 3 to focus on how the modular architecture isolates instability (e.g., 'prevents error propagation') rather than just saving compute.",
          "expected_effect": "Makes the innovation claim logically sound and directly relevant to the main issue."
        },
        "experiments_plan": {
          "issue": "Standard metrics (Acc, F1) measure performance, not stability. There is no plan to empirically validate the main issue.",
          "edit_instruction": "Add a specific evaluation for stability, such as 'Decision Consistency' across augmented samples or 'Variance' across multiple random seeds.",
          "expected_effect": "Provides the necessary empirical evidence to support the stability claims."
        }
      },
      "suggested_edits": [
        {
          "field": "innovation_claims",
          "action": "rewrite",
          "content": "Ensures inference stability by decoupling the audio backbone from text interference via modular routing, preventing the catastrophic degradation and oscillation seen in monolithic fusion models."
        },
        {
          "field": "method_skeleton",
          "action": "expand",
          "content": "Step 4: Optimize the framework using a loss function that reinforces correct expert selection and incorporates a consistency regularization term to stabilize the routing policy during training."
        },
        {
          "field": "experiments_plan",
          "action": "add",
          "content": "Evaluation includes a stability analysis measuring the variance of routing decisions and prediction consistency across noisy adversarial perturbations."
        }
      ],
      "priority": [
        "innovation_claims",
        "method_skeleton",
        "experiments_plan"
      ],
      "review_coach": {
        "field_feedback": {
          "title": {
            "issue": "The phrase 'Dynamic Composition' may trigger skepticism regarding training convergence and inference consistency, which directly contradicts the main issue of 'stability'.",
            "edit_instruction": "Modify the title to emphasize 'Reliable' or 'Stable' routing, or ensure the abstract immediately qualifies 'Dynamic' with stability guarantees.",
            "expected_effect": "Preempts reviewer concerns about the inherent instability of dynamic routing mechanisms."
          },
          "abstract": {
            "issue": "The abstract focuses on 'robustness' and 'generalization' but fails to explicitly address 'stability', despite it being the main issue.",
            "edit_instruction": "Insert a phrase stating that the method 'stabilizes inference' or 'ensures consistent decision-making' when describing the routing mechanism.",
            "expected_effect": "Aligns the abstract's promises with the paper's core technical contribution."
          },
          "problem_framing": {
            "issue": "The problem is framed as a 'bias' issue leading to errors, but not explicitly as a 'stability' issue where model performance fluctuates wildly under conflict.",
            "edit_instruction": "Reframe text-dominant bias as a source of 'prediction instability' in adversarial scenarios, rather than just an accuracy drop.",
            "expected_effect": "Elevates the problem definition to match the high-stakes nature of stability research."
          },
          "method_skeleton": {
            "issue": "The skeleton describes the components but lacks a mechanism to enforce stability in the gating/routing logic, which is prone to oscillation.",
            "edit_instruction": "In Step 2 or Step 4, explicitly mention a regularization technique (e.g., 'consistency loss' or 'entropy regularization') to stabilize the router's training.",
            "expected_effect": "Demonstrates technical depth and provides a concrete solution to the stability problem."
          },
          "innovation_claims": {
            "issue": "Claim 3 conflates 'efficiency' (lightweight adapters) with 'stability'. Efficiency does not inherently guarantee stable performance.",
            "edit_instruction": "Rewrite Claim 3 to focus on how the modular architecture isolates instability (e.g., 'prevents error propagation') rather than just saving compute.",
            "expected_effect": "Makes the innovation claim logically sound and directly relevant to the main issue."
          },
          "experiments_plan": {
            "issue": "Standard metrics (Acc, F1) measure performance, not stability. There is no plan to empirically validate the main issue.",
            "edit_instruction": "Add a specific evaluation for stability, such as 'Decision Consistency' across augmented samples or 'Variance' across multiple random seeds.",
            "expected_effect": "Provides the necessary empirical evidence to support the stability claims."
          }
        },
        "suggested_edits": [
          {
            "field": "innovation_claims",
            "action": "rewrite",
            "content": "Ensures inference stability by decoupling the audio backbone from text interference via modular routing, preventing the catastrophic degradation and oscillation seen in monolithic fusion models."
          },
          {
            "field": "method_skeleton",
            "action": "expand",
            "content": "Step 4: Optimize the framework using a loss function that reinforces correct expert selection and incorporates a consistency regularization term to stabilize the routing policy during training."
          },
          {
            "field": "experiments_plan",
            "action": "add",
            "content": "Evaluation includes a stability analysis measuring the variance of routing decisions and prediction consistency across noisy adversarial perturbations."
          }
        ],
        "priority": [
          "innovation_claims",
          "method_skeleton",
          "experiments_plan"
        ]
      }
    }
  ],
  "results_dir": "results/run_20260204_152014_100379_c1bb45",
  "novelty_report": {
    "run_id": "run_20260204_152014_100379_c1bb45",
    "created_at": "2026-02-04T15:43:53.311026+00:00",
    "user_idea": "我们研究音频-文本大模型的文本主导偏见：在 faithful/adversarial/irrelevant 三类设定下，提出基于模态可信度的路由/融合框架。通过构造“文本干扰”表示能量与时序证据，训练轻量门控/适配器，在推理时选择 Joint 或 Audio 专家，实现对抗文本误导下的稳健情绪识别",
    "embedding_available": true,
    "embedding_model": "Qwen/Qwen3-Embedding-8B",
    "top_k": 100,
    "thresholds": {
      "high": 0.88,
      "medium": 0.82
    },
    "risk_level": "low",
    "max_similarity": 0.580741286277771,
    "candidates": [
      {
        "paper_id": "TPZRq4FALB",
        "title": "Test-time Adaptation against Multi-modal Reliability Bias",
        "pattern_id": "",
        "domain": "Machine Learning",
        "text_hash": "85ecbb7da83a1aaae886311195041898f0bd5a40aeffc5097b95fc79d213ed50",
        "cosine": 0.580741286277771,
        "keyword_overlap": 0.0821917808219178
      },
      {
        "paper_id": "ePJrZLIqpV",
        "title": "Rethinking Audio-Visual Adversarial Vulnerability from Temporal and Modality Perspectives",
        "pattern_id": "",
        "domain": "Machine Learning",
        "text_hash": "d3f943cf44bf7e970363fa0aedc8716afdd67cc46e915d54ff2861ebe664249d",
        "cosine": 0.5643728971481323,
        "keyword_overlap": 0.09863945578231292
      },
      {
        "paper_id": "vtT09dYPGI",
        "title": "Routing Experts: Learning to Route Dynamic Experts in Existing Multi-modal Large Language Models",
        "pattern_id": "pattern_74",
        "domain": "Machine Learning",
        "text_hash": "59b22728a0b85817d085cc72b342dae6e768ff39578b9a159327d2d8445e6d8c",
        "cosine": 0.5507671236991882,
        "keyword_overlap": 0.10884353741496598
      },
      {
        "paper_id": "AV7OXVlAyi",
        "title": "Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality",
        "pattern_id": "pattern_13",
        "domain": "Machine Learning",
        "text_hash": "2b00d0adacffa9b4b76e94a68ebe21328497754c613a608494520d4cccc2d32e",
        "cosine": 0.5421634912490845,
        "keyword_overlap": 0.11888111888111888
      },
      {
        "paper_id": "74vnDs1R97",
        "title": "Wayward Concepts In Multimodal Models",
        "pattern_id": "pattern_51",
        "domain": "Machine Learning",
        "text_hash": "e3575cced82c01c31692bb5ad0e1b47a531a5a79bf5f4da6c81d24b34c00224a",
        "cosine": 0.5278177857398987,
        "keyword_overlap": 0.06896551724137931
      },
      {
        "paper_id": "l60EM8md3t",
        "title": "Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation",
        "pattern_id": "",
        "domain": "Machine Learning",
        "text_hash": "283fd505312f835e0c070c65fdd13e3d83bcd45f433e9e03aed7bb9a3e4e5108",
        "cosine": 0.5277820825576782,
        "keyword_overlap": 0.07586206896551724
      },
      {
        "paper_id": "1SYUKPeM12",
        "title": "Aligned Better, Listen Better for Audio-Visual Large Language Models",
        "pattern_id": "pattern_13",
        "domain": "Machine Learning",
        "text_hash": "e80f050a72eb521e9a39c1ad0e8bf7d83712f9be0580d5b59057d0d81b9fcc27",
        "cosine": 0.5181591510772705,
        "keyword_overlap": 0.09897610921501707
      },
      {
        "paper_id": "jTEKTdI3K9",
        "title": "AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models",
        "pattern_id": "pattern_13",
        "domain": "Machine Learning",
        "text_hash": "f46910c6a6945456aa4c0dba4aa96ee893bbd85f0ed1a42faf6d3c9a926a10f7",
        "cosine": 0.50555020570755,
        "keyword_overlap": 0.09215017064846416
      },
      {
        "paper_id": "rTDyN8yajn",
        "title": "Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE",
        "pattern_id": "",
        "domain": "Machine Learning",
        "text_hash": "80b14c084ad1e87644887c71b1c490518cf88c908295e6af1ef55a1f196f00b1",
        "cosine": 0.5045864582061768,
        "keyword_overlap": 0.07446808510638298
      },
      {
        "paper_id": "qIbbBSzH6n",
        "title": "MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models",
        "pattern_id": "",
        "domain": "Machine Learning",
        "text_hash": "4a5847d7fa158f5d6a1740afbd354e1e45daeae6fd608cb3b6e8bbef209562d9",
        "cosine": 0.501451313495636,
        "keyword_overlap": 0.07931034482758621
      }
    ],
    "notes": [
      "index_reused"
    ],
    "report_path": "results/run_20260204_152014_100379_c1bb45/novelty_report.json",
    "pivot_attempts": 0,
    "action": "pivot"
  },
  "recall_audit": {
    "final_top_k": [
      {
        "pattern_id": "pattern_112",
        "name": "Reframing Multimodal Reasoning Challenges",
        "final_score": 0.7614161213283019,
        "path1_score": 0.7614161213283019,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 115
      },
      {
        "pattern_id": "pattern_113",
        "name": "Reframing Multimodal Learning Narratives",
        "final_score": 0.5890354269315895,
        "path1_score": 0.5890354269315895,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 46
      },
      {
        "pattern_id": "pattern_74",
        "name": "Democratizing Large Language Model Accessibility",
        "final_score": 0.5685728602473885,
        "path1_score": 0.5685728602473885,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 156
      },
      {
        "pattern_id": "pattern_51",
        "name": "Adversarial Vulnerabilities and Robustness in Large Language Models",
        "final_score": 0.2700678650062621,
        "path1_score": 0.21574144799815415,
        "path2_score": 0.0,
        "path3_score": 0.05432641700810795,
        "cluster_size": 92
      },
      {
        "pattern_id": "pattern_115",
        "name": "Semantic Alignment for Compositional Generation",
        "final_score": 0.20984736978819418,
        "path1_score": 0.20984736978819418,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 107
      },
      {
        "pattern_id": "pattern_7",
        "name": "Reframing Audio Understanding Through Multimodal and Probabilistic Learning",
        "final_score": 0.1990098295426834,
        "path1_score": 0.1944255263059509,
        "path2_score": 0.0045843032367325134,
        "path3_score": 0.0,
        "cluster_size": 41
      },
      {
        "pattern_id": "pattern_89",
        "name": "Contextual Adaptation and Interpretability in Language Models",
        "final_score": 0.18350047601326205,
        "path1_score": 0.18350047601326205,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 16
      },
      {
        "pattern_id": "pattern_19",
        "name": "Adaptive Knowledge Distillation Dynamics",
        "final_score": 0.1803201310613285,
        "path1_score": 0.1803201310613285,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 65
      },
      {
        "pattern_id": "pattern_6",
        "name": "Reframing Molecular Generation with Structural Priors",
        "final_score": 0.05072254315932068,
        "path1_score": 0.0,
        "path2_score": 0.0,
        "path3_score": 0.05072254315932068,
        "cluster_size": 214
      },
      {
        "pattern_id": "pattern_106",
        "name": "Adaptive Dynamic Reasoning Trajectories",
        "final_score": 0.050406741381964076,
        "path1_score": 0.0,
        "path2_score": 0.0,
        "path3_score": 0.050406741381964076,
        "cluster_size": 49
      }
    ],
    "path1": {
      "top_ideas": [
        {
          "idea_id": "idea_5003",
          "similarity": 0.5533411676861143,
          "snippet": "Introduce a comprehensive framework to evaluate and enhance the adversarial robustness of multimodal language model agents in real environments.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_6771",
          "similarity": 0.5426738333016822,
          "snippet": "Introduce a challenging task and dataset to enhance the nuanced comprehension capabilities of multi-modal large models by focusing on interleaved image-text scenarios.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_2557",
          "similarity": 0.5393536199953853,
          "snippet": "Investigate the vulnerability of linguistic modalities in multimodal models to data poisoning attacks and propose defenses to mitigate these attacks.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_6838",
          "similarity": 0.5288297744687367,
          "snippet": "Introduce a dynamic expert routing method to optimize path selection in multimodal large language models without altering their structure.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_4237",
          "similarity": 0.5246184244704855,
          "snippet": "Introduce a modular framework to seamlessly integrate text and image generation models for high-quality multimodal outputs.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_4261",
          "similarity": 0.5124158382808983,
          "snippet": "Investigate and mitigate the limitations of large multimodal models using in-context learning and propose new ICL variants to address specific flaws.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_1280",
          "similarity": 0.5045954091287363,
          "snippet": "Identify and quantify modality complementariness as a key factor affecting the robustness of multi-modal models.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7643",
          "similarity": 0.48606381576487717,
          "snippet": "Introduce a comprehensive framework for generating natural language explanations of audio differences using large language models and novel datasets.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7492",
          "similarity": 0.48535041289997677,
          "snippet": "Utilize multimodal information through a CLIP-powered framework to enhance data selection by effectively identifying and removing noisy samples, improving both training efficiency and model performance.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7100",
          "similarity": 0.4826427453002604,
          "snippet": "Introduce a dynamic context sparsification framework to enhance the efficiency of multimodal large language models without degrading performance.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_6957",
          "similarity": 0.47661563165982684,
          "snippet": "Introduce a dynamic data selection approach to enhance continual multimodal instruction tuning by balancing sample efficiency and effectiveness.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_6302",
          "similarity": 0.46754365629599864,
          "snippet": "Introduce a comprehensive dataset and benchmark for evaluating and improving the understanding of dynamic GUI content by multimodal large language models.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_4956",
          "similarity": 0.4593869919157459,
          "snippet": "Develop a framework for routing between LLMs to optimize cost and performance using human preference data and data augmentation.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_5419",
          "similarity": 0.4587511900331551,
          "snippet": "Utilize a graph-based framework to enhance the selection process of Large Language Models by leveraging contextual interactions among tasks, queries, and models.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_5874",
          "similarity": 0.45493871056974405,
          "snippet": "Introduce a method to detect and quantify segments of text written by large language models within mixed-text documents.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_7811",
          "similarity": 0.4508003276533212,
          "snippet": "Introduce a framework for efficiently distilling knowledge from large-scale to small-scale multimodal language models using a sparse Mixture of Experts and progressive knowledge transfer.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_4825",
          "similarity": 0.4482147334286806,
          "snippet": "Introduce a benchmark to evaluate the reasoning capabilities of multimodal large language models in sports contexts through textual and visual tasks.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_5104",
          "similarity": 0.4469987100082369,
          "snippet": "Introduce a joint inference framework for fully unsupervised adaptation of large language and vision-language models, eliminating the need for manual prompt engineering and labeled examples.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_7803",
          "similarity": 0.44510808029439347,
          "snippet": "Introduce a comprehensive benchmark, MME-RealWorld, to evaluate the performance of Multimodal Large Language Models on high-resolution, real-world scenarios.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7857",
          "similarity": 0.4332153842339887,
          "snippet": "Enhance large language model performance by clustering training data based on gradient directions and using low-rank expert adapters for task-specific optimization.",
          "pattern_count": 1
        }
      ],
      "pattern_scores_topn": [
        {
          "pattern_id": "pattern_112",
          "score": 0.7614161213283019
        },
        {
          "pattern_id": "pattern_113",
          "score": 0.5890354269315895
        },
        {
          "pattern_id": "pattern_74",
          "score": 0.5685728602473885
        },
        {
          "pattern_id": "pattern_51",
          "score": 0.21574144799815415
        },
        {
          "pattern_id": "pattern_115",
          "score": 0.20984736978819418
        },
        {
          "pattern_id": "pattern_7",
          "score": 0.1944255263059509
        },
        {
          "pattern_id": "pattern_89",
          "score": 0.18350047601326205
        },
        {
          "pattern_id": "pattern_19",
          "score": 0.1803201310613285
        }
      ]
    },
    "path2": {
      "top_domains": [
        {
          "domain_id": "domain_45",
          "name": "Audio Processing",
          "weight": 0.41022073292462735,
          "paper_count": 5
        },
        {
          "domain_id": "domain_82",
          "name": "Speech Recognition",
          "weight": 0.39503734142319724,
          "paper_count": 2
        },
        {
          "domain_id": "domain_48",
          "name": "Speech Processing",
          "weight": 0.37665077066564256,
          "paper_count": 7
        },
        {
          "domain_id": "domain_0",
          "name": "Fairness & Accountability",
          "weight": 0.3480291138233781,
          "paper_count": 69
        },
        {
          "domain_id": "domain_63",
          "name": "Neuromorphic Computing",
          "weight": 0.3444230493431408,
          "paper_count": 2
        }
      ],
      "top_subdomains": [
        {
          "domain_id": "domain_45",
          "subdomains": [
            {
              "name": "Contrastive Learning",
              "score": 0.3004169967859043
            },
            {
              "name": "Diffusion Models",
              "score": 0.28951372374948053
            }
          ]
        },
        {
          "domain_id": "domain_82",
          "subdomains": [
            {
              "name": "Speech Synthesis",
              "score": 0.30466997654114975
            },
            {
              "name": "Contrastive Learning",
              "score": 0.3004169967859043
            }
          ]
        },
        {
          "domain_id": "domain_48",
          "subdomains": [
            {
              "name": "Speech Synthesis",
              "score": 0.3052615815149051
            },
            {
              "name": "Contrastive Learning",
              "score": 0.30119005373877206
            },
            {
              "name": "Diffusion Models",
              "score": 0.2895870112619752
            }
          ]
        },
        {
          "domain_id": "domain_0",
          "subdomains": [
            {
              "name": "Out-of-Distribution Detection",
              "score": 0.35204400136021236
            },
            {
              "name": "Bias Mitigation",
              "score": 0.32311886299265563
            },
            {
              "name": "Contrastive Learning",
              "score": 0.3010938851068649
            },
            {
              "name": "Robustness",
              "score": 0.29495693446895177
            },
            {
              "name": "Federated Learning",
              "score": 0.28014078767067785
            }
          ]
        },
        {
          "domain_id": "domain_63",
          "subdomains": [
            {
              "name": "Contrastive Learning",
              "score": 0.3004169967859043
            },
            {
              "name": "Robustness",
              "score": 0.29535772474209543
            },
            {
              "name": "Neuromorphic Computing",
              "score": 0.23047056914772723
            }
          ]
        }
      ],
      "candidate_stats": [
        {
          "domain_id": "domain_45",
          "candidates_before": 1,
          "candidates_after": 1
        },
        {
          "domain_id": "domain_82",
          "candidates_before": 1,
          "candidates_after": 1
        },
        {
          "domain_id": "domain_48",
          "candidates_before": 2,
          "candidates_after": 2
        },
        {
          "domain_id": "domain_0",
          "candidates_before": 5,
          "candidates_after": 5
        },
        {
          "domain_id": "domain_63",
          "candidates_before": 1,
          "candidates_after": 1
        }
      ],
      "pattern_scores_topn": [
        {
          "pattern_id": "pattern_9",
          "score": 0.009209677707406591
        },
        {
          "pattern_id": "pattern_7",
          "score": 0.0045843032367325134
        },
        {
          "pattern_id": "pattern_8",
          "score": 0.00149864892016317
        },
        {
          "pattern_id": "pattern_87",
          "score": 0.0014116520269308271
        },
        {
          "pattern_id": "pattern_45",
          "score": 0.001352048143028003
        }
      ],
      "subdomain_taxonomy_used": true,
      "raw_subdomain_count": 319,
      "canonical_subdomain_count": 58,
      "stoplist_count": 12
    },
    "path3": {
      "top_papers": [
        {
          "paper_id": "lOi6FtIwR8",
          "similarity": 0.396861838031324,
          "title": "Model Editing as a Robust and Denoised variant of DPO: A Case Study on Toxicity",
          "quality": 0.585,
          "review_count": 4
        },
        {
          "paper_id": "J6e4hurEKd",
          "similarity": 0.3875689456033966,
          "title": "RetroInText: A Multimodal Large Language Model Enhanced Framework for Retrosynthetic Planning via In-Context Representation Learning",
          "quality": 0.572,
          "review_count": 5
        },
        {
          "paper_id": "44CoQe6VCq",
          "similarity": 0.35476119728534244,
          "title": "Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning",
          "quality": 0.596,
          "review_count": 5
        },
        {
          "paper_id": "pfuqQQCB34",
          "similarity": 0.29171652300801276,
          "title": "Variance Reduction is an Antidote to Byzantines: Better Rates, Weaker Assumptions and Communication Compression as a Cherry on the Top",
          "quality": 0.6858333333333334,
          "review_count": 6
        },
        {
          "paper_id": "02Bt_4tx6r",
          "similarity": 0.3458667983870858,
          "title": "Joint rotational invariance and adversarial training of a dual-stream Transformer yields state of the art Brain-Score for Area V4",
          "quality": 0.5660000000000001,
          "review_count": 5
        }
      ],
      "pattern_scores_topn": [
        {
          "pattern_id": "pattern_51",
          "score": 0.05432641700810795
        },
        {
          "pattern_id": "pattern_6",
          "score": 0.05072254315932068
        },
        {
          "pattern_id": "pattern_106",
          "score": 0.050406741381964076
        },
        {
          "pattern_id": "pattern_22",
          "score": 0.044320201625637316
        }
      ]
    }
  },
  "review_summary": {
    "total_reviews": 1,
    "final_score": 9.999999999999831
  },
  "refinement_summary": {
    "total_refinements": 0,
    "issues_addressed": []
  },
  "verification_summary": {
    "collision_detected": false,
    "max_similarity": 0.580741286277771
  },
  "idea_packaging": {
    "raw_idea": "我们研究音频-文本大模型的文本主导偏见：在 faithful/adversarial/irrelevant 三类设定下，提出基于模态可信度的路由/融合框架。通过构造“文本干扰”表示能量与时序证据，训练轻量门控/适配器，在推理时选择 Joint 或 Audio 专家，实现对抗文本误导下的稳健情绪识别",
    "brief_a": {
      "motivation": "Current audio-text large models suffer from 'text-dominant bias,' where the model relies excessively on textual modality, leading to errors in emotion recognition when text is misleading, conflicting, or irrelevant. Addressing this bias is critical for achieving robust performance in real-world scenarios where modalities may not always align.",
      "problem_definition": "The task is robust multimodal emotion recognition. The system takes audio and text as input and outputs emotion labels. The study specifically evaluates performance under three distinct settings: faithful (consistent modalities), adversarial (conflicting modalities), and irrelevant (unrelated text).",
      "constraints": [
        "robustness",
        "computational efficiency"
      ],
      "technical_plan": "Propose a routing and fusion framework based on modality reliability. The core method involves constructing a 'text interference' representation characterized by energy and temporal evidence to quantify modality trustworthiness. Lightweight gating mechanisms or adapters are trained using these features. During inference, the system dynamically routes the input to either a Joint (Audio+Text) expert or an Audio-only expert to mitigate the impact of misleading text.",
      "expected_contributions": [
        "Identification and analysis of text-dominant bias in audio-text large models across faithful, adversarial, and irrelevant settings",
        "Development of a modality reliability-based routing framework utilizing energy and temporal evidence for interference detection",
        "Demonstration of improved robustness in emotion recognition through dynamic expert selection (Joint vs. Audio)"
      ],
      "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., IEMOCAP, MELD) potentially augmented with adversarial/irrelevant samples. Metrics: Accuracy, Weighted Accuracy (WA), Unweighted Accuracy (UA), F1-score. Baselines: Standard audio-text LMMs, static fusion methods. Ablations: Impact of the routing mechanism, the 'text interference' representation module, and the adapter training.",
      "keywords_en": [
        "Audio-Text Large Models",
        "Text-Dominant Bias",
        "Robust Emotion Recognition",
        "Modality Routing",
        "Adversarial Settings"
      ],
      "keywords_zh": [
        "音频-文本大模型",
        "文本主导偏见",
        "稳健情绪识别",
        "模态路由",
        "对抗设定"
      ],
      "assumptions": {
        "explicit": [
          "The research focuses on audio-text large models",
          "The problem involves text-dominant bias",
          "Three settings are considered: faithful, adversarial, and irrelevant",
          "The method uses energy and temporal evidence",
          "The solution involves training lightweight gates/adapters",
          "Inference involves selecting between Joint or Audio experts",
          "The target application is emotion recognition"
        ],
        "inferred": [
          "The 'Audio expert' implies a model component capable of processing audio-only inputs effectively",
          "Energy refers to acoustic signal energy features",
          "Temporal evidence refers to time-series alignment or duration features",
          "The framework aims to reduce computational cost or improve accuracy by selectively ignoring text"
        ]
      }
    },
    "query_a": "Keywords: Audio-Text Large Models, Text-Dominant Bias, Robust Emotion Recognition, Modality Routing, Adversarial Settings The task is robust multimodal emotion recognition. The system takes audio and text as input and outputs emotion labels. The study specifically evaluates performance under three distinct settings: faithful (consistent modalities), adversarial (conflicting modalities), and irrelevant (unrelated text). Constraints: robustness, computational efficiency Propose a routing and fusion framework based on modality reliability. The core method involves constructing a 'text interference' representation characterized by energy and temporal evidence to quantify modality trustworthiness. Lightweight gating mechanisms or adapters are trained using these features. During inference, the system dynamically routes the input to either a Joint (Audio+Text) expert or an Audio-only expert to mitigate the impact of misleading text. 关键词: 音频-文本大模型，文本主导偏见，稳健情绪识别，模态路由，对抗设定",
    "candidates": [
      {
        "pattern_id": "pattern_112",
        "pattern_name": "Reframing Multimodal Reasoning Challenges",
        "score": 0.953575186604936,
        "brief": {
          "motivation": "Current audio-text large models suffer from 'text-dominant bias,' where the model relies excessively on textual modality, leading to errors in emotion recognition when text is misleading, conflicting, or irrelevant. Addressing this bias is critical for achieving robust performance in real-world scenarios where modalities may not always align. Furthermore, existing monolithic approaches often lack the flexibility to handle these conflicts efficiently. By reframing the fusion strategy from static joint processing to dynamic, modular composition, we can enhance the model's generalization capacity and robustness without the prohibitive cost of retraining entire architectures.",
          "problem_definition": "The task is robust multimodal emotion recognition, framed as a challenge of selective reasoning under modality conflict. The system takes audio and text as input and outputs emotion labels. The study specifically evaluates performance under three distinct settings: faithful (consistent modalities), adversarial (conflicting modalities), and irrelevant (unrelated text), to test the model's ability to discern trustworthy information sources.",
          "constraints": [
            "Robustness to misleading textual information",
            "Computational efficiency via lightweight adapters",
            "Modularity for dynamic expert selection"
          ],
          "technical_plan": "Propose a modular routing and fusion framework based on modality reliability, moving away from rigid joint training. The core method involves constructing a 'text interference' representation characterized by energy and temporal evidence to quantify modality trustworthiness. Lightweight gating mechanisms or adapters are trained using these features. During inference, the system dynamically routes the input to either a Joint (Audio+Text) expert or an Audio-only expert, effectively reframing the task as a selective composition problem to mitigate the impact of misleading text.",
          "expected_contributions": [
            "Identification and analysis of text-dominant bias in audio-text large models across faithful, adversarial, and irrelevant settings",
            "Development of a modality reliability-based routing framework utilizing energy and temporal evidence as intermediate signals for interference detection",
            "Demonstration that dynamic expert selection (Joint vs. Audio) significantly improves robustness and generalization in emotion recognition tasks"
          ],
          "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., IEMOCAP, MELD) potentially augmented with adversarial/irrelevant samples to simulate real-world inconsistencies. Metrics: Accuracy, Weighted Accuracy (WA), Unweighted Accuracy (UA), F1-score. Baselines: Standard audio-text LMMs, static fusion methods. Ablations: Impact of the routing mechanism, the 'text interference' representation module, and the adapter training to validate the modular design.",
          "keywords_en": [
            "Audio-Text Large Models",
            "Text-Dominant Bias",
            "Robust Emotion Recognition",
            "Modality Routing",
            "Adversarial Settings",
            "Modular Multimodal Learning",
            "Dynamic Fusion"
          ],
          "keywords_zh": [
            "音频-文本大模型",
            "文本主导偏见",
            "稳健情绪识别",
            "模态路由",
            "对抗设定",
            "模块化多模态学习",
            "动态融合"
          ],
          "assumptions": {
            "explicit": [
              "The research focuses on audio-text large models",
              "The problem involves text-dominant bias",
              "Three settings are considered: faithful, adversarial, and irrelevant",
              "The method uses energy and temporal evidence",
              "The solution involves training lightweight gates/adapters",
              "Inference involves selecting between Joint or Audio experts",
              "The target application is emotion recognition"
            ],
            "inferred": [
              "The 'Audio expert' implies a model component capable of processing audio-only inputs effectively",
              "Energy refers to acoustic signal energy features",
              "Temporal evidence refers to time-series alignment or duration features",
              "The framework aims to reduce computational cost or improve accuracy by selectively ignoring text",
              "The 'text interference' representation serves as an intermediate signal for routing decisions"
            ]
          }
        },
        "query": "Keywords: Audio-Text Large Models, Text-Dominant Bias, Robust Emotion Recognition, Modality Routing, Adversarial Settings, Modular Multimodal Learning, Dynamic Fusion The task is robust multimodal emotion recognition, framed as a challenge of selective reasoning under modality conflict. The system takes audio and text as input and outputs emotion labels. The study specifically evaluates performance under three distinct settings: faithful (consistent modalities), adversarial (conflicting modalities), and irrelevant (unrelated text), to test the model's ability to discern trustworthy information sources. Constraints: Robustness to misleading textual information, Computational efficiency via lightweight adapters, Modularity for dynamic expert selection Propose a modular routing and fusion framework based on modality reliability, moving away from rigid joint training. The core method involves constructing a 'text interference' representation characterized by energy and temporal evidence to quantify modality trustworthiness. Lightweight gating mechanisms or adapters are trained using these features. During inference, the system dynamically routes the input to either a Joint (Audio+Text) expert or an Audio-only expert, effectively reframing the task as a selective composition problem to mitigate the impact of misleading text. 关键词: 音频-文本大模型，文本主导偏见，稳健情绪识别，模态路由，对抗设定，模块化多模态学习，动态融合"
      },
      {
        "pattern_id": "pattern_113",
        "pattern_name": "Reframing Multimodal Learning Narratives",
        "score": 0.3869484917615473,
        "brief": {
          "motivation": "Current audio-text large models exhibit a 'text-dominant bias,' leading to significant performance degradation when modalities are misaligned. This aligns with the broader challenge of 'inter-modality variation' and 'conditional dependency structures' in multimodal learning. Addressing this requires reframing the fusion process to handle arbitrary modality conditions—specifically, scenarios where text is faithful, adversarial, or irrelevant—rather than assuming consistent cross-modal alignment.",
          "problem_definition": "The task is robust multimodal emotion recognition under arbitrary modality conditions. The system must process audio and text inputs and output emotion labels, specifically addressing the challenge of 'text-dominant bias' where the model over-relies on potentially misleading textual information.",
          "constraints": [
            "Robustness to adversarial/irrelevant text",
            "Computational efficiency (lightweight adapters)",
            "Handling cross-modality discrepancies"
          ],
          "technical_plan": "We propose a dynamic routing and fusion framework based on modality reliability. Drawing on the concept of analyzing dependency structures, we construct a 'text interference' representation using energy and temporal evidence to quantify trustworthiness. We train lightweight gating mechanisms or adapters to detect these discrepancies. During inference, the system dynamically routes inputs to either a Joint (Audio+Text) expert or an Audio-only expert, effectively mitigating the impact of misleading text by leveraging strong uni-modal feature learning when necessary.",
          "expected_contributions": [
            "Reframing the text-dominant bias problem as a challenge of handling arbitrary modality conditions and cross-modality discrepancies.",
            "Development of a reliability-based routing framework that utilizes energy and temporal evidence to detect text interference.",
            "Demonstration that dynamic expert selection (Joint vs. Audio) significantly improves robustness, validating the importance of uni-modal feature learning in multimodal systems."
          ],
          "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., IEMOCAP, MELD) augmented with adversarial and irrelevant samples to simulate arbitrary modality conditions. Metrics: Accuracy, Weighted Accuracy (WA), Unweighted Accuracy (UA), F1-score. Baselines: Standard audio-text LMMs, static fusion methods, and existing robust multimodal approaches. Ablations: Impact of the routing mechanism, the 'text interference' representation module, and the adapter training strategy.",
          "keywords_en": [
            "Audio-Text Large Models",
            "Text-Dominant Bias",
            "Robust Emotion Recognition",
            "Modality Routing",
            "Arbitrary Modality Conditions",
            "Cross-Modality Discrepancy"
          ],
          "keywords_zh": [
            "音频-文本大模型",
            "文本主导偏见",
            "稳健情绪识别",
            "模态路由",
            "任意模态条件",
            "跨模态差异"
          ],
          "assumptions": {
            "explicit": [
              "The research focuses on audio-text large models",
              "The problem involves text-dominant bias",
              "Three settings are considered: faithful, adversarial, and irrelevant",
              "The method uses energy and temporal evidence",
              "The solution involves training lightweight gates/adapters",
              "Inference involves selecting between Joint or Audio experts",
              "The target application is emotion recognition"
            ],
            "inferred": [
              "The 'Audio expert' implies a model component capable of processing audio-only inputs effectively",
              "Energy refers to acoustic signal energy features",
              "Temporal evidence refers to time-series alignment or duration features",
              "The framework aims to reduce computational cost or improve accuracy by selectively ignoring text",
              "Energy and temporal features are sufficient proxies for detecting semantic conflict between modalities"
            ]
          }
        },
        "query": "Keywords: Audio-Text Large Models, Text-Dominant Bias, Robust Emotion Recognition, Modality Routing, Arbitrary Modality Conditions, Cross-Modality Discrepancy The task is robust multimodal emotion recognition under arbitrary modality conditions. The system must process audio and text inputs and output emotion labels, specifically addressing the challenge of 'text-dominant bias' where the model over-relies on potentially misleading textual information. Constraints: Robustness to adversarial/irrelevant text, Computational efficiency (lightweight adapters), Handling cross-modality discrepancies We propose a dynamic routing and fusion framework based on modality reliability. Drawing on the concept of analyzing dependency structures, we construct a 'text interference' representation using energy and temporal evidence to quantify trustworthiness. We train lightweight gating mechanisms or adapters to detect these discrepancies. During inference, the system dynamically routes inputs to either a Joint (Audio+Text) expert or an Audio-only expert, effectively mitigating the impact of misleading text by leveraging strong uni-modal feature learning when necessary. 关键词: 音频-文本大模型，文本主导偏见，稳健情绪识别，模态路由，任意模态条件，跨模态差异"
      },
      {
        "pattern_id": "pattern_51",
        "pattern_name": "Adversarial Vulnerabilities and Robustness in Large Language Models",
        "score": 0.2717693785265833,
        "brief": {
          "motivation": "Current audio-text Large Models (LMMs) exhibit a critical 'text-dominant bias,' functioning similarly to adversarial vulnerabilities found in pure LLMs where textual input can override other evidence. This bias creates a security and robustness risk, as misleading or conflicting text (acting as an adversarial trigger) can hijack the model's decision-making, leading to incorrect emotion recognition. Addressing this bias is essential for achieving robust performance in real-world scenarios where modalities may not always align or where text might be manipulated.",
          "problem_definition": "The task is robust multimodal emotion recognition under adversarial conditions. The system takes audio and text as input and outputs emotion labels. The study specifically evaluates performance under three distinct settings: faithful (consistent modalities), adversarial (conflicting modalities where text acts as a misleading prompt), and irrelevant (unrelated text). The core challenge is preventing the model from blindly following the text modality when it conflicts with the audio signal.",
          "constraints": [
            "robustness against adversarial text",
            "computational efficiency"
          ],
          "technical_plan": "Propose a dynamic routing and fusion framework grounded in modality reliability estimation. The method constructs a 'text interference' representation using acoustic energy and temporal evidence to quantify the trustworthiness of the text input. We train lightweight gating mechanisms or adapters to detect when text acts as misleading interference. During inference, the system dynamically routes the input to either a Joint (Audio+Text) expert or a robust Audio-only expert, effectively mitigating the impact of adversarial textual prompts.",
          "expected_contributions": [
            "Identification and analysis of text-dominant bias as an adversarial vulnerability in audio-text large models across faithful, adversarial, and irrelevant settings",
            "Development of a modality reliability-based routing framework utilizing energy and temporal evidence for interference detection",
            "Demonstration of improved robustness in emotion recognition through dynamic expert selection (Joint vs. Audio) to defend against misleading text"
          ],
          "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., IEMOCAP, MELD) augmented with constructed adversarial and irrelevant text samples to simulate 'red-teaming' scenarios. Metrics: Accuracy, Weighted Accuracy (WA), Unweighted Accuracy (UA), F1-score, and Robustness Gap (performance drop in adversarial vs. faithful settings). Baselines: Standard audio-text LMMs, static fusion methods, and existing adversarial defense techniques. Ablations: Impact of the routing mechanism, the 'text interference' representation module, and the adapter training.",
          "keywords_en": [
            "Audio-Text Large Models",
            "Text-Dominant Bias",
            "Robust Emotion Recognition",
            "Modality Routing",
            "Adversarial Robustness",
            "Multimodal Interference"
          ],
          "keywords_zh": [
            "音频-文本大模型",
            "文本主导偏见",
            "稳健情绪识别",
            "模态路由",
            "对抗鲁棒性"
          ],
          "assumptions": {
            "explicit": [
              "The research focuses on audio-text large models",
              "The problem involves text-dominant bias",
              "Three settings are considered: faithful, adversarial, and irrelevant",
              "The method uses energy and temporal evidence",
              "The solution involves training lightweight gates/adapters",
              "Inference involves selecting between Joint or Audio experts",
              "The target application is emotion recognition"
            ],
            "inferred": [
              "The 'Audio expert' implies a model component capable of processing audio-only inputs effectively",
              "Energy refers to acoustic signal energy features",
              "Temporal evidence refers to time-series alignment or duration features",
              "The framework aims to reduce computational cost or improve accuracy by selectively ignoring text",
              "Misleading text acts similarly to adversarial noise or prompt injection"
            ]
          }
        },
        "query": "Keywords: Audio-Text Large Models, Text-Dominant Bias, Robust Emotion Recognition, Modality Routing, Adversarial Robustness, Multimodal Interference The task is robust multimodal emotion recognition under adversarial conditions. The system takes audio and text as input and outputs emotion labels. The study specifically evaluates performance under three distinct settings: faithful (consistent modalities), adversarial (conflicting modalities where text acts as a misleading prompt), and irrelevant (unrelated text). The core challenge is preventing the model from blindly following the text modality when it conflicts with the audio signal. Constraints: robustness against adversarial text, computational efficiency Propose a dynamic routing and fusion framework grounded in modality reliability estimation. The method constructs a 'text interference' representation using acoustic energy and temporal evidence to quantify the trustworthiness of the text input. We train lightweight gating mechanisms or adapters to detect when text acts as misleading interference. During inference, the system dynamically routes the input to either a Joint (Audio+Text) expert or a robust Audio-only expert, effectively mitigating the impact of adversarial textual prompts. 关键词: 音频-文本大模型，文本主导偏见，稳健情绪识别，模态路由，对抗鲁棒性"
      }
    ],
    "judge": {
      "best_index": 0,
      "rationale": "Candidate 0 (pattern_112) is the best choice because it faithfully captures the specific technical details of the raw idea, such as the 'text interference' representation based on energy and temporal evidence, the use of lightweight gates/adapters, and the dynamic routing between Joint and Audio experts. It comprehensively addresses all three settings (faithful, adversarial, irrelevant) without over-emphasizing one aspect (unlike Candidate 3's heavy focus on security/adversarial framing) or becoming too abstract (like Candidate 2). The structure is complete, covering motivation, problem definition, technical plan, and a concrete evaluation plan, making it highly actionable and clear."
    },
    "recall_scores": {
      "0": 0.0,
      "1": 0.0,
      "2": 0.0
    },
    "chosen_index": 0,
    "query_best": "Keywords: Audio-Text Large Models, Text-Dominant Bias, Robust Emotion Recognition, Modality Routing, Adversarial Settings, Modular Multimodal Learning, Dynamic Fusion The task is robust multimodal emotion recognition, framed as a challenge of selective reasoning under modality conflict. The system takes audio and text as input and outputs emotion labels. The study specifically evaluates performance under three distinct settings: faithful (consistent modalities), adversarial (conflicting modalities), and irrelevant (unrelated text), to test the model's ability to discern trustworthy information sources. Constraints: Robustness to misleading textual information, Computational efficiency via lightweight adapters, Modularity for dynamic expert selection Propose a modular routing and fusion framework based on modality reliability, moving away from rigid joint training. The core method involves constructing a 'text interference' representation characterized by energy and temporal evidence to quantify modality trustworthiness. Lightweight gating mechanisms or adapters are trained using these features. During inference, the system dynamically routes the input to either a Joint (Audio+Text) expert or an Audio-only expert, effectively reframing the task as a selective composition problem to mitigate the impact of misleading text. 关键词: 音频-文本大模型，文本主导偏见，稳健情绪识别，模态路由，对抗设定，模块化多模态学习，动态融合",
    "brief_best": {
      "motivation": "Current audio-text large models suffer from 'text-dominant bias,' where the model relies excessively on textual modality, leading to errors in emotion recognition when text is misleading, conflicting, or irrelevant. Addressing this bias is critical for achieving robust performance in real-world scenarios where modalities may not always align. Furthermore, existing monolithic approaches often lack the flexibility to handle these conflicts efficiently. By reframing the fusion strategy from static joint processing to dynamic, modular composition, we can enhance the model's generalization capacity and robustness without the prohibitive cost of retraining entire architectures.",
      "problem_definition": "The task is robust multimodal emotion recognition, framed as a challenge of selective reasoning under modality conflict. The system takes audio and text as input and outputs emotion labels. The study specifically evaluates performance under three distinct settings: faithful (consistent modalities), adversarial (conflicting modalities), and irrelevant (unrelated text), to test the model's ability to discern trustworthy information sources.",
      "constraints": [
        "Robustness to misleading textual information",
        "Computational efficiency via lightweight adapters",
        "Modularity for dynamic expert selection"
      ],
      "technical_plan": "Propose a modular routing and fusion framework based on modality reliability, moving away from rigid joint training. The core method involves constructing a 'text interference' representation characterized by energy and temporal evidence to quantify modality trustworthiness. Lightweight gating mechanisms or adapters are trained using these features. During inference, the system dynamically routes the input to either a Joint (Audio+Text) expert or an Audio-only expert, effectively reframing the task as a selective composition problem to mitigate the impact of misleading text.",
      "expected_contributions": [
        "Identification and analysis of text-dominant bias in audio-text large models across faithful, adversarial, and irrelevant settings",
        "Development of a modality reliability-based routing framework utilizing energy and temporal evidence as intermediate signals for interference detection",
        "Demonstration that dynamic expert selection (Joint vs. Audio) significantly improves robustness and generalization in emotion recognition tasks"
      ],
      "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., IEMOCAP, MELD) potentially augmented with adversarial/irrelevant samples to simulate real-world inconsistencies. Metrics: Accuracy, Weighted Accuracy (WA), Unweighted Accuracy (UA), F1-score. Baselines: Standard audio-text LMMs, static fusion methods. Ablations: Impact of the routing mechanism, the 'text interference' representation module, and the adapter training to validate the modular design.",
      "keywords_en": [
        "Audio-Text Large Models",
        "Text-Dominant Bias",
        "Robust Emotion Recognition",
        "Modality Routing",
        "Adversarial Settings",
        "Modular Multimodal Learning",
        "Dynamic Fusion"
      ],
      "keywords_zh": [
        "音频-文本大模型",
        "文本主导偏见",
        "稳健情绪识别",
        "模态路由",
        "对抗设定",
        "模块化多模态学习",
        "动态融合"
      ],
      "assumptions": {
        "explicit": [
          "The research focuses on audio-text large models",
          "The problem involves text-dominant bias",
          "Three settings are considered: faithful, adversarial, and irrelevant",
          "The method uses energy and temporal evidence",
          "The solution involves training lightweight gates/adapters",
          "Inference involves selecting between Joint or Audio experts",
          "The target application is emotion recognition"
        ],
        "inferred": [
          "The 'Audio expert' implies a model component capable of processing audio-only inputs effectively",
          "Energy refers to acoustic signal energy features",
          "Temporal evidence refers to time-series alignment or duration features",
          "The framework aims to reduce computational cost or improve accuracy by selectively ignoring text",
          "The 'text interference' representation serves as an intermediate signal for routing decisions"
        ]
      }
    }
  }
}