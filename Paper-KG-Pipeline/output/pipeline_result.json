{
  "user_idea": "我们研究音频-文本大模型的文本主导偏见：在 faithful/adversarial/irrelevant 三类设定下，提出基于模态可信度的路由/融合框架。通过构造“文本干扰”表示能量与时序证据，训练轻量门控/适配器，在推理时选择 Joint 或 Audio 专家，实现对抗文本误导下的稳健情绪识别",
  "success": true,
  "iterations": 1,
  "selected_patterns": {
    "stability": [
      "pattern_74",
      "pattern_112",
      "pattern_115",
      "pattern_106",
      "pattern_3"
    ],
    "novelty": [
      "pattern_106",
      "pattern_3",
      "pattern_113",
      "pattern_95",
      "pattern_112"
    ],
    "domain_distance": [
      "pattern_106",
      "pattern_112",
      "pattern_113",
      "pattern_13",
      "pattern_89"
    ]
  },
  "final_story": {
    "title": "Mitigating Text-Dominance Bias in Audio-Text Large Models via Modality Reliability and Dynamic Routing",
    "abstract": "Audio-Text Large Models (ATLMs) often suffer from text-dominance bias, over-relying on textual cues at the expense of acoustic information. This limits robustness in real-world scenarios where text may be adversarial or irrelevant. We propose a reliability-based routing framework to address this. By constructing a 'text interference' representation using energy and temporal evidence, our model quantifies the trustworthiness of textual input. We then train lightweight gating mechanisms to dynamically route inputs to either a Joint expert or an Audio expert during inference. This modular approach ensures robust emotion recognition by suppressing misleading text and prioritizing reliable modalities, significantly improving performance across faithful, adversarial, and irrelevant settings.",
    "problem_framing": "We reframe the challenge of multimodal emotion recognition from a simple feature aggregation problem to a conflict resolution problem characterized by text-dominance bias. While existing models assume consistent and cooperative inputs, real-world data often contains adversarial or irrelevant text that misleads the model. This static view of multimodal integration fails to account for the varying reliability of modalities, causing models to ignore critical acoustic cues when text is deceptive.",
    "gap_pattern": "Current ATLMs fail to handle text-dominance bias because they rely on static fusion strategies that indiscriminately combine audio and text features. These approaches lack the mechanism to detect when text is interfering with the true emotional signal. Consequently, they suffer from significant performance degradation in adversarial settings, as they cannot dynamically adapt their reasoning process to exclude unreliable textual information.",
    "solution": "We propose a modular composition strategy that transforms multimodal fusion into a dynamic reasoning step based on modality reliability. Drawing on principles of modular composition, we introduce a 'text interference' representation that captures energy and temporal evidence of conflict. This intermediate representation guides lightweight gating mechanisms, effectively training the model to assess modality trustworthiness. During inference, the framework dynamically routes the input to the most appropriate expert—either a Joint expert for consistent inputs or an Audio expert when text interference is detected—thereby ensuring robust emotion recognition.",
    "method_skeleton": "Construct a 'text interference' representation utilizing energy and temporal evidence to quantify multimodal conflict; train lightweight gating mechanisms and adapters to learn modality trustworthiness without full retraining; implement dynamic routing logic to select between Joint or Audio experts based on the interference representation during inference.",
    "innovation_claims": [
      "Transform multimodal fusion from static aggregation to dynamic reliability-based routing by introducing modality trustworthiness as a decision criterion, enabling adaptive expert selection for robust reasoning.",
      "Reframe text-dominance bias mitigation as a quantifiable interference problem by modeling 'text interference' through energy and temporal evidence, providing an explicit signal for conflict detection.",
      "Enhance the generalization capacity of Audio-Text Large Models by integrating lightweight adapters that route to audio-only experts under adversarial conditions, effectively neutralizing misleading textual cues."
    ],
    "experiments_plan": "Evaluation on CMU-MOSEI and IEMOCAP datasets using faithful, adversarial, and irrelevant text splits. Metrics include Accuracy and F1-score. Baselines comprise vanilla ATLMs and static fusion models. Ablation studies analyze the impact of the interference representation and routing logic, demonstrating robustness improvements specifically in adversarial settings."
  },
  "review_history": [
    {
      "pass": true,
      "avg_score": 6.5233333333332375,
      "reviews": [
        {
          "reviewer": "Reviewer A",
          "role": "Methodology",
          "score": 6.579999999999903,
          "feedback": "Main gaps: Limited evaluation on only two datasets, Lack of theoretical grounding for interference representation, Vague implementation details for key components. Anchored against 7 papers."
        },
        {
          "reviewer": "Reviewer B",
          "role": "Novelty",
          "score": 7.149999999999891,
          "feedback": "Main gaps: Limited theoretical justification for the 'text interference' representation and how it quantifies multimodal conflict., Lack of comparison with other state-of-the-art methods for addressing text-dominance bias in multimodal models., Potential scalability issues with the dynamic routing approach for larger models or more complex multimodal scenarios.. Anchored against 7 papers."
        },
        {
          "reviewer": "Reviewer C",
          "role": "Storyteller",
          "score": 5.839999999999919,
          "feedback": "Main gaps: Limited evaluation on only two datasets, Lack of comparison with state-of-the-art methods, Limited analysis of computational efficiency. Anchored against 7 papers."
        }
      ],
      "main_issue": "domain_distance",
      "suggestions": [
        "从domain_distance维度选择跨域Pattern",
        "引入不同视角优化叙事"
      ],
      "audit": {
        "pattern_id": "pattern_74",
        "anchors": [
          {
            "paper_id": "l0gZS0sAlf",
            "title": "Ensembles of Low-Rank Expert Adapters",
            "pattern_id": "pattern_74",
            "score10": 5.920000000000001,
            "review_count": 6,
            "dispersion10": 1.08,
            "weight": 0.9355337255073621
          },
          {
            "paper_id": "i0zzO7Hslk",
            "title": "Parameter and Memory Efficient Pretraining via Low-rank Riemannian Optimization",
            "pattern_id": "pattern_74",
            "score10": 6.004,
            "review_count": 5,
            "dispersion10": 0.7200000000000006,
            "weight": 1.0417206216442176
          },
          {
            "paper_id": "BTKAeLqLMw",
            "title": "What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning",
            "pattern_id": "pattern_74",
            "score10": 6.175,
            "review_count": 4,
            "dispersion10": 1.2600000000000002,
            "weight": 0.7121406692186284
          },
          {
            "paper_id": "3E8YNv1HjU",
            "title": "Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon",
            "pattern_id": "pattern_74",
            "score10": 6.292000000000001,
            "review_count": 5,
            "dispersion10": 1.2600000000000002,
            "weight": 0.7928139244371923
          },
          {
            "paper_id": "dGVZwyq5tV",
            "title": "Training-Free Activation Sparsity in Large Language Models",
            "pattern_id": "pattern_74",
            "score10": 6.544,
            "review_count": 5,
            "dispersion10": 0.9000000000000008,
            "weight": 0.9430312995937128
          },
          {
            "paper_id": "-Aw0rrrPUF",
            "title": "GLM-130B: An Open Bilingual Pre-trained Model",
            "pattern_id": "pattern_74",
            "score10": 7.624,
            "review_count": 5,
            "dispersion10": 1.7550000000000006,
            "weight": 0.6503664135129055
          },
          {
            "paper_id": "gUL6zYN4Uaf",
            "title": "Cramming: Training a language model on a single GPU in one day",
            "pattern_id": "pattern_74",
            "score10": 5.788,
            "review_count": 5,
            "dispersion10": 2.1149999999999998,
            "weight": 0.5752036819351701
          }
        ],
        "role_details": {
          "Methodology": {
            "comparisons": [
              {
                "paper_id": "l0gZS0sAlf",
                "judgement": "better",
                "confidence": 0.7,
                "rationale": "Dynamic routing for text-dominance bias is more innovative than adapter ensembles, score10: 5.9"
              },
              {
                "paper_id": "i0zzO7Hslk",
                "judgement": "tie",
                "confidence": 0.6,
                "rationale": "Both address efficiency in different contexts with comparable rigor, score10: 6.0"
              },
              {
                "paper_id": "BTKAeLqLMw",
                "judgement": "better",
                "confidence": 0.65,
                "rationale": "Targeted robustness methodology is more technical than data selection study, score10: 6.2"
              },
              {
                "paper_id": "3E8YNv1HjU",
                "judgement": "tie",
                "confidence": 0.6,
                "rationale": "Different research focuses with comparable methodological contributions, score10: 6.3"
              },
              {
                "paper_id": "dGVZwyq5tV",
                "judgement": "tie",
                "confidence": 0.55,
                "rationale": "Both propose novel approaches in different domains with similar complexity, score10: 6.5"
              },
              {
                "paper_id": "-Aw0rrrPUF",
                "judgement": "worse",
                "confidence": 0.7,
                "rationale": "GLM-130B represents more substantial engineering with broader impact, score10: 7.6"
              },
              {
                "paper_id": "gUL6zYN4Uaf",
                "judgement": "better",
                "confidence": 0.65,
                "rationale": "Targeted methodology is more sophisticated than cramming approach, score10: 5.8"
              }
            ],
            "main_gaps": [
              "Limited evaluation on only two datasets",
              "Lack of theoretical grounding for interference representation",
              "Vague implementation details for key components"
            ],
            "score": 6.579999999999903,
            "loss": 0.07494826485444674,
            "avg_confidence": 0.6357142857142858,
            "monotonic_violations": 1
          },
          "Novelty": {
            "comparisons": [
              {
                "paper_id": "l0gZS0sAlf",
                "judgement": "better",
                "confidence": 0.7,
                "rationale": "Story's text-dominance bias approach is more novel than general ensemble methods with low-rank adapters (score10: 5.9)."
              },
              {
                "paper_id": "i0zzO7Hslk",
                "judgement": "better",
                "confidence": 0.7,
                "rationale": "Story's text-dominance bias approach is more novel than general efficient pretraining methods (score10: 6.0)."
              },
              {
                "paper_id": "BTKAeLqLMw",
                "judgement": "better",
                "confidence": 0.6,
                "rationale": "Story's dynamic routing approach is more novel than data selection for instruction tuning (score10: 6.2)."
              },
              {
                "paper_id": "3E8YNv1HjU",
                "judgement": "better",
                "confidence": 0.6,
                "rationale": "Story's dynamic routing approach is more novel than studying memorization in LMs (score10: 6.3)."
              },
              {
                "paper_id": "dGVZwyq5tV",
                "judgement": "better",
                "confidence": 0.6,
                "rationale": "Story's dynamic routing approach is more novel than achieving activation sparsity (score10: 6.5)."
              },
              {
                "paper_id": "-Aw0rrrPUF",
                "judgement": "worse",
                "confidence": 0.7,
                "rationale": "GLM-130B's large-scale bilingual model is more novel than Story's text-dominance bias approach (score10: 7.6)."
              },
              {
                "paper_id": "gUL6zYN4Uaf",
                "judgement": "better",
                "confidence": 0.7,
                "rationale": "Story's dynamic routing approach is more novel than efficient training on limited resources (score10: 5.8)."
              }
            ],
            "main_gaps": [
              "Limited theoretical justification for the 'text interference' representation and how it quantifies multimodal conflict.",
              "Lack of comparison with other state-of-the-art methods for addressing text-dominance bias in multimodal models.",
              "Potential scalability issues with the dynamic routing approach for larger models or more complex multimodal scenarios."
            ],
            "score": 7.149999999999891,
            "loss": 0.030392361609058335,
            "avg_confidence": 0.6571428571428571,
            "monotonic_violations": 0
          },
          "Storyteller": {
            "comparisons": [
              {
                "paper_id": "l0gZS0sAlf",
                "judgement": "better",
                "confidence": 0.7,
                "rationale": "Story's novel text-dominance bias approach is more innovative than general ensemble methods, score10: 5.9"
              },
              {
                "paper_id": "i0zzO7Hslk",
                "judgement": "worse",
                "confidence": 0.6,
                "rationale": "Anchor's fundamental optimization technique has broader impact than Story's specific application, score10: 6.0"
              },
              {
                "paper_id": "BTKAeLqLMw",
                "judgement": "worse",
                "confidence": 0.6,
                "rationale": "Anchor's comprehensive data selection study is more valuable than Story's specific application, score10: 6.2"
              },
              {
                "paper_id": "3E8YNv1HjU",
                "judgement": "worse",
                "confidence": 0.6,
                "rationale": "Anchor's fundamental memorization analysis is more impactful than Story's specific application, score10: 6.3"
              },
              {
                "paper_id": "dGVZwyq5tV",
                "judgement": "worse",
                "confidence": 0.6,
                "rationale": "Anchor's innovative sparsity approach has broader implications than Story's specific application, score10: 6.5"
              },
              {
                "paper_id": "-Aw0rrrPUF",
                "judgement": "worse",
                "confidence": 0.8,
                "rationale": "Anchor's substantial bilingual model contribution has broader impact than Story's specific application, score10: 7.6"
              },
              {
                "paper_id": "gUL6zYN4Uaf",
                "judgement": "better",
                "confidence": 0.7,
                "rationale": "Story's novel text-dominance bias approach is more innovative than anchor's training efficiency focus, score10: 5.8"
              }
            ],
            "main_gaps": [
              "Limited evaluation on only two datasets",
              "Lack of comparison with state-of-the-art methods",
              "Limited analysis of computational efficiency"
            ],
            "score": 5.839999999999919,
            "loss": 0.251201876430916,
            "avg_confidence": 0.6571428571428571,
            "monotonic_violations": 0
          }
        },
        "anchors_rounds": [
          [
            {
              "paper_id": "l0gZS0sAlf",
              "title": "Ensembles of Low-Rank Expert Adapters",
              "pattern_id": "pattern_74",
              "score10": 5.920000000000001,
              "review_count": 6,
              "dispersion10": 1.08,
              "weight": 0.9355337255073621
            },
            {
              "paper_id": "i0zzO7Hslk",
              "title": "Parameter and Memory Efficient Pretraining via Low-rank Riemannian Optimization",
              "pattern_id": "pattern_74",
              "score10": 6.004,
              "review_count": 5,
              "dispersion10": 0.7200000000000006,
              "weight": 1.0417206216442176
            },
            {
              "paper_id": "BTKAeLqLMw",
              "title": "What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning",
              "pattern_id": "pattern_74",
              "score10": 6.175,
              "review_count": 4,
              "dispersion10": 1.2600000000000002,
              "weight": 0.7121406692186284
            },
            {
              "paper_id": "3E8YNv1HjU",
              "title": "Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon",
              "pattern_id": "pattern_74",
              "score10": 6.292000000000001,
              "review_count": 5,
              "dispersion10": 1.2600000000000002,
              "weight": 0.7928139244371923
            },
            {
              "paper_id": "dGVZwyq5tV",
              "title": "Training-Free Activation Sparsity in Large Language Models",
              "pattern_id": "pattern_74",
              "score10": 6.544,
              "review_count": 5,
              "dispersion10": 0.9000000000000008,
              "weight": 0.9430312995937128
            },
            {
              "paper_id": "-Aw0rrrPUF",
              "title": "GLM-130B: An Open Bilingual Pre-trained Model",
              "pattern_id": "pattern_74",
              "score10": 7.624,
              "review_count": 5,
              "dispersion10": 1.7550000000000006,
              "weight": 0.6503664135129055
            },
            {
              "paper_id": "gUL6zYN4Uaf",
              "title": "Cramming: Training a language model on a single GPU in one day",
              "pattern_id": "pattern_74",
              "score10": 5.788,
              "review_count": 5,
              "dispersion10": 2.1149999999999998,
              "weight": 0.5752036819351701
            }
          ]
        ],
        "pass": {
          "mode": "two_of_three_q75_and_avg_ge_q50",
          "used_distribution": "pattern",
          "pattern_paper_count": 156,
          "q50": 6.175,
          "q75": 6.292000000000001,
          "count_roles_ge_q75": 2,
          "roles_ge_q75": {
            "Methodology": true,
            "Novelty": true,
            "Storyteller": false
          },
          "avg_ge_q50": true,
          "avg_score": 6.5233333333332375
        }
      }
    }
  ],
  "results_dir": "results/run_20260203_061842_26735_f974d7",
  "novelty_report": {
    "run_id": "run_20260203_061842_26735_f974d7",
    "created_at": "2026-02-03T06:39:40.645966+00:00",
    "user_idea": "我们研究音频-文本大模型的文本主导偏见：在 faithful/adversarial/irrelevant 三类设定下，提出基于模态可信度的路由/融合框架。通过构造“文本干扰”表示能量与时序证据，训练轻量门控/适配器，在推理时选择 Joint 或 Audio 专家，实现对抗文本误导下的稳健情绪识别",
    "embedding_available": true,
    "embedding_model": "Qwen/Qwen3-Embedding-8B",
    "top_k": 100,
    "thresholds": {
      "high": 0.88,
      "medium": 0.82
    },
    "risk_level": "low",
    "max_similarity": 0.5893037915229797,
    "candidates": [
      {
        "paper_id": "AV7OXVlAyi",
        "title": "Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality",
        "pattern_id": "pattern_13",
        "domain": "Machine Learning",
        "text_hash": "2b00d0adacffa9b4b76e94a68ebe21328497754c613a608494520d4cccc2d32e",
        "cosine": 0.5893037915229797,
        "keyword_overlap": 0.1285140562248996
      },
      {
        "paper_id": "ePJrZLIqpV",
        "title": "Rethinking Audio-Visual Adversarial Vulnerability from Temporal and Modality Perspectives",
        "pattern_id": "",
        "domain": "Machine Learning",
        "text_hash": "d3f943cf44bf7e970363fa0aedc8716afdd67cc46e915d54ff2861ebe664249d",
        "cosine": 0.5855258703231812,
        "keyword_overlap": 0.09652509652509653
      },
      {
        "paper_id": "TPZRq4FALB",
        "title": "Test-time Adaptation against Multi-modal Reliability Bias",
        "pattern_id": "",
        "domain": "Machine Learning",
        "text_hash": "85ecbb7da83a1aaae886311195041898f0bd5a40aeffc5097b95fc79d213ed50",
        "cosine": 0.5853828191757202,
        "keyword_overlap": 0.10358565737051793
      },
      {
        "paper_id": "1SYUKPeM12",
        "title": "Aligned Better, Listen Better for Audio-Visual Large Language Models",
        "pattern_id": "pattern_13",
        "domain": "Machine Learning",
        "text_hash": "e80f050a72eb521e9a39c1ad0e8bf7d83712f9be0580d5b59057d0d81b9fcc27",
        "cosine": 0.5699200630187988,
        "keyword_overlap": 0.1141732283464567
      },
      {
        "paper_id": "l60EM8md3t",
        "title": "Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation",
        "pattern_id": "",
        "domain": "Machine Learning",
        "text_hash": "283fd505312f835e0c070c65fdd13e3d83bcd45f433e9e03aed7bb9a3e4e5108",
        "cosine": 0.5526226162910461,
        "keyword_overlap": 0.0963855421686747
      },
      {
        "paper_id": "74vnDs1R97",
        "title": "Wayward Concepts In Multimodal Models",
        "pattern_id": "pattern_51",
        "domain": "Machine Learning",
        "text_hash": "e3575cced82c01c31692bb5ad0e1b47a531a5a79bf5f4da6c81d24b34c00224a",
        "cosine": 0.5514384508132935,
        "keyword_overlap": 0.0796812749003984
      },
      {
        "paper_id": "jTEKTdI3K9",
        "title": "AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models",
        "pattern_id": "pattern_13",
        "domain": "Machine Learning",
        "text_hash": "f46910c6a6945456aa4c0dba4aa96ee893bbd85f0ed1a42faf6d3c9a926a10f7",
        "cosine": 0.5447805523872375,
        "keyword_overlap": 0.09765625
      },
      {
        "paper_id": "vtT09dYPGI",
        "title": "Routing Experts: Learning to Route Dynamic Experts in Existing Multi-modal Large Language Models",
        "pattern_id": "pattern_74",
        "domain": "Machine Learning",
        "text_hash": "59b22728a0b85817d085cc72b342dae6e768ff39578b9a159327d2d8445e6d8c",
        "cosine": 0.5387470126152039,
        "keyword_overlap": 0.1388888888888889
      },
      {
        "paper_id": "vbmSSIhKAM",
        "title": "VoxDialogue: Can Spoken Dialogue Systems Understand Information Beyond Words?",
        "pattern_id": "pattern_58",
        "domain": "Natural Language Processing",
        "text_hash": "7774ff496e604d63f22e6484690265ff5fec1017bc86a0ca9d5a457451c90865",
        "cosine": 0.5363541841506958,
        "keyword_overlap": 0.06319702602230483
      },
      {
        "paper_id": "H-T3F0dMbyj",
        "title": "CLIPSep: Learning Text-queried Sound Separation with Noisy Unlabeled Videos",
        "pattern_id": "pattern_7",
        "domain": "Machine Learning",
        "text_hash": "eb4f5bb7908e3c70f31e81e249b34175823164cc723accd9cda3bcc4ffcfb121",
        "cosine": 0.5360878705978394,
        "keyword_overlap": 0.09561752988047809
      }
    ],
    "notes": [
      "index_reused"
    ],
    "report_path": "results/run_20260203_061842_26735_f974d7/novelty_report.json",
    "pivot_attempts": 0,
    "action": "pivot"
  },
  "recall_audit": {
    "final_top_k": [
      {
        "pattern_id": "pattern_112",
        "name": "Reframing Multimodal Reasoning Challenges",
        "final_score": 0.7834140447991009,
        "path1_score": 0.7834140447991009,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 115
      },
      {
        "pattern_id": "pattern_13",
        "name": "Hallucination Mitigation via Multimodal Alignment",
        "final_score": 0.40524548143096856,
        "path1_score": 0.40524548143096856,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 30
      },
      {
        "pattern_id": "pattern_113",
        "name": "Reframing Multimodal Learning Narratives",
        "final_score": 0.38877741327543114,
        "path1_score": 0.38877741327543114,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 46
      },
      {
        "pattern_id": "pattern_89",
        "name": "Contextual Adaptation and Interpretability in Language Models",
        "final_score": 0.3801288265092605,
        "path1_score": 0.3801288265092605,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 16
      },
      {
        "pattern_id": "pattern_74",
        "name": "Democratizing Large Language Model Accessibility",
        "final_score": 0.2928511691502884,
        "path1_score": 0.22432482714044957,
        "path2_score": 0.0,
        "path3_score": 0.06852634200983881,
        "cluster_size": 156
      },
      {
        "pattern_id": "pattern_115",
        "name": "Semantic Alignment for Compositional Generation",
        "final_score": 0.2171061842051729,
        "path1_score": 0.2171061842051729,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 107
      },
      {
        "pattern_id": "pattern_96",
        "name": "Reframing Language Generation Challenges",
        "final_score": 0.2019288707986188,
        "path1_score": 0.2019288707986188,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 33
      },
      {
        "pattern_id": "pattern_3",
        "name": "Anomaly Detection Reframed for Interpretability",
        "final_score": 0.200063333908376,
        "path1_score": 0.200063333908376,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 46
      },
      {
        "pattern_id": "pattern_95",
        "name": "Reframing Knowledge Integration Narratives",
        "final_score": 0.18703554876372483,
        "path1_score": 0.18703554876372483,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 27
      },
      {
        "pattern_id": "pattern_106",
        "name": "Adaptive Dynamic Reasoning Trajectories",
        "final_score": 0.1860885272780084,
        "path1_score": 0.1860885272780084,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 49
      }
    ],
    "path1": {
      "top_ideas": [
        {
          "idea_id": "idea_5003",
          "similarity": 0.5693821754402342,
          "snippet": "Introduce a comprehensive framework to evaluate and enhance the adversarial robustness of multimodal language model agents in real environments.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_6838",
          "similarity": 0.5608120678511239,
          "snippet": "Introduce a dynamic expert routing method to optimize path selection in multimodal large language models without altering their structure.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_4237",
          "similarity": 0.5427654605129322,
          "snippet": "Introduce a modular framework to seamlessly integrate text and image generation models for high-quality multimodal outputs.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_6950",
          "similarity": 0.5292562639313112,
          "snippet": "Introduce a modular and efficient framework for video-language reasoning that integrates multiple modalities without extensive parameter updates.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7635",
          "similarity": 0.519771639305313,
          "snippet": "Introduce a causal inference framework to address modality prior-induced biases in multimodal large language models by focusing on attention mechanism causality.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_3621",
          "similarity": 0.504822176996547,
          "snippet": "Introduce model arithmetic as a novel inference framework for composing and biasing LLMs to achieve precise control over text generation without retraining.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7660",
          "similarity": 0.50015833477094,
          "snippet": "Introduce a comprehensive benchmark to evaluate and enhance the performance of Multimodal Large Language Models in industrial anomaly detection.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_6804",
          "similarity": 0.4960913364832625,
          "snippet": "Introduce a Chain-of-Action framework to enhance multimodal QA by decomposing questions into reasoning chains and retrieving real-time information to improve faithfulness and reasoning.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_7610",
          "similarity": 0.49334206427210825,
          "snippet": "Alleviate hallucinations in multimodal large language models by dynamically intervening in the eigenspectrum variance of attention weights without complex decoding strategies.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_3276",
          "similarity": 0.49183206391443657,
          "snippet": "Utilize language as an intermediate representation to compose zero-shot multimodal reasoning systems by leveraging pretrained models without additional finetuning.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7100",
          "similarity": 0.4907844151508335,
          "snippet": "Introduce a dynamic context sparsification framework to enhance the efficiency of multimodal large language models without degrading performance.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_5419",
          "similarity": 0.48338114987565123,
          "snippet": "Utilize a graph-based framework to enhance the selection process of Large Language Models by leveraging contextual interactions among tasks, queries, and models.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7558",
          "similarity": 0.48115911803774436,
          "snippet": "Introduce a unified framework for handling multi-image, video, and 3D tasks in large multimodal models using an interleaved data format.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_6957",
          "similarity": 0.4770420372874495,
          "snippet": "Introduce a dynamic data selection approach to enhance continual multimodal instruction tuning by balancing sample efficiency and effectiveness.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_6302",
          "similarity": 0.4705157923937771,
          "snippet": "Introduce a comprehensive dataset and benchmark for evaluating and improving the understanding of dynamic GUI content by multimodal large language models.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7792",
          "similarity": 0.46851662036720576,
          "snippet": "Introduce a method to selectively control LLM responses by analyzing activation patterns and applying conditional steering based on input context.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_3655",
          "similarity": 0.46758887190931203,
          "snippet": "Develop a prompting-based framework to detect and mitigate self-contradictions in large language models without relying on external knowledge retrieval.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_5584",
          "similarity": 0.46694091639749996,
          "snippet": "Introduce a framework to enhance large language models by externally storing and reusing task vectors, improving adaptability without additional training.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_4825",
          "similarity": 0.4669309917582275,
          "snippet": "Introduce a benchmark to evaluate the reasoning capabilities of multimodal large language models in sports contexts through textual and visual tasks.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_1128",
          "similarity": 0.465221318195021,
          "snippet": "Introduce a Selection-Inference framework to enhance multi-step logical reasoning in large language models by alternating between selection and inference steps.",
          "pattern_count": 1
        }
      ],
      "pattern_scores_topn": [
        {
          "pattern_id": "pattern_112",
          "score": 0.7834140447991009
        },
        {
          "pattern_id": "pattern_13",
          "score": 0.40524548143096856
        },
        {
          "pattern_id": "pattern_113",
          "score": 0.38877741327543114
        },
        {
          "pattern_id": "pattern_89",
          "score": 0.3801288265092605
        },
        {
          "pattern_id": "pattern_74",
          "score": 0.22432482714044957
        },
        {
          "pattern_id": "pattern_115",
          "score": 0.2171061842051729
        },
        {
          "pattern_id": "pattern_96",
          "score": 0.2019288707986188
        },
        {
          "pattern_id": "pattern_3",
          "score": 0.200063333908376
        },
        {
          "pattern_id": "pattern_95",
          "score": 0.18703554876372483
        },
        {
          "pattern_id": "pattern_106",
          "score": 0.1860885272780084
        }
      ]
    },
    "path2": {
      "top_domains": [
        {
          "domain_id": "domain_45",
          "name": "Audio Processing",
          "weight": 0.4306128836726603,
          "paper_count": 5
        },
        {
          "domain_id": "domain_82",
          "name": "Speech Recognition",
          "weight": 0.40448047147905286,
          "paper_count": 2
        },
        {
          "domain_id": "domain_48",
          "name": "Speech Processing",
          "weight": 0.3881055043870888,
          "paper_count": 7
        },
        {
          "domain_id": "domain_0",
          "name": "Fairness & Accountability",
          "weight": 0.356021882786588,
          "paper_count": 69
        },
        {
          "domain_id": "domain_63",
          "name": "Neuromorphic Computing",
          "weight": 0.33817386157022816,
          "paper_count": 2
        }
      ],
      "top_subdomains": [
        {
          "domain_id": "domain_45",
          "subdomains": [
            {
              "name": "Diffusion Models",
              "score": 0.314953212745662
            },
            {
              "name": "Contrastive Learning",
              "score": 0.299556276554952
            }
          ]
        },
        {
          "domain_id": "domain_82",
          "subdomains": [
            {
              "name": "Speech Synthesis",
              "score": 0.3197465236283434
            },
            {
              "name": "Contrastive Learning",
              "score": 0.29983286004462467
            }
          ]
        },
        {
          "domain_id": "domain_48",
          "subdomains": [
            {
              "name": "Speech Synthesis",
              "score": 0.3197465236283434
            },
            {
              "name": "Diffusion Models",
              "score": 0.3149532125392339
            },
            {
              "name": "Contrastive Learning",
              "score": 0.299556276554952
            }
          ]
        },
        {
          "domain_id": "domain_0",
          "subdomains": [
            {
              "name": "Out-of-Distribution Detection",
              "score": 0.369548942830209
            },
            {
              "name": "Bias Mitigation",
              "score": 0.3236620736690997
            },
            {
              "name": "Robustness",
              "score": 0.3190638599091825
            },
            {
              "name": "Contrastive Learning",
              "score": 0.30005950059088454
            },
            {
              "name": "Federated Learning",
              "score": 0.2844667699310532
            }
          ]
        },
        {
          "domain_id": "domain_63",
          "subdomains": [
            {
              "name": "Robustness",
              "score": 0.3195266978213968
            },
            {
              "name": "Contrastive Learning",
              "score": 0.299556276554952
            },
            {
              "name": "Neuromorphic Computing",
              "score": 0.24184529693939433
            }
          ]
        }
      ],
      "candidate_stats": [
        {
          "domain_id": "domain_45",
          "candidates_before": 1,
          "candidates_after": 1
        },
        {
          "domain_id": "domain_82",
          "candidates_before": 1,
          "candidates_after": 1
        },
        {
          "domain_id": "domain_48",
          "candidates_before": 2,
          "candidates_after": 2
        },
        {
          "domain_id": "domain_0",
          "candidates_before": 5,
          "candidates_after": 5
        },
        {
          "domain_id": "domain_63",
          "candidates_before": 1,
          "candidates_after": 1
        }
      ],
      "pattern_scores_topn": [
        {
          "pattern_id": "pattern_9",
          "score": 0.009425053272817445
        },
        {
          "pattern_id": "pattern_7",
          "score": 0.004816646078329966
        },
        {
          "pattern_id": "pattern_8",
          "score": 0.001558213476541804
        },
        {
          "pattern_id": "pattern_87",
          "score": 0.0014627681795843766
        },
        {
          "pattern_id": "pattern_45",
          "score": 0.0014088467967618339
        }
      ],
      "subdomain_taxonomy_used": true,
      "raw_subdomain_count": 319,
      "canonical_subdomain_count": 58,
      "stoplist_count": 12
    },
    "path3": {
      "top_papers": [
        {
          "paper_id": "9WYMDgxDac",
          "similarity": 0.5004964382842869,
          "title": "Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models",
          "quality": 0.596,
          "review_count": 5
        },
        {
          "paper_id": "vtT09dYPGI",
          "similarity": 0.5163599989890681,
          "title": "Routing Experts: Learning to Route Dynamic Experts in Existing Multi-modal Large Language Models",
          "quality": 0.576,
          "review_count": 5
        },
        {
          "paper_id": "pfuqQQCB34",
          "similarity": 0.2996266108103354,
          "title": "Variance Reduction is an Antidote to Byzantines: Better Rates, Weaker Assumptions and Communication Compression as a Cherry on the Top",
          "quality": 0.6858333333333334,
          "review_count": 6
        }
      ],
      "pattern_scores_topn": [
        {
          "pattern_id": "pattern_87",
          "score": 0.0711137371286365
        },
        {
          "pattern_id": "pattern_74",
          "score": 0.06852634200983881
        }
      ]
    }
  },
  "review_summary": {
    "total_reviews": 1,
    "final_score": 6.5233333333332375
  },
  "refinement_summary": {
    "total_refinements": 0,
    "issues_addressed": []
  },
  "verification_summary": {
    "collision_detected": false,
    "max_similarity": 0.5893037915229797
  },
  "idea_packaging": {
    "raw_idea": "我们研究音频-文本大模型的文本主导偏见：在 faithful/adversarial/irrelevant 三类设定下，提出基于模态可信度的路由/融合框架。通过构造“文本干扰”表示能量与时序证据，训练轻量门控/适配器，在推理时选择 Joint 或 Audio 专家，实现对抗文本误导下的稳健情绪识别",
    "brief_a": {
      "motivation": "Current Audio-Text Large Models (ATLMs) suffer from 'text-dominance bias,' where the model over-relies on textual input and ignores acoustic cues. This leads to poor performance in real-world scenarios where text may be misleading, adversarial, or irrelevant to the actual emotional state conveyed in audio.",
      "problem_definition": "Task: Robust Multimodal Emotion Recognition. Input: Audio and Text streams. Output: Emotion classification. Settings: 1) Faithful (consistent audio-text), 2) Adversarial (text misleads emotion), 3) Irrelevant (text unrelated to emotion).",
      "constraints": [
        "robustness",
        "efficiency (lightweight modules)",
        "adaptability to large models"
      ],
      "technical_plan": "Propose a routing and fusion framework based on modality reliability. 1) Construct a 'text interference' representation utilizing energy and temporal evidence to quantify conflict. 2) Train lightweight gating mechanisms or adapters to learn modality trustworthiness. 3) Inference: Dynamically route the input to either a 'Joint' expert (if modalities are consistent) or an 'Audio' expert (if text interference is detected) to mitigate bias.",
      "expected_contributions": [
        "Identification and analysis of text-dominance bias in Audio-Text Large Models",
        "A novel reliability-based routing framework utilizing 'text interference' representation (energy & temporal evidence)",
        "Significant improvement in robustness for emotion recognition under adversarial and irrelevant text conditions"
      ],
      "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., CMU-MOSEI, IEMOCAP) with constructed adversarial/irrelevant splits. Metrics: Accuracy, F1-score. Baselines: Vanilla ATLMs, static fusion models. Ablations: Impact of the interference representation, routing logic, and adapter training. Robustness Settings: Performance comparison specifically on Faithful vs. Adversarial vs. Irrelevant test sets.",
      "keywords_en": [
        "Audio-Text Large Models",
        "Text-Dominance Bias",
        "Modality Reliability",
        "Robust Emotion Recognition",
        "Dynamic Routing",
        "Adversarial Settings"
      ],
      "keywords_zh": [
        "音频-文本大模型",
        "文本主导偏见",
        "模态可信度",
        "稳健情绪识别",
        "动态路由",
        "对抗设定"
      ],
      "assumptions": {
        "explicit": [
          "Research focuses on Audio-Text Large Models",
          "Evaluation covers faithful, adversarial, and irrelevant settings",
          "Method involves lightweight gating/adapter training",
          "Inference involves selecting between Joint or Audio experts"
        ],
        "inferred": [
          "The 'text interference' representation requires a method to quantify energy and temporal alignment (inferred)",
          "The model assumes audio is the ground truth for emotion when text is adversarial (inferred)",
          "The framework is designed to be a plug-and-play module or fine-tuning strategy for existing ATLMs (inferred)"
        ]
      }
    },
    "query_a": "Keywords: Audio-Text Large Models, Text-Dominance Bias, Modality Reliability, Robust Emotion Recognition, Dynamic Routing, Adversarial Settings Task: Robust Multimodal Emotion Recognition. Input: Audio and Text streams. Output: Emotion classification. Settings: 1) Faithful (consistent audio-text), 2) Adversarial (text misleads emotion), 3) Irrelevant (text unrelated to emotion). Constraints: robustness, efficiency (lightweight modules), adaptability to large models Propose a routing and fusion framework based on modality reliability. 1) Construct a 'text interference' representation utilizing energy and temporal evidence to quantify conflict. 2) Train lightweight gating mechanisms or adapters to learn modality trustworthiness. 3) Inference: Dynamically route the input to either a 'Joint' expert (if modalities are consistent) or an 'Audio' expert (if text interference is detected) to mitigate bias. 关键词: 音频-文本大模型，文本主导偏见，模态可信度，稳健情绪识别，动态路由，对抗设定",
    "candidates": [
      {
        "pattern_id": "pattern_112",
        "pattern_name": "Reframing Multimodal Reasoning Challenges",
        "score": 0.9475998943153746,
        "brief": {
          "motivation": "Current Audio-Text Large Models (ATLMs) suffer from 'text-dominance bias,' where the model over-relies on textual input and ignores acoustic cues, limiting their generalization capacity in real-world scenarios. Drawing on the pattern of reframing multimodal reasoning challenges, we argue that static fusion approaches are insufficient for handling conflicting inputs. Instead, we propose a dynamic, modular approach that treats modality selection as a reasoning step, enhancing robustness when text is misleading, adversarial, or irrelevant to the actual emotional state.",
          "problem_definition": "Task: Robust Multimodal Emotion Recognition. Input: Audio and Text streams. Output: Emotion classification. Settings: 1) Faithful (consistent audio-text), 2) Adversarial (text misleads emotion), 3) Irrelevant (text unrelated to emotion). The core challenge is maintaining high performance and generalization when multimodal inputs are misaligned, a common failure mode in generic multimodal reasoning.",
          "constraints": [
            "Robustness to adversarial and irrelevant text",
            "Efficiency (lightweight gating/adapter modules)",
            "Adaptability to existing Audio-Text Large Models",
            "Modular integration for dynamic expert selection"
          ],
          "technical_plan": "Propose a routing and fusion framework based on modality reliability, inspired by modular composition strategies. 1) Construct a 'text interference' representation utilizing energy and temporal evidence to quantify conflict, serving as an intermediate representation for decision-making. 2) Train lightweight gating mechanisms or adapters to learn modality trustworthiness without full retraining. 3) Inference: Dynamically route the input to either a 'Joint' expert (if modalities are consistent) or an 'Audio' expert (if text interference is detected), effectively composing the most reliable expert for the specific reasoning context.",
          "expected_contributions": [
            "Identification and analysis of text-dominance bias in Audio-Text Large Models",
            "A novel reliability-based routing framework utilizing 'text interference' as an intermediate representation for expert selection",
            "Significant improvement in robustness and generalization capacity for emotion recognition under adversarial and irrelevant text conditions",
            "Demonstration of a modular approach to multimodal fusion that adapts to input reliability"
          ],
          "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., CMU-MOSEI, IEMOCAP) with constructed adversarial/irrelevant splits to test generalization. Metrics: Accuracy, F1-score. Baselines: Vanilla ATLMs, static fusion models, and other modular composition approaches. Ablations: Impact of the interference representation, routing logic, and adapter training. Robustness Settings: Performance comparison specifically on Faithful vs. Adversarial vs. Irrelevant test sets to measure generalization capacity.",
          "keywords_en": [
            "Audio-Text Large Models",
            "Text-Dominance Bias",
            "Modality Reliability",
            "Robust Emotion Recognition",
            "Dynamic Routing",
            "Adversarial Settings",
            "Modular Composition",
            "Intermediate Representation"
          ],
          "keywords_zh": [
            "音频-文本大模型",
            "文本主导偏见",
            "模态可信度",
            "稳健情绪识别",
            "动态路由",
            "对抗设定",
            "模块化组合",
            "中间表示"
          ],
          "assumptions": {
            "explicit": [
              "Research focuses on Audio-Text Large Models",
              "Evaluation covers faithful, adversarial, and irrelevant settings",
              "Method involves lightweight gating/adapter training",
              "Inference involves selecting between Joint or Audio experts"
            ],
            "inferred": [
              "The 'text interference' representation requires a method to quantify energy and temporal alignment",
              "The model assumes audio is the ground truth for emotion when text is adversarial",
              "The framework is designed to be a plug-and-play module or fine-tuning strategy for existing ATLMs",
              "Modular routing improves generalization compared to static fusion"
            ]
          }
        },
        "query": "Keywords: Audio-Text Large Models, Text-Dominance Bias, Modality Reliability, Robust Emotion Recognition, Dynamic Routing, Adversarial Settings, Modular Composition, Intermediate Representation Task: Robust Multimodal Emotion Recognition. Input: Audio and Text streams. Output: Emotion classification. Settings: 1) Faithful (consistent audio-text), 2) Adversarial (text misleads emotion), 3) Irrelevant (text unrelated to emotion). The core challenge is maintaining high performance and generalization when multimodal inputs are misaligned, a common failure mode in generic multimodal reasoning. Constraints: Robustness to adversarial and irrelevant text, Efficiency (lightweight gating/adapter modules), Adaptability to existing Audio-Text Large Models, Modular integration for dynamic expert selection Propose a routing and fusion framework based on modality reliability, inspired by modular composition strategies. 1) Construct a 'text interference' representation utilizing energy and temporal evidence to quantify conflict, serving as an intermediate representation for decision-making. 2) Train lightweight gating mechanisms or adapters to learn modality trustworthiness without full retraining. 3) Inference: Dynamically route the input to either a 'Joint' expert (if modalities are consistent) or an 'Audio' expert (if text interference is detected), effectively composing the most reliable expert for the specific reasoning context. 关键词: 音频-文本大模型，文本主导偏见，模态可信度，稳健情绪识别，动态路由，对抗设定，模块化组合，中间表示"
      },
      {
        "pattern_id": "pattern_113",
        "pattern_name": "Reframing Multimodal Learning Narratives",
        "score": 0.7786646338790686,
        "brief": {
          "motivation": "Current Audio-Text Large Models (ATLMs) suffer from 'text-dominance bias,' where the model over-relies on textual input and fails to capture the correct conditional dependency structures between modalities. This leads to poor performance in real-world scenarios where text may be misleading, adversarial, or irrelevant. Following the pattern of reframing multimodal learning to address inter-modality discrepancies, this research aims to shift from static fusion to a dynamic reliability-based approach that can handle arbitrary modality conditions.",
          "problem_definition": "Task: Robust Multimodal Emotion Recognition. Input: Audio and Text streams. Output: Emotion classification. Settings: 1) Faithful (consistent audio-text), 2) Adversarial (text misleads emotion), 3) Irrelevant (text unrelated to emotion). The core challenge is the model's inability to discern the true emotional signal when cross-modal cues conflict, necessitating a framework that can analytically characterize and mitigate these dependency conflicts.",
          "constraints": [
            "robustness against adversarial and irrelevant text",
            "efficiency (lightweight modules/gating)",
            "adaptability to existing Audio-Text Large Models"
          ],
          "technical_plan": "Propose a dynamic routing and fusion framework based on modality reliability to resolve cross-modal conflicts. 1) Construct a 'text interference' representation utilizing energy and temporal evidence to analytically characterize the conditional dependency and conflict between modalities. 2) Train lightweight gating mechanisms or adapters (inspired by dual-path fusion concepts) to learn modality trustworthiness. 3) Inference: Dynamically route the input to either a 'Joint' expert (if modalities show high dependency/consistency) or an 'Audio' expert (if text interference is detected) to mitigate bias and ensure robust feature learning.",
          "expected_contributions": [
            "Identification and analysis of text-dominance bias in Audio-Text Large Models through the lens of conditional dependency structures",
            "A novel reliability-based routing framework utilizing 'text interference' representation (energy & temporal evidence) to handle arbitrary modality conditions",
            "Significant improvement in robustness for emotion recognition under adversarial and irrelevant text settings, reframing the fusion narrative from static combination to dynamic conflict resolution"
          ],
          "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., CMU-MOSEI, IEMOCAP) with constructed adversarial/irrelevant splits to simulate arbitrary modality conditions. Metrics: Accuracy, F1-score. Baselines: Vanilla ATLMs, static fusion models. Ablations: Impact of the interference representation, routing logic, and adapter training. Robustness Settings: Performance comparison specifically on Faithful vs. Adversarial vs. Irrelevant test sets to validate the model's ability to reframe cross-modal dependencies.",
          "keywords_en": [
            "Audio-Text Large Models",
            "Text-Dominance Bias",
            "Modality Reliability",
            "Robust Emotion Recognition",
            "Dynamic Routing",
            "Conditional Dependency",
            "Arbitrary Modality Conditions"
          ],
          "keywords_zh": [
            "音频-文本大模型",
            "文本主导偏见",
            "模态可信度",
            "稳健情绪识别",
            "动态路由",
            "条件依赖",
            "任意模态条件"
          ],
          "assumptions": {
            "explicit": [
              "Research focuses on Audio-Text Large Models",
              "Evaluation covers faithful, adversarial, and irrelevant settings",
              "Method involves lightweight gating/adapter training",
              "Inference involves selecting between Joint or Audio experts based on reliability"
            ],
            "inferred": [
              "The 'text interference' representation requires a method to quantify energy and temporal alignment to characterize dependency",
              "The model assumes audio is the ground truth for emotion when text is adversarial or irrelevant",
              "The framework is designed to be a plug-and-play module or fine-tuning strategy for existing ATLMs to handle arbitrary modality conditions"
            ]
          }
        },
        "query": "Keywords: Audio-Text Large Models, Text-Dominance Bias, Modality Reliability, Robust Emotion Recognition, Dynamic Routing, Conditional Dependency, Arbitrary Modality Conditions Task: Robust Multimodal Emotion Recognition. Input: Audio and Text streams. Output: Emotion classification. Settings: 1) Faithful (consistent audio-text), 2) Adversarial (text misleads emotion), 3) Irrelevant (text unrelated to emotion). The core challenge is the model's inability to discern the true emotional signal when cross-modal cues conflict, necessitating a framework that can analytically characterize and mitigate these dependency conflicts. Constraints: robustness against adversarial and irrelevant text, efficiency (lightweight modules/gating), adaptability to existing Audio-Text Large Models Propose a dynamic routing and fusion framework based on modality reliability to resolve cross-modal conflicts. 1) Construct a 'text interference' representation utilizing energy and temporal evidence to analytically characterize the conditional dependency and conflict between modalities. 2) Train lightweight gating mechanisms or adapters (inspired by dual-path fusion concepts) to learn modality trustworthiness. 3) Inference: Dynamically route the input to either a 'Joint' expert (if modalities show high dependency/consistency) or an 'Audio' expert (if text interference is detected) to mitigate bias and ensure robust feature learning. 关键词: 音频-文本大模型，文本主导偏见，模态可信度，稳健情绪识别，动态路由，条件依赖，任意模态条件"
      },
      {
        "pattern_id": "pattern_74",
        "pattern_name": "Democratizing Large Language Model Accessibility",
        "score": 0.26868233838365096,
        "brief": {
          "motivation": "Current Audio-Text Large Models (ATLMs) suffer from 'text-dominance bias,' where the model over-relies on textual input and ignores acoustic cues, leading to poor performance in real-world scenarios where text may be misleading, adversarial, or irrelevant. Addressing this issue requires not only improving robustness but also doing so through parameter-efficient methods. By leveraging lightweight adapters and gating mechanisms, this research aims to democratize access to robust multimodal intelligence, ensuring high performance without the prohibitive computational costs of full model retraining or massive parameter scaling.",
          "problem_definition": "Task: Robust Multimodal Emotion Recognition. Input: Audio and Text streams. Output: Emotion classification. Settings: 1) Faithful (consistent audio-text), 2) Adversarial (text misleads emotion), 3) Irrelevant (text unrelated to emotion). The core challenge is to mitigate modality bias and handle interference efficiently, maintaining high accuracy under adversarial conditions while adhering to strict constraints on model efficiency and parameter usage.",
          "constraints": [
            "robustness against adversarial and irrelevant text",
            "efficiency via lightweight modules (adapters/gates)",
            "adaptability to existing Audio-Text Large Models (ATLMs)",
            "parameter-efficient tuning to minimize computational overhead"
          ],
          "technical_plan": "Propose a routing and fusion framework based on modality reliability, utilizing parameter-efficient tuning strategies. 1) Construct a 'text interference' representation utilizing energy and temporal evidence to quantify conflict between modalities. 2) Train lightweight gating mechanisms or adapters (inspired by efficient fine-tuning methods) to learn modality trustworthiness without updating the entire backbone. 3) Inference: Dynamically route the input to either a 'Joint' expert (if modalities are consistent) or an 'Audio' expert (if text interference is detected) to mitigate bias while preserving computational efficiency.",
          "expected_contributions": [
            "Identification and analysis of text-dominance bias in Audio-Text Large Models",
            "A novel reliability-based routing framework utilizing 'text interference' representation (energy & temporal evidence)",
            "Significant improvement in robustness for emotion recognition under adversarial and irrelevant text conditions",
            "Demonstration that robust multimodal inference can be achieved via parameter-efficient modules (adapters), aligning with goals of model accessibility and efficiency"
          ],
          "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., CMU-MOSEI, IEMOCAP) with constructed adversarial/irrelevant splits. Metrics: Accuracy, F1-score. Baselines: Vanilla ATLMs, static fusion models. Ablations: Impact of the interference representation, routing logic, and adapter training. Efficiency Analysis: Measurement of parameter counts and inference latency to validate the lightweight nature of the solution. Robustness Settings: Performance comparison specifically on Faithful vs. Adversarial vs. Irrelevant test sets.",
          "keywords_en": [
            "Audio-Text Large Models",
            "Text-Dominance Bias",
            "Modality Reliability",
            "Robust Emotion Recognition",
            "Dynamic Routing",
            "Parameter-Efficient Tuning",
            "Adapters",
            "Model Efficiency"
          ],
          "keywords_zh": [
            "音频-文本大模型",
            "文本主导偏见",
            "模态可信度",
            "稳健情绪识别",
            "动态路由",
            "参数高效微调",
            "适配器",
            "模型效率"
          ],
          "assumptions": {
            "explicit": [
              "Research focuses on Audio-Text Large Models",
              "Evaluation covers faithful, adversarial, and irrelevant settings",
              "Method involves lightweight gating/adapter training for parameter efficiency",
              "Inference involves selecting between Joint or Audio experts based on reliability"
            ],
            "inferred": [
              "The 'text interference' representation requires a method to quantify energy and temporal alignment",
              "The model assumes audio is the ground truth for emotion when text is adversarial",
              "The framework is designed to be a plug-and-play module or fine-tuning strategy for existing ATLMs to ensure accessibility",
              "Lightweight modules will significantly reduce computational costs compared to full fine-tuning"
            ]
          }
        },
        "query": "Keywords: Audio-Text Large Models, Text-Dominance Bias, Modality Reliability, Robust Emotion Recognition, Dynamic Routing, Parameter-Efficient Tuning, Adapters, Model Efficiency Task: Robust Multimodal Emotion Recognition. Input: Audio and Text streams. Output: Emotion classification. Settings: 1) Faithful (consistent audio-text), 2) Adversarial (text misleads emotion), 3) Irrelevant (text unrelated to emotion). The core challenge is to mitigate modality bias and handle interference efficiently, maintaining high accuracy under adversarial conditions while adhering to strict constraints on model efficiency and parameter usage. Constraints: robustness against adversarial and irrelevant text, efficiency via lightweight modules (adapters/gates), adaptability to existing Audio-Text Large Models (ATLMs), parameter-efficient tuning to minimize computational overhead Propose a routing and fusion framework based on modality reliability, utilizing parameter-efficient tuning strategies. 1) Construct a 'text interference' representation utilizing energy and temporal evidence to quantify conflict between modalities. 2) Train lightweight gating mechanisms or adapters (inspired by efficient fine-tuning methods) to learn modality trustworthiness without updating the entire backbone. 3) Inference: Dynamically route the input to either a 'Joint' expert (if modalities are consistent) or an 'Audio' expert (if text interference is detected) to mitigate bias while preserving computational efficiency. 关键词: 音频-文本大模型，文本主导偏见，模态可信度，稳健情绪识别，动态路由，参数高效微调，适配器，模型效率"
      }
    ],
    "judge": {
      "best_index": 0,
      "rationale": "Candidate 1 (pattern_112) is the best fit because it faithfully captures the core intent of the raw idea: addressing 'text-dominance bias' in Audio-Text Large Models through a dynamic routing mechanism. It accurately translates the raw idea's components into a structured proposal, specifically the construction of a 'text interference' representation using energy and temporal evidence, and the use of lightweight gating to route inputs to 'Joint' or 'Audio' experts. The candidate provides a complete and actionable plan with clear definitions of the problem (faithful/adversarial/irrelevant settings), a robust technical plan, and a concrete evaluation strategy. While Candidate 2 is similar, Candidate 1 frames the solution as a 'reasoning step' which aligns better with the routing logic. Candidate 3 shifts the focus too heavily towards 'democratization' and parameter efficiency, which are secondary to the primary goal of robust emotion recognition stated in the raw idea."
    },
    "recall_scores": {
      "0": 0.0,
      "1": 0.0,
      "2": 0.0
    },
    "chosen_index": 0,
    "query_best": "Keywords: Audio-Text Large Models, Text-Dominance Bias, Modality Reliability, Robust Emotion Recognition, Dynamic Routing, Adversarial Settings, Modular Composition, Intermediate Representation Task: Robust Multimodal Emotion Recognition. Input: Audio and Text streams. Output: Emotion classification. Settings: 1) Faithful (consistent audio-text), 2) Adversarial (text misleads emotion), 3) Irrelevant (text unrelated to emotion). The core challenge is maintaining high performance and generalization when multimodal inputs are misaligned, a common failure mode in generic multimodal reasoning. Constraints: Robustness to adversarial and irrelevant text, Efficiency (lightweight gating/adapter modules), Adaptability to existing Audio-Text Large Models, Modular integration for dynamic expert selection Propose a routing and fusion framework based on modality reliability, inspired by modular composition strategies. 1) Construct a 'text interference' representation utilizing energy and temporal evidence to quantify conflict, serving as an intermediate representation for decision-making. 2) Train lightweight gating mechanisms or adapters to learn modality trustworthiness without full retraining. 3) Inference: Dynamically route the input to either a 'Joint' expert (if modalities are consistent) or an 'Audio' expert (if text interference is detected), effectively composing the most reliable expert for the specific reasoning context. 关键词: 音频-文本大模型，文本主导偏见，模态可信度，稳健情绪识别，动态路由，对抗设定，模块化组合，中间表示",
    "brief_best": {
      "motivation": "Current Audio-Text Large Models (ATLMs) suffer from 'text-dominance bias,' where the model over-relies on textual input and ignores acoustic cues, limiting their generalization capacity in real-world scenarios. Drawing on the pattern of reframing multimodal reasoning challenges, we argue that static fusion approaches are insufficient for handling conflicting inputs. Instead, we propose a dynamic, modular approach that treats modality selection as a reasoning step, enhancing robustness when text is misleading, adversarial, or irrelevant to the actual emotional state.",
      "problem_definition": "Task: Robust Multimodal Emotion Recognition. Input: Audio and Text streams. Output: Emotion classification. Settings: 1) Faithful (consistent audio-text), 2) Adversarial (text misleads emotion), 3) Irrelevant (text unrelated to emotion). The core challenge is maintaining high performance and generalization when multimodal inputs are misaligned, a common failure mode in generic multimodal reasoning.",
      "constraints": [
        "Robustness to adversarial and irrelevant text",
        "Efficiency (lightweight gating/adapter modules)",
        "Adaptability to existing Audio-Text Large Models",
        "Modular integration for dynamic expert selection"
      ],
      "technical_plan": "Propose a routing and fusion framework based on modality reliability, inspired by modular composition strategies. 1) Construct a 'text interference' representation utilizing energy and temporal evidence to quantify conflict, serving as an intermediate representation for decision-making. 2) Train lightweight gating mechanisms or adapters to learn modality trustworthiness without full retraining. 3) Inference: Dynamically route the input to either a 'Joint' expert (if modalities are consistent) or an 'Audio' expert (if text interference is detected), effectively composing the most reliable expert for the specific reasoning context.",
      "expected_contributions": [
        "Identification and analysis of text-dominance bias in Audio-Text Large Models",
        "A novel reliability-based routing framework utilizing 'text interference' as an intermediate representation for expert selection",
        "Significant improvement in robustness and generalization capacity for emotion recognition under adversarial and irrelevant text conditions",
        "Demonstration of a modular approach to multimodal fusion that adapts to input reliability"
      ],
      "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., CMU-MOSEI, IEMOCAP) with constructed adversarial/irrelevant splits to test generalization. Metrics: Accuracy, F1-score. Baselines: Vanilla ATLMs, static fusion models, and other modular composition approaches. Ablations: Impact of the interference representation, routing logic, and adapter training. Robustness Settings: Performance comparison specifically on Faithful vs. Adversarial vs. Irrelevant test sets to measure generalization capacity.",
      "keywords_en": [
        "Audio-Text Large Models",
        "Text-Dominance Bias",
        "Modality Reliability",
        "Robust Emotion Recognition",
        "Dynamic Routing",
        "Adversarial Settings",
        "Modular Composition",
        "Intermediate Representation"
      ],
      "keywords_zh": [
        "音频-文本大模型",
        "文本主导偏见",
        "模态可信度",
        "稳健情绪识别",
        "动态路由",
        "对抗设定",
        "模块化组合",
        "中间表示"
      ],
      "assumptions": {
        "explicit": [
          "Research focuses on Audio-Text Large Models",
          "Evaluation covers faithful, adversarial, and irrelevant settings",
          "Method involves lightweight gating/adapter training",
          "Inference involves selecting between Joint or Audio experts"
        ],
        "inferred": [
          "The 'text interference' representation requires a method to quantify energy and temporal alignment",
          "The model assumes audio is the ground truth for emotion when text is adversarial",
          "The framework is designed to be a plug-and-play module or fine-tuning strategy for existing ATLMs",
          "Modular routing improves generalization compared to static fusion"
        ]
      }
    }
  }
}