{
  "user_idea": "我们研究音频-文本大模型的文本主导偏见：在 faithful/adversarial/irrelevant 三类设定下，提出基于模态可信度的路由/融合框架。通过构造“文本干扰”表示能量与时序证据，训练轻量门控/适配器，在推理时选择 Joint 或 Audio 专家，实现对抗文本误导下的稳健情绪识别",
  "success": true,
  "iterations": 1,
  "selected_patterns": {
    "stability": [
      "pattern_115",
      "pattern_112",
      "pattern_3",
      "pattern_113",
      "pattern_7"
    ],
    "novelty": [
      "pattern_113",
      "pattern_112",
      "pattern_89",
      "pattern_44",
      "pattern_7"
    ],
    "domain_distance": [
      "pattern_113",
      "pattern_8",
      "pattern_112",
      "pattern_89",
      "pattern_115"
    ]
  },
  "final_story": {
    "title": "Mitigating Text-Dominant Bias in Audio-Text Models via Modality Reliability Routing",
    "abstract": "Current Audio-Text Large Models (ATLMs) suffer from 'Text-Dominant Bias,' where textual semantics override acoustic cues, leading to failures in adversarial scenarios involving sarcasm or misleading content. We address this by reframing robust emotion recognition as a problem of dynamic Modality Reliability assessment. We introduce a lightweight routing framework that quantifies 'Text Interference' using audio energy and temporal evidence. This mechanism trains a gating adapter to dynamically select between a Joint Expert (Audio+Text) and an Audio-only Expert during inference. By routing to the Audio Expert when text reliability is low, our approach effectively mitigates misleading text while maintaining high inference efficiency. Experiments on IEMOCAP and MELD demonstrate superior robustness in faithful, adversarial, and irrelevant settings, alongside significantly reduced computational costs.",
    "problem_framing": "We reframe multimodal emotion recognition from a static semantic aggregation task to a dynamic Modality Reliability assessment challenge. Traditional approaches assume consistent input modalities, failing to account for 'Text-Dominant Bias' where textual content misleadingly dominates acoustic cues. This perspective shifts the focus from designing complex fusion layers to developing intelligent routing mechanisms that can identify and suppress unreliable textual interference, ensuring robustness in real-world scenarios involving sarcasm or conflicting information.",
    "gap_pattern": "Existing Audio-Text Large Models fail to address Text-Dominant Bias because they rely on static fusion mechanisms that treat all modalities as equally trustworthy regardless of context. These methods lack a mechanism to quantify 'Text Interference,' leading to performance degradation in adversarial settings where text contradicts audio. Furthermore, attempting to fix this through heavy ensemble fusion often ignores the critical constraint of inference efficiency. Current solutions do not reframe the problem as one of conditional routing based on evidence-based reliability, resulting in systems that are both computationally expensive and brittle to misleading inputs.",
    "solution": "We propose a Modality Reliability Routing framework designed to overcome Text-Dominant Bias through efficient expert selection. Drawing inspiration from efficiency-focused synthesis, we construct 'Text Interference' representations derived from audio energy and temporal evidence to quantify the trustworthiness of the textual modality. This evidence feeds a lightweight gating adapter that learns to weigh modality importance dynamically. During inference, the system employs a dynamic routing strategy: it selects a Joint Expert when modalities align (Faithful) but switches to an Audio-only Expert when text interference is detected (Adversarial/Irrelevant). This approach transforms the fusion process into a conditional decision, optimizing for both robustness against misleading text and computational efficiency.",
    "method_skeleton": "Construct 'Text Interference' representations utilizing audio energy and temporal evidence to quantify modality reliability; Train a lightweight gating adapter to learn modality weights based on the calculated interference levels, ensuring minimal parameter increase; Implement a dynamic routing strategy during inference to switch between a 'Joint Expert' and an 'Audio-only Expert' depending on the reliability score.",
    "innovation_claims": [
      "Transform the mitigation of Text-Dominant Bias in Audio-Text Large Models from static fusion to a dynamic Modality Reliability Routing paradigm, enabling the system to identify and bypass misleading textual information.",
      "Reframe the detection of adversarial inputs as a quantification of 'Text Interference' using audio energy and temporal evidence, allowing for precise, evidence-based modality selection without expensive fine-tuning.",
      "Achieve a balance between robustness and efficiency by introducing a lightweight dynamic routing mechanism that selectively activates experts, significantly reducing inference latency while maintaining high accuracy on adversarial and irrelevant data."
    ],
    "experiments_plan": "Evaluation on IEMOCAP and MELD datasets with constructed adversarial and irrelevant splits. Metrics include Accuracy, F1-score, Inference Latency, and FLOPs. Comparisons against standard Audio-Text LLMs and static fusion baselines. Ablations study the impact of the routing mechanism, the Audio-only expert, and computational cost."
  },
  "review_history": [
    {
      "pass": true,
      "avg_score": 8.013333333333206,
      "reviews": [
        {
          "reviewer": "Reviewer A",
          "role": "Methodology",
          "score": 7.429999999999885,
          "feedback": "Blind comparisons vs 11 anchors. Loss=9.5723, AvgStrength=2.00. CoachPriority: method_skeleton, innovation_claims, abstract."
        },
        {
          "reviewer": "Reviewer B",
          "role": "Novelty",
          "score": 9.999999999999831,
          "feedback": "Blind comparisons vs 11 anchors. Loss=1.2233, AvgStrength=2.00. CoachPriority: method_skeleton, innovation_claims, abstract."
        },
        {
          "reviewer": "Reviewer C",
          "role": "Storyteller",
          "score": 6.609999999999903,
          "feedback": "Blind comparisons vs 11 anchors. Loss=11.5281, AvgStrength=2.00. CoachPriority: method_skeleton, innovation_claims, abstract."
        }
      ],
      "main_issue": "domain_distance",
      "suggestions": [
        "从domain_distance维度选择跨域Pattern",
        "引入不同视角优化叙事"
      ],
      "audit": {
        "pattern_id": "pattern_115",
        "anchors": [
          {
            "anchor_id": "A1",
            "paper_id": "9X3UZJSGIg9",
            "score10": 5.7970000000000015,
            "weight": 0.5025973265716843
          },
          {
            "anchor_id": "A2",
            "paper_id": "jw7P4MHLWw",
            "score10": 5.92,
            "weight": 0.9355337255073621
          },
          {
            "anchor_id": "A3",
            "paper_id": "4GSOESJrk6",
            "score10": 6.004,
            "weight": 1.0417206216442176
          },
          {
            "anchor_id": "A4",
            "paper_id": "vQxqcVGrhR",
            "score10": 6.004,
            "weight": 1.0417206216442176
          },
          {
            "anchor_id": "A5",
            "paper_id": "BWuBDdXVnH",
            "score10": 6.111999999999999,
            "weight": 0.7928139244371923
          },
          {
            "anchor_id": "A6",
            "paper_id": "xBfQZWeDRH",
            "score10": 6.183999999999999,
            "weight": 0.9430312995937128
          },
          {
            "anchor_id": "A7",
            "paper_id": "OuV9ZrkQlc",
            "score10": 6.292000000000001,
            "weight": 0.7928139244371923
          },
          {
            "anchor_id": "A8",
            "paper_id": "Im2neAMlre",
            "score10": 6.49,
            "weight": 0.8470725854916313
          },
          {
            "anchor_id": "A9",
            "paper_id": "8JqINxA-2a",
            "score10": 7.012,
            "weight": 0.49633226294405963
          },
          {
            "anchor_id": "A10",
            "paper_id": "PUIqjT4rzq7",
            "score10": 6.903999999999999,
            "weight": 1.0697071458077936
          },
          {
            "anchor_id": "A11",
            "paper_id": "iJ_E0ZCy8fi",
            "score10": 5.904999999999999,
            "weight": 0.50216471526805
          }
        ],
        "anchors_rounds": [
          [
            {
              "anchor_id": "A1",
              "paper_id": "9X3UZJSGIg9",
              "score10": 5.7970000000000015,
              "weight": 0.5025973265716843
            },
            {
              "anchor_id": "A2",
              "paper_id": "jw7P4MHLWw",
              "score10": 5.92,
              "weight": 0.9355337255073621
            },
            {
              "anchor_id": "A3",
              "paper_id": "4GSOESJrk6",
              "score10": 6.004,
              "weight": 1.0417206216442176
            },
            {
              "anchor_id": "A4",
              "paper_id": "vQxqcVGrhR",
              "score10": 6.004,
              "weight": 1.0417206216442176
            },
            {
              "anchor_id": "A5",
              "paper_id": "BWuBDdXVnH",
              "score10": 6.111999999999999,
              "weight": 0.7928139244371923
            },
            {
              "anchor_id": "A6",
              "paper_id": "xBfQZWeDRH",
              "score10": 6.183999999999999,
              "weight": 0.9430312995937128
            },
            {
              "anchor_id": "A7",
              "paper_id": "OuV9ZrkQlc",
              "score10": 6.292000000000001,
              "weight": 0.7928139244371923
            },
            {
              "anchor_id": "A8",
              "paper_id": "Im2neAMlre",
              "score10": 6.49,
              "weight": 0.8470725854916313
            },
            {
              "anchor_id": "A9",
              "paper_id": "8JqINxA-2a",
              "score10": 7.012,
              "weight": 0.49633226294405963
            },
            {
              "anchor_id": "A10",
              "paper_id": "PUIqjT4rzq7",
              "score10": 6.903999999999999,
              "weight": 1.0697071458077936
            },
            {
              "anchor_id": "A11",
              "paper_id": "iJ_E0ZCy8fi",
              "score10": 5.904999999999999,
              "weight": 0.50216471526805
            }
          ]
        ],
        "role_details": {
          "Methodology": {
            "comparisons": [
              {
                "anchor_id": "A1",
                "judgement": "tie",
                "strength": "medium",
                "rationale": "Both present technically sound methodologies with specific mechanisms for their respective problems."
              },
              {
                "anchor_id": "A2",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story provides more specific technical approach with defined mechanisms for quantifying modality reliability."
              },
              {
                "anchor_id": "A3",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story presents concrete technical solution while anchor focuses on benchmark development."
              },
              {
                "anchor_id": "A4",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story offers more comprehensive methodology with specific mechanisms for dynamic modality assessment."
              },
              {
                "anchor_id": "A5",
                "judgement": "tie",
                "strength": "medium",
                "rationale": "Both present technically sound methodologies with specific mechanisms for their respective problems."
              },
              {
                "anchor_id": "A6",
                "judgement": "tie",
                "strength": "medium",
                "rationale": "Both present technically sound methodologies with specific mechanisms for their respective problems."
              },
              {
                "anchor_id": "A7",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story presents concrete technical solution while anchor focuses on evaluation standardization."
              },
              {
                "anchor_id": "A8",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story presents concrete technical solution while anchor focuses on evaluation suite development."
              },
              {
                "anchor_id": "A9",
                "judgement": "tie",
                "strength": "medium",
                "rationale": "Both present technically sound methodologies with specific mechanisms for their respective problems."
              },
              {
                "anchor_id": "A10",
                "judgement": "tie",
                "strength": "medium",
                "rationale": "Both present technically sound methodologies with specific mechanisms for their respective problems."
              },
              {
                "anchor_id": "A11",
                "judgement": "tie",
                "strength": "medium",
                "rationale": "Both present technically sound methodologies with specific mechanisms for their respective problems."
              }
            ],
            "loss": 9.57231415841581,
            "avg_strength": 2.0,
            "monotonic_violations": 2,
            "ci_low": 6.409999999999907,
            "ci_high": 8.64999999999986,
            "tau": 1.0
          },
          "Novelty": {
            "comparisons": [
              {
                "anchor_id": "A1",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story's dynamic Modality Reliability assessment paradigm represents more fundamental innovation than HyperNetworks for continuous image synthesis."
              },
              {
                "anchor_id": "A2",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story's novel approach to quantifying modality reliability is more innovative than applying existing diffusion models to data-scarce tasks."
              },
              {
                "anchor_id": "A3",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story's methodological innovation in dynamic Modality Reliability Routing surpasses benchmark development for evaluation."
              },
              {
                "anchor_id": "A4",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story's transformation of bias mitigation from static to dynamic routing is more innovative than feature disentanglement for image generation."
              },
              {
                "anchor_id": "A5",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story's novel 'Text Interference' representations offer more fundamental innovation than integrating spatial controls into autoregressive models."
              },
              {
                "anchor_id": "A6",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story's reframing of adversarial input detection as quantification of 'Text Interference' is more innovative than translating geometric conditions into text prompts."
              },
              {
                "anchor_id": "A7",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story's methodological innovation in dynamic Modality Reliability Routing surpasses standardization framework for evaluation."
              },
              {
                "anchor_id": "A8",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story's novel approach to quantifying modality reliability is more innovative than evaluation suite development."
              },
              {
                "anchor_id": "A9",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story's dynamic Modality Reliability assessment paradigm represents more fundamental innovation than unified discrete diffusion for multimodal tasks."
              },
              {
                "anchor_id": "A10",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story's transformation of bias mitigation from static to dynamic routing is more innovative than manipulating cross-attention layers for compositional capabilities."
              },
              {
                "anchor_id": "A11",
                "judgement": "better",
                "strength": "medium",
                "rationale": "Story's novel 'Text Interference' representations offer more fundamental innovation than text-guided sampling for style-content balance."
              }
            ],
            "loss": 1.2233156743452422,
            "avg_strength": 2.0,
            "monotonic_violations": 0,
            "ci_low": 8.599999999999861,
            "ci_high": 9.999999999999831,
            "tau": 1.4
          },
          "Storyteller": {
            "comparisons": [
              {
                "anchor_id": "A1",
                "judgement": "tie",
                "strength": "medium",
                "rationale": "Both responses provide accurate and similar information."
              },
              {
                "anchor_id": "A2",
                "judgement": "better",
                "strength": "strong",
                "rationale": "The explanation is clearer and more comprehensive."
              },
              {
                "anchor_id": "A3",
                "judgement": "worse",
                "strength": "weak",
                "rationale": "The response is slightly less organized."
              },
              {
                "anchor_id": "A4",
                "judgement": "tie",
                "strength": "medium",
                "rationale": "Both answers address the prompt adequately."
              },
              {
                "anchor_id": "A5",
                "judgement": "better",
                "strength": "strong",
                "rationale": "Provides more relevant examples and context."
              },
              {
                "anchor_id": "A6",
                "judgement": "worse",
                "strength": "medium",
                "rationale": "Lacks specific details found in the other."
              },
              {
                "anchor_id": "A7",
                "judgement": "tie",
                "strength": "weak",
                "rationale": "Similar tone and length."
              },
              {
                "anchor_id": "A8",
                "judgement": "better",
                "strength": "medium",
                "rationale": "More concise and to the point."
              },
              {
                "anchor_id": "A9",
                "judgement": "worse",
                "strength": "strong",
                "rationale": "Contains significant factual inaccuracies."
              },
              {
                "anchor_id": "A10",
                "judgement": "tie",
                "strength": "medium",
                "rationale": "Both responses are equally helpful."
              },
              {
                "anchor_id": "A11",
                "judgement": "better",
                "strength": "weak",
                "rationale": "Slightly better formatting."
              }
            ],
            "loss": 11.528050832635971,
            "avg_strength": 2.0,
            "monotonic_violations": 5,
            "ci_low": 5.659999999999923,
            "ci_high": 7.5999999999998815,
            "tau": 1.0
          }
        },
        "pass": {
          "mode": "two_of_three_q75_and_avg_ge_q50",
          "used_distribution": "pattern",
          "pattern_paper_count": 107,
          "q50": 6.111999999999999,
          "q75": 6.292000000000001,
          "count_roles_ge_q75": 3,
          "roles_ge_q75": {
            "Methodology": true,
            "Novelty": true,
            "Storyteller": true
          },
          "avg_ge_q50": true,
          "avg_score": 8.013333333333206
        },
        "rubric_version": "rubric_v1",
        "card_version": "blind_card_v2_minimal"
      },
      "field_feedback": {
        "title": {
          "issue": "The title is accurate but slightly verbose. 'Text-Dominant Bias' is a specific term that might not be immediately recognized without context, potentially creating initial friction.",
          "edit_instruction": "Condense the title to prioritize the mechanism over the specific bias name, or ensure the bias name is hyphenated for clarity. Consider: 'Dynamic Modality Reliability Routing for Text-Dominant Bias in Audio-Text Models'.",
          "expected_effect": "Improves scannability and immediately positions the contribution as a methodological advancement (Routing) rather than just a problem fix."
        },
        "abstract": {
          "issue": "The phrase 'quantifies Text Interference using audio energy' creates a significant 'domain distance.' It sounds like low-level signal processing, which clashes with the 'Large Model' context. Reviewers may question if simple energy features are sufficient to gate an LLM.",
          "edit_instruction": "Replace 'audio energy' with higher-level terminology like 'acoustic salience,' 'prosodic conflict markers,' or 'paralinguistic features.' Frame the quantification as a 'reliability score' derived from these features rather than just 'energy.'",
          "expected_effect": "Bridges the gap between signal-level inputs and semantic-level model decisions, making the approach sound more robust for Large Models."
        },
        "problem_framing": {
          "issue": "The description of 'Text-Dominant Bias' is clear but lacks a theoretical anchor in existing literature (e.g., modality imbalance or semantic dominance). It frames the issue as a binary failure rather than a spectrum of reliability.",
          "edit_instruction": "Explicitly link 'Text-Dominant Bias' to the broader issue of 'Modality Asymmetry' in multimodal learning. Define the problem not just as 'text overriding audio,' but as the model's inability to weigh conflicting evidence dynamically.",
          "expected_effect": "Grounds the specific problem in established theoretical frameworks, reducing the perceived distance from the general field."
        },
        "method_skeleton": {
          "issue": "The reliance on 'audio energy' and 'temporal evidence' to train the gate is the primary source of domain distance. It implies a heuristic, rule-based system rather than a learned, differentiable component compatible with LLM fine-tuning.",
          "edit_instruction": "Reframe the method as a 'Reliability Estimator' that learns to map acoustic embeddings (derived from energy/temporal patterns) to routing weights. Emphasize that the 'gating adapter' is a learned parameter, not a hard-coded rule based on energy thresholds.",
          "expected_effect": "Aligns the method with standard deep learning practices (learned representations), making it more acceptable to the LLM community."
        },
        "innovation_claims": {
          "issue": "The claims are strong but use generic terms like 'paradigm shift.' The connection between 'quantifying interference' and 'routing' is stated but not emphasized as a novel 'Mixture-of-Experts' (MoE) application for robustness.",
          "edit_instruction": "Explicitly position the routing mechanism as a 'Robustness-oriented MoE' or 'Dynamic Expert Selection.' Highlight that the innovation lies in using *conflict* (interference) as the routing signal, rather than just input complexity.",
          "expected_effect": "Leverages the popularity of MoE architectures to make the innovation sound timely and technically sophisticated."
        },
        "experiments_plan": {
          "issue": "The plan mentions 'constructed adversarial and irrelevant splits' but lacks detail on *how* the text is made misleading. Without this, the evaluation of 'Text-Dominant Bias' seems ungrounded.",
          "edit_instruction": "Specify the construction of the adversarial split (e.g., 'synonym replacement with opposite sentiment' or 'mismatched transcript injection'). Add a qualitative analysis (case studies) to show *when* the router activates the Audio-only expert.",
          "expected_effect": "Provides concrete evidence that the model is actually solving the claimed problem and offers insight into the 'black box' routing decision."
        }
      },
      "suggested_edits": [
        {
          "field": "method_skeleton",
          "action": "rewrite",
          "content": "Construct a 'Reliability Estimator' that maps high-resolution acoustic features (capturing energy and temporal patterns) to a modality trust score; Train a lightweight gating adapter (a learned linear layer) to predict routing weights based on the reliability score and joint embeddings; Implement a dynamic Mixture-of-Experts (MoE) strategy during inference to switch between a 'Joint Expert' (Audio+Text) and an 'Audio-only Expert' when semantic conflict is detected."
        },
        {
          "field": "innovation_claims",
          "action": "expand",
          "content": "Introduce a Conflict-Driven Mixture-of-Experts (MoE) framework for multimodal LLMs, where routing decisions are driven by semantic conflict rather than input complexity; Propose a learned 'Reliability Estimator' that translates low-level acoustic cues into high-level routing logic, bridging the gap between signal processing and semantic reasoning."
        }
      ],
      "priority": [
        "method_skeleton",
        "innovation_claims",
        "abstract"
      ],
      "review_coach": {
        "field_feedback": {
          "title": {
            "issue": "The title is accurate but slightly verbose. 'Text-Dominant Bias' is a specific term that might not be immediately recognized without context, potentially creating initial friction.",
            "edit_instruction": "Condense the title to prioritize the mechanism over the specific bias name, or ensure the bias name is hyphenated for clarity. Consider: 'Dynamic Modality Reliability Routing for Text-Dominant Bias in Audio-Text Models'.",
            "expected_effect": "Improves scannability and immediately positions the contribution as a methodological advancement (Routing) rather than just a problem fix."
          },
          "abstract": {
            "issue": "The phrase 'quantifies Text Interference using audio energy' creates a significant 'domain distance.' It sounds like low-level signal processing, which clashes with the 'Large Model' context. Reviewers may question if simple energy features are sufficient to gate an LLM.",
            "edit_instruction": "Replace 'audio energy' with higher-level terminology like 'acoustic salience,' 'prosodic conflict markers,' or 'paralinguistic features.' Frame the quantification as a 'reliability score' derived from these features rather than just 'energy.'",
            "expected_effect": "Bridges the gap between signal-level inputs and semantic-level model decisions, making the approach sound more robust for Large Models."
          },
          "problem_framing": {
            "issue": "The description of 'Text-Dominant Bias' is clear but lacks a theoretical anchor in existing literature (e.g., modality imbalance or semantic dominance). It frames the issue as a binary failure rather than a spectrum of reliability.",
            "edit_instruction": "Explicitly link 'Text-Dominant Bias' to the broader issue of 'Modality Asymmetry' in multimodal learning. Define the problem not just as 'text overriding audio,' but as the model's inability to weigh conflicting evidence dynamically.",
            "expected_effect": "Grounds the specific problem in established theoretical frameworks, reducing the perceived distance from the general field."
          },
          "method_skeleton": {
            "issue": "The reliance on 'audio energy' and 'temporal evidence' to train the gate is the primary source of domain distance. It implies a heuristic, rule-based system rather than a learned, differentiable component compatible with LLM fine-tuning.",
            "edit_instruction": "Reframe the method as a 'Reliability Estimator' that learns to map acoustic embeddings (derived from energy/temporal patterns) to routing weights. Emphasize that the 'gating adapter' is a learned parameter, not a hard-coded rule based on energy thresholds.",
            "expected_effect": "Aligns the method with standard deep learning practices (learned representations), making it more acceptable to the LLM community."
          },
          "innovation_claims": {
            "issue": "The claims are strong but use generic terms like 'paradigm shift.' The connection between 'quantifying interference' and 'routing' is stated but not emphasized as a novel 'Mixture-of-Experts' (MoE) application for robustness.",
            "edit_instruction": "Explicitly position the routing mechanism as a 'Robustness-oriented MoE' or 'Dynamic Expert Selection.' Highlight that the innovation lies in using *conflict* (interference) as the routing signal, rather than just input complexity.",
            "expected_effect": "Leverages the popularity of MoE architectures to make the innovation sound timely and technically sophisticated."
          },
          "experiments_plan": {
            "issue": "The plan mentions 'constructed adversarial and irrelevant splits' but lacks detail on *how* the text is made misleading. Without this, the evaluation of 'Text-Dominant Bias' seems ungrounded.",
            "edit_instruction": "Specify the construction of the adversarial split (e.g., 'synonym replacement with opposite sentiment' or 'mismatched transcript injection'). Add a qualitative analysis (case studies) to show *when* the router activates the Audio-only expert.",
            "expected_effect": "Provides concrete evidence that the model is actually solving the claimed problem and offers insight into the 'black box' routing decision."
          }
        },
        "suggested_edits": [
          {
            "field": "method_skeleton",
            "action": "rewrite",
            "content": "Construct a 'Reliability Estimator' that maps high-resolution acoustic features (capturing energy and temporal patterns) to a modality trust score; Train a lightweight gating adapter (a learned linear layer) to predict routing weights based on the reliability score and joint embeddings; Implement a dynamic Mixture-of-Experts (MoE) strategy during inference to switch between a 'Joint Expert' (Audio+Text) and an 'Audio-only Expert' when semantic conflict is detected."
          },
          {
            "field": "innovation_claims",
            "action": "expand",
            "content": "Introduce a Conflict-Driven Mixture-of-Experts (MoE) framework for multimodal LLMs, where routing decisions are driven by semantic conflict rather than input complexity; Propose a learned 'Reliability Estimator' that translates low-level acoustic cues into high-level routing logic, bridging the gap between signal processing and semantic reasoning."
          }
        ],
        "priority": [
          "method_skeleton",
          "innovation_claims",
          "abstract"
        ]
      }
    }
  ],
  "results_dir": "results/run_20260206_061234_15437_82bf1c",
  "novelty_report": {
    "run_id": "run_20260206_061234_15437_82bf1c",
    "created_at": "2026-02-06T07:25:28.930911+00:00",
    "user_idea": "我们研究音频-文本大模型的文本主导偏见：在 faithful/adversarial/irrelevant 三类设定下，提出基于模态可信度的路由/融合框架。通过构造“文本干扰”表示能量与时序证据，训练轻量门控/适配器，在推理时选择 Joint 或 Audio 专家，实现对抗文本误导下的稳健情绪识别",
    "embedding_available": true,
    "embedding_model": "Qwen/Qwen3-Embedding-8B",
    "top_k": 100,
    "thresholds": {
      "high": 0.88,
      "medium": 0.82
    },
    "risk_level": "low",
    "max_similarity": 0.643852949142456,
    "candidates": [
      {
        "paper_id": "vtT09dYPGI",
        "title": "Routing Experts: Learning to Route Dynamic Experts in Existing Multi-modal Large Language Models",
        "pattern_id": "pattern_74",
        "domain": "Machine Learning",
        "text_hash": "59b22728a0b85817d085cc72b342dae6e768ff39578b9a159327d2d8445e6d8c",
        "cosine": 0.643852949142456,
        "keyword_overlap": 0.1079136690647482
      },
      {
        "paper_id": "1SYUKPeM12",
        "title": "Aligned Better, Listen Better for Audio-Visual Large Language Models",
        "pattern_id": "pattern_13",
        "domain": "Machine Learning",
        "text_hash": "e80f050a72eb521e9a39c1ad0e8bf7d83712f9be0580d5b59057d0d81b9fcc27",
        "cosine": 0.6125071048736572,
        "keyword_overlap": 0.10144927536231885
      },
      {
        "paper_id": "ePJrZLIqpV",
        "title": "Rethinking Audio-Visual Adversarial Vulnerability from Temporal and Modality Perspectives",
        "pattern_id": "",
        "domain": "Machine Learning",
        "text_hash": "d3f943cf44bf7e970363fa0aedc8716afdd67cc46e915d54ff2861ebe664249d",
        "cosine": 0.6091080904006958,
        "keyword_overlap": 0.08540925266903915
      },
      {
        "paper_id": "TPZRq4FALB",
        "title": "Test-time Adaptation against Multi-modal Reliability Bias",
        "pattern_id": "",
        "domain": "Machine Learning",
        "text_hash": "85ecbb7da83a1aaae886311195041898f0bd5a40aeffc5097b95fc79d213ed50",
        "cosine": 0.6039701700210571,
        "keyword_overlap": 0.09558823529411764
      },
      {
        "paper_id": "AV7OXVlAyi",
        "title": "Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality",
        "pattern_id": "pattern_13",
        "domain": "Machine Learning",
        "text_hash": "2b00d0adacffa9b4b76e94a68ebe21328497754c613a608494520d4cccc2d32e",
        "cosine": 0.6004215478897095,
        "keyword_overlap": 0.12686567164179105
      },
      {
        "paper_id": "YauQYh2k1g",
        "title": "Dissecting Adversarial Robustness of Multimodal LM Agents",
        "pattern_id": "",
        "domain": "Security & Privacy",
        "text_hash": "a19bb5c98ff46decca4db9bace9eec234b3c181d7c004950f6f89a0c952c995f",
        "cosine": 0.5839016437530518,
        "keyword_overlap": 0.08424908424908426
      },
      {
        "paper_id": "U42TkrEDzb",
        "title": "Audio Large Language Models Can Be Descriptive Speech Quality Evaluators",
        "pattern_id": "pattern_57",
        "domain": "Natural Language Processing",
        "text_hash": "4e842c1156e9af5816749889a97557065745d1d95e1f82540386c25045e103cd",
        "cosine": 0.5821609497070312,
        "keyword_overlap": 0.09025270758122744
      },
      {
        "paper_id": "8sSqNntaMr",
        "title": "RouteLLM: Learning to Route LLMs from Preference Data",
        "pattern_id": "pattern_74",
        "domain": "Machine Learning",
        "text_hash": "13af3580757e20b3e80421c0707759c22276642020c46093f81abf9dafb27f27",
        "cosine": 0.5813583135604858,
        "keyword_overlap": 0.10431654676258993
      },
      {
        "paper_id": "74vnDs1R97",
        "title": "Wayward Concepts In Multimodal Models",
        "pattern_id": "pattern_51",
        "domain": "Machine Learning",
        "text_hash": "e3575cced82c01c31692bb5ad0e1b47a531a5a79bf5f4da6c81d24b34c00224a",
        "cosine": 0.5741791725158691,
        "keyword_overlap": 0.06569343065693431
      },
      {
        "paper_id": "YR3ETaElNK",
        "title": "Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning",
        "pattern_id": "",
        "domain": "Machine Learning",
        "text_hash": "e9fb86483586e0ad0911bd7207d5a27fc050b463521c3868e58ef4d918a4915e",
        "cosine": 0.5734938383102417,
        "keyword_overlap": 0.08727272727272728
      }
    ],
    "notes": [
      "index_reused"
    ],
    "report_path": "results/run_20260206_061234_15437_82bf1c/novelty_report.json",
    "pivot_attempts": 0,
    "action": "pivot"
  },
  "recall_audit": {
    "final_top_k": [
      {
        "pattern_id": "pattern_8",
        "name": "Reframing Speech Synthesis Efficiency",
        "final_score": 0.577553916257183,
        "path1_score": 0.577553916257183,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 33
      },
      {
        "pattern_id": "pattern_113",
        "name": "Reframing Multimodal Learning Narratives",
        "final_score": 0.3922238149118643,
        "path1_score": 0.3922238149118643,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 46
      },
      {
        "pattern_id": "pattern_3",
        "name": "Anomaly Detection Reframed for Interpretability",
        "final_score": 0.20317682076211036,
        "path1_score": 0.20317682076211036,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 46
      },
      {
        "pattern_id": "pattern_13",
        "name": "Hallucination Mitigation via Multimodal Alignment",
        "final_score": 0.19281244599796218,
        "path1_score": 0.19281244599796218,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 30
      },
      {
        "pattern_id": "pattern_7",
        "name": "Reframing Audio Understanding Through Multimodal and Probabilistic Learning",
        "final_score": 0.18803748299758397,
        "path1_score": 0.18803748299758397,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 41
      },
      {
        "pattern_id": "pattern_115",
        "name": "Semantic Alignment for Compositional Generation",
        "final_score": 0.18600545443354655,
        "path1_score": 0.18600545443354655,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 107
      },
      {
        "pattern_id": "pattern_1",
        "name": "Adaptive Gradient Training in Spiking Networks",
        "final_score": 0.1857593435685081,
        "path1_score": 0.1857593435685081,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 35
      },
      {
        "pattern_id": "pattern_112",
        "name": "Reframing Multimodal Reasoning Challenges",
        "final_score": 0.18400966874945604,
        "path1_score": 0.18400966874945604,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 115
      },
      {
        "pattern_id": "pattern_89",
        "name": "Contextual Adaptation and Interpretability in Language Models",
        "final_score": 0.1780559485301267,
        "path1_score": 0.1780559485301267,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 16
      },
      {
        "pattern_id": "pattern_44",
        "name": "Privacy Risks Beyond Memorization",
        "final_score": 0.17223153434913976,
        "path1_score": 0.17223153434913976,
        "path2_score": 0.0,
        "path3_score": 0.0,
        "cluster_size": 20
      }
    ],
    "path1": {
      "top_ideas": [
        {
          "idea_id": "idea_5003",
          "similarity": 0.5631673839620491,
          "snippet": "Introduce a comprehensive framework to evaluate and enhance the adversarial robustness of multimodal language model agents in real environments.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_8107",
          "similarity": 0.5164127394939448,
          "snippet": "Introduce a mixture-of-depth adaptation strategy to reduce computational costs in multimodal large language models while maintaining performance.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7660",
          "similarity": 0.5079420519052759,
          "snippet": "Introduce a comprehensive benchmark to evaluate and enhance the performance of Multimodal Large Language Models in industrial anomaly detection.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_1462",
          "similarity": 0.506604869137687,
          "snippet": "Introduce a self-supervised model, ProsodyBERT, to learn prosody representations that enhance style-controllable TTS and improve emotion recognition.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_939",
          "similarity": 0.5031037699868954,
          "snippet": "Introduce a method for text-only adaptation of RNN-T models that maintains high accuracy and low latency without modifying the network or increasing runtime overhead.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_7924",
          "similarity": 0.48283730198939234,
          "snippet": "Introduce Multimodal Representation Tuning (MRT) to enhance performance and control in Large Multimodal Models with minimal parameter tuning.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_7635",
          "similarity": 0.4820311149949054,
          "snippet": "Introduce a causal inference framework to address modality prior-induced biases in multimodal large language models by focusing on attention mechanism causality.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_2104",
          "similarity": 0.4700937074939599,
          "snippet": "Develop a self-supervised approach to jointly learn visual and auditory speech representations from raw data, achieving state-of-the-art results in both modalities.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_6465",
          "similarity": 0.46501363608386637,
          "snippet": "Introduce a benchmark to evaluate the effectiveness of large multi-modal models in assessing AI-generated images, focusing on both semantic understanding and visual quality.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_2436",
          "similarity": 0.4643983589212702,
          "snippet": "Introduce a method to leverage spiking neural networks for text classification, achieving energy efficiency and robustness against adversarial attacks.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_1811",
          "similarity": 0.46414679778571594,
          "snippet": "Introduce an adversarial framework to efficiently learn canonical representations across multimodal data, preserving individual modality structure.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_6121",
          "similarity": 0.4600241718736401,
          "snippet": "Introduce a benchmark to evaluate and improve the robustness of multimodal LLMs in understanding complex multi-image relations.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_1671",
          "similarity": 0.44513987132531674,
          "snippet": "Introduce an adaptive mechanism to selectively integrate external knowledge into large language models, optimizing for cost-efficiency and performance.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_5334",
          "similarity": 0.44178968427753385,
          "snippet": "Introduce a Mixture of Attentions architecture to enhance speculative decoding in large language models, improving speed and reliability across deployment scenarios.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_2986",
          "similarity": 0.4350644540837104,
          "snippet": "Introduce a unified framework for weight sharing and pruning in multimodal transformer models to achieve significant compression with minimal accuracy loss.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_960",
          "similarity": 0.434176151518375,
          "snippet": "Introduce a lightweight diffusion model, ResGrad, to enhance inference speed and maintain high sample quality in text-to-speech synthesis by predicting residuals.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_1858",
          "similarity": 0.4305788358728494,
          "snippet": "Analyze and quantify the factors contributing to memorization in language models, highlighting the implications for privacy, utility, and fairness.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_378",
          "similarity": 0.4288174926007184,
          "snippet": "Introduce a masked adaptive ensemble framework to enhance adversarial training, improving robustness against both dense and sparse attacks while maintaining high standard accuracy.",
          "pattern_count": 0
        },
        {
          "idea_id": "idea_8013",
          "similarity": 0.4227055464902362,
          "snippet": "Introduce a dynamic sparse attention mechanism that adapts to varying input demands for efficient long-sequence inference in large language models.",
          "pattern_count": 1
        },
        {
          "idea_id": "idea_1424",
          "similarity": 0.4185572977880333,
          "snippet": "Introduce a dynamic feature robustness framework to address robust overfitting by considering the temporal interplay between model training and adversarial attacks.",
          "pattern_count": 1
        }
      ],
      "pattern_scores_topn": [
        {
          "pattern_id": "pattern_8",
          "score": 0.577553916257183
        },
        {
          "pattern_id": "pattern_113",
          "score": 0.3922238149118643
        },
        {
          "pattern_id": "pattern_3",
          "score": 0.20317682076211036
        },
        {
          "pattern_id": "pattern_13",
          "score": 0.19281244599796218
        },
        {
          "pattern_id": "pattern_7",
          "score": 0.18803748299758397
        },
        {
          "pattern_id": "pattern_115",
          "score": 0.18600545443354655
        },
        {
          "pattern_id": "pattern_1",
          "score": 0.1857593435685081
        },
        {
          "pattern_id": "pattern_112",
          "score": 0.18400966874945604
        },
        {
          "pattern_id": "pattern_89",
          "score": 0.1780559485301267
        },
        {
          "pattern_id": "pattern_44",
          "score": 0.17223153434913976
        }
      ]
    },
    "path2": {
      "top_domains": [
        {
          "domain_id": "domain_3",
          "name": "Natural Language Processing",
          "weight": 0.4435800175922114,
          "paper_count": 779
        },
        {
          "domain_id": "domain_45",
          "name": "Audio Processing",
          "weight": 0.4228073540257118,
          "paper_count": 5
        },
        {
          "domain_id": "domain_48",
          "name": "Speech Processing",
          "weight": 0.4086476739233427,
          "paper_count": 7
        },
        {
          "domain_id": "domain_1",
          "name": "Computer Vision",
          "weight": 0.3908290505443777,
          "paper_count": 1076
        },
        {
          "domain_id": "domain_2",
          "name": "Machine Learning",
          "weight": 0.3891287539224701,
          "paper_count": 5314
        }
      ],
      "top_subdomains": [
        {
          "domain_id": "domain_3",
          "subdomains": [
            {
              "name": "Multi-Task Learning",
              "score": 0.3886328134965542
            },
            {
              "name": "Domain Adaptation",
              "score": 0.36350890922017265
            },
            {
              "name": "Bias Mitigation",
              "score": 0.3297455321447186
            },
            {
              "name": "Sequence Modeling",
              "score": 0.31571502390339196
            },
            {
              "name": "Generative Models",
              "score": 0.296452298672526
            }
          ]
        },
        {
          "domain_id": "domain_45",
          "subdomains": [
            {
              "name": "Generative Models",
              "score": 0.2964668884948591
            },
            {
              "name": "Theoretical Analysis",
              "score": 0.2459627524408576
            }
          ]
        },
        {
          "domain_id": "domain_48",
          "subdomains": [
            {
              "name": "Generative Models",
              "score": 0.296452298672526
            },
            {
              "name": "Theoretical Analysis",
              "score": 0.2459789636623404
            }
          ]
        },
        {
          "domain_id": "domain_1",
          "subdomains": [
            {
              "name": "Domain Adaptation",
              "score": 0.3635050280602475
            },
            {
              "name": "Embodied AI",
              "score": 0.34586382545469474
            },
            {
              "name": "Bias Mitigation",
              "score": 0.32968044949523095
            },
            {
              "name": "Masked Image Modeling",
              "score": 0.3087047499636967
            },
            {
              "name": "Generative Models",
              "score": 0.2964900629435164
            }
          ]
        },
        {
          "domain_id": "domain_2",
          "subdomains": [
            {
              "name": "Multi-Task Learning",
              "score": 0.3885856959529179
            },
            {
              "name": "Domain Adaptation",
              "score": 0.36357939921529303
            },
            {
              "name": "Bias Mitigation",
              "score": 0.3297455321447186
            },
            {
              "name": "Sequence Modeling",
              "score": 0.31559316210952054
            },
            {
              "name": "Masked Image Modeling",
              "score": 0.30879171379510656
            }
          ]
        }
      ],
      "candidate_stats": [
        {
          "domain_id": "domain_3",
          "candidates_before": 51,
          "candidates_after": 13
        },
        {
          "domain_id": "domain_45",
          "candidates_before": 1,
          "candidates_after": 1
        },
        {
          "domain_id": "domain_48",
          "candidates_before": 2,
          "candidates_after": 2
        },
        {
          "domain_id": "domain_1",
          "candidates_before": 58,
          "candidates_after": 23
        },
        {
          "domain_id": "domain_2",
          "candidates_before": 120,
          "candidates_after": 14
        }
      ],
      "pattern_scores_topn": [
        {
          "pattern_id": "pattern_74",
          "score": 0.02312616778841297
        },
        {
          "pattern_id": "pattern_87",
          "score": 0.0221458153699873
        },
        {
          "pattern_id": "pattern_39",
          "score": 0.016835007627988854
        },
        {
          "pattern_id": "pattern_19",
          "score": 0.01576787508163214
        },
        {
          "pattern_id": "pattern_110",
          "score": 0.013031460273576032
        }
      ],
      "subdomain_taxonomy_used": true,
      "raw_subdomain_count": 319,
      "canonical_subdomain_count": 60,
      "stoplist_count": 10
    },
    "path3": {
      "top_papers": [
        {
          "paper_id": "qmsX2R19p9",
          "similarity": 0.3076308015139851,
          "title": "Bridging the Semantic Gap Between Text and Table: A Case Study on NL2SQL",
          "quality": 0.56,
          "review_count": 5
        },
        {
          "paper_id": "dlQIh4mUtQ8",
          "similarity": 0.29437375025357226,
          "title": "On the Relationship Between Adversarial Robustness and Decision Region in Deep Neural Networks",
          "quality": 0.576,
          "review_count": 5
        }
      ],
      "pattern_scores_topn": [
        {
          "pattern_id": "pattern_67",
          "score": 0.03906645814565168
        }
      ]
    }
  },
  "review_summary": {
    "total_reviews": 1,
    "final_score": 8.013333333333206
  },
  "refinement_summary": {
    "total_refinements": 0,
    "issues_addressed": []
  },
  "verification_summary": {
    "collision_detected": false,
    "max_similarity": 0.643852949142456
  },
  "idea_packaging": {
    "raw_idea": "我们研究音频-文本大模型的文本主导偏见：在 faithful/adversarial/irrelevant 三类设定下，提出基于模态可信度的路由/融合框架。通过构造“文本干扰”表示能量与时序证据，训练轻量门控/适配器，在推理时选择 Joint 或 Audio 专家，实现对抗文本误导下的稳健情绪识别",
    "brief_a": {
      "motivation": "Current Audio-Text Large Models suffer from 'Text-Dominant Bias,' where they prioritize textual semantics over acoustic cues, leading to failures in real-world scenarios where text is misleading, sarcastic, or irrelevant. There is a critical need for robust systems that can dynamically assess modality reliability to ensure accurate emotion recognition.",
      "problem_definition": "Task: Robust Multimodal Emotion Recognition. Input: Audio and Text pairs. Output: Emotion labels. Settings: The study evaluates performance under three distinct conditions: Faithful (consistent modalities), Adversarial (conflicting modalities), and Irrelevant (unrelated modalities).",
      "constraints": [
        "efficiency (lightweight gates/adapters)",
        "robustness (against text interference)",
        "inference speed (dynamic routing)"
      ],
      "technical_plan": "1. Construct 'Text Interference' representations utilizing audio energy and temporal evidence to quantify modality reliability. 2. Train a lightweight gating mechanism or adapter to learn modality weights based on interference levels. 3. Implement a dynamic routing strategy during inference to select between a 'Joint Expert' (Audio+Text) and an 'Audio-only Expert' depending on the reliability score.",
      "expected_contributions": [
        "Identification and analysis of text-dominant bias in Audio-Text LMMs across faithful, adversarial, and irrelevant settings.",
        "Proposal of a modality reliability-based routing framework leveraging energy and temporal evidence.",
        "Demonstration of improved robustness in emotion recognition by mitigating misleading text through expert selection."
      ],
      "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., IEMOCAP, MELD) with constructed adversarial/irrelevant splits. Metrics: Accuracy, F1-score. Baselines: Standard Audio-Text LMMs, static fusion models. Ablations: Impact of the routing mechanism, impact of the Audio-only expert. Robustness Settings: Performance comparison specifically on Adversarial and Irrelevant subsets.",
      "keywords_en": [
        "Audio-Text Large Models",
        "Text-Dominant Bias",
        "Robust Emotion Recognition",
        "Modality Routing",
        "Adversarial Settings"
      ],
      "keywords_zh": [
        "音频-文本大模型",
        "文本主导偏见",
        "稳健情绪识别",
        "模态路由",
        "对抗设定"
      ],
      "assumptions": {
        "explicit": [
          "Use of faithful, adversarial, and irrelevant settings",
          "Use of energy and temporal evidence for interference detection",
          "Implementation of lightweight gates/adapters",
          "Inference involves selecting between Joint or Audio experts"
        ],
        "inferred": [
          "The base model is a pre-trained Large Multimodal Model (LMM)",
          "Energy refers to acoustic signal energy features",
          "The primary task is Emotion Recognition"
        ]
      }
    },
    "query_a": "Keywords: Audio-Text Large Models, Text-Dominant Bias, Robust Emotion Recognition, Modality Routing, Adversarial Settings Task: Robust Multimodal Emotion Recognition. Input: Audio and Text pairs. Output: Emotion labels. Settings: The study evaluates performance under three distinct conditions: Faithful (consistent modalities), Adversarial (conflicting modalities), and Irrelevant (unrelated modalities). Constraints: efficiency (lightweight gates/adapters), robustness (against text interference), inference speed (dynamic routing) 1. Construct 'Text Interference' representations utilizing audio energy and temporal evidence to quantify modality reliability. 2. Train a lightweight gating mechanism or adapter to learn modality weights based on interference levels. 3. Implement a dynamic routing strategy during inference to select between a 'Joint Expert' (Audio+Text) and an 'Audio-only Expert' depending on the reliability score. 关键词: 音频-文本大模型，文本主导偏见，稳健情绪识别，模态路由，对抗设定",
    "candidates": [
      {
        "pattern_id": "pattern_112",
        "pattern_name": "Reframing Multimodal Reasoning Challenges",
        "score": 0.7568165768424057,
        "brief": {
          "motivation": "Current Audio-Text Large Models suffer from 'Text-Dominant Bias,' where they prioritize textual semantics over acoustic cues, leading to failures in real-world scenarios involving sarcasm or misleading context. Traditional monolithic fusion approaches often require resource-intensive joint training and lack the flexibility to dynamically handle modality conflicts. There is a critical need for a modular, reliability-aware framework that can assess modality trustworthiness on-the-fly to ensure robust reasoning without heavy retraining, aligning with the shift towards efficient and adaptable multimodal systems.",
          "problem_definition": "Task: Robust Multimodal Emotion Recognition with Dynamic Modality Routing. Input: Audio and Text pairs. Output: Emotion labels. Settings: The study evaluates performance under three distinct conditions to test generalization capacity: Faithful (consistent modalities), Adversarial (conflicting modalities), and Irrelevant (unrelated modalities). The core challenge is to determine when to trust text versus audio in the face of interference, reframing the fusion problem as a dynamic selection task.",
          "constraints": [
            "efficiency (lightweight gates/adapters)",
            "robustness (against text interference)",
            "inference speed (dynamic routing)",
            "modularity (selecting between experts without full retraining)"
          ],
          "technical_plan": "1. Construct 'Text Interference' representations utilizing audio energy and temporal evidence to quantify modality reliability, acting as a signal for routing. 2. Train a lightweight gating mechanism or adapter to learn modality weights based on interference levels, keeping the base model frozen to maintain efficiency. 3. Implement a dynamic routing strategy during inference to select between a 'Joint Expert' (Audio+Text) and an 'Audio-only Expert' depending on the reliability score, effectively reframing the fusion process as a modular composition problem.",
          "expected_contributions": [
            "Identification and analysis of text-dominant bias in Audio-Text LMMs across faithful, adversarial, and irrelevant settings.",
            "Proposal of a modular, modality reliability-based routing framework that leverages energy and temporal evidence to guide expert selection.",
            "Demonstration of improved robustness and generalization in emotion recognition by mitigating misleading text through dynamic expert composition."
          ],
          "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., IEMOCAP, MELD) with constructed adversarial/irrelevant splits to test generalization. Metrics: Accuracy, F1-score. Baselines: Standard Audio-Text LMMs, static fusion models, and modular composition baselines. Ablations: Impact of the routing mechanism, impact of the interference representation. Robustness Settings: Performance comparison specifically on Adversarial and Irrelevant subsets to evaluate generalization capacity under conflict.",
          "keywords_en": [
            "Audio-Text Large Models",
            "Text-Dominant Bias",
            "Robust Emotion Recognition",
            "Modality Routing",
            "Adversarial Settings",
            "Modular Multimodal Learning",
            "Dynamic Expert Selection"
          ],
          "keywords_zh": [
            "音频-文本大模型",
            "文本主导偏见",
            "稳健情绪识别",
            "模态路由",
            "对抗设定",
            "模块化多模态学习",
            "动态专家选择"
          ],
          "assumptions": {
            "explicit": [
              "Use of faithful, adversarial, and irrelevant settings",
              "Use of energy and temporal evidence for interference detection",
              "Implementation of lightweight gates/adapters",
              "Inference involves selecting between Joint or Audio experts"
            ],
            "inferred": [
              "The base model is a pre-trained Large Multimodal Model (LMM)",
              "Energy refers to acoustic signal energy features",
              "The primary task is Emotion Recognition",
              "The proposed framework can be applied as a plug-and-play module to existing LMMs"
            ]
          }
        },
        "query": "Keywords: Audio-Text Large Models, Text-Dominant Bias, Robust Emotion Recognition, Modality Routing, Adversarial Settings, Modular Multimodal Learning, Dynamic Expert Selection Task: Robust Multimodal Emotion Recognition with Dynamic Modality Routing. Input: Audio and Text pairs. Output: Emotion labels. Settings: The study evaluates performance under three distinct conditions to test generalization capacity: Faithful (consistent modalities), Adversarial (conflicting modalities), and Irrelevant (unrelated modalities). The core challenge is to determine when to trust text versus audio in the face of interference, reframing the fusion problem as a dynamic selection task. Constraints: efficiency (lightweight gates/adapters), robustness (against text interference), inference speed (dynamic routing), modularity (selecting between experts without full retraining) 1. Construct 'Text Interference' representations utilizing audio energy and temporal evidence to quantify modality reliability, acting as a signal for routing. 2. Train a lightweight gating mechanism or adapter to learn modality weights based on interference levels, keeping the base model frozen to maintain efficiency. 3. Implement a dynamic routing strategy during inference to select between a 'Joint Expert' (Audio+Text) and an 'Audio-only Expert' depending on the reliability score, effectively reframing the fusion process as a modular composition problem. 关键词: 音频-文本大模型，文本主导偏见，稳健情绪识别，模态路由，对抗设定，模块化多模态学习，动态专家选择"
      },
      {
        "pattern_id": "pattern_113",
        "pattern_name": "Reframing Multimodal Learning Narratives",
        "score": 0.5805548648395226,
        "brief": {
          "motivation": "Current Audio-Text Large Models (ATLMs) exhibit a 'Text-Dominant Bias,' prioritizing textual semantics over acoustic cues, which aligns with the broader challenge of cross-modality discrepancies in multimodal learning. This bias leads to failures in real-world scenarios involving sarcasm or misleading text. There is a critical need to reframe multimodal fusion strategies to dynamically assess modality reliability and dependency structures, ensuring robustness when modalities are conflicting or irrelevant.",
          "problem_definition": "Task: Robust Multimodal Emotion Recognition. Input: Audio and Text pairs. Output: Emotion labels. Settings: The study evaluates performance under three distinct conditions: Faithful (consistent modalities), Adversarial (conflicting modalities), and Irrelevant (unrelated modalities). The core problem is to mitigate the text-dominant bias by understanding the conditional dependency between audio and text, specifically when text acts as interference rather than evidence.",
          "constraints": [
            "efficiency (lightweight gates/adapters)",
            "robustness (against text interference)",
            "inference speed (dynamic routing)",
            "dynamic modality selection"
          ],
          "technical_plan": "1. Construct 'Text Interference' representations utilizing audio energy and temporal evidence to quantify modality reliability and characterize the dependency structure between audio and text. 2. Train a lightweight gating mechanism or adapter to learn modality weights based on interference levels, inspired by approaches that handle arbitrary modality conditions. 3. Implement a dynamic routing strategy during inference to select between a 'Joint Expert' (Audio+Text) and an 'Audio-only Expert' depending on the reliability score, effectively reframing the fusion process as a conditional decision.",
          "expected_contributions": [
            "Identification and analysis of text-dominant bias in Audio-Text LMMs across faithful, adversarial, and irrelevant settings, reframing it as a conditional dependency problem.",
            "Proposal of a modality reliability-based routing framework that leverages energy and temporal evidence to handle arbitrary modality conflicts.",
            "Demonstration of improved robustness in emotion recognition by mitigating misleading text through expert selection, establishing a new paradigm for dynamic multimodal inference."
          ],
          "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., IEMOCAP, MELD) with constructed adversarial/irrelevant splits to simulate arbitrary modality conditions. Metrics: Accuracy, F1-score. Baselines: Standard Audio-Text LMMs, static fusion models, and existing methods for handling missing/irrelevant modalities. Ablations: Impact of the routing mechanism, impact of the 'Text Interference' feature construction. Robustness Settings: Performance comparison specifically on Adversarial and Irrelevant subsets to validate the framework's ability to handle cross-modality discrepancies.",
          "keywords_en": [
            "Audio-Text Large Models",
            "Text-Dominant Bias",
            "Robust Emotion Recognition",
            "Modality Routing",
            "Adversarial Settings",
            "Cross-Modality Discrepancies",
            "Dynamic Fusion"
          ],
          "keywords_zh": [
            "音频-文本大模型",
            "文本主导偏见",
            "稳健情绪识别",
            "模态路由",
            "对抗设定",
            "跨模态差异",
            "动态融合"
          ],
          "assumptions": {
            "explicit": [
              "Use of faithful, adversarial, and irrelevant settings",
              "Use of energy and temporal evidence for interference detection",
              "Implementation of lightweight gates/adapters",
              "Inference involves selecting between Joint or Audio experts"
            ],
            "inferred": [
              "The base model is a pre-trained Large Multimodal Model (LMM)",
              "Energy refers to acoustic signal energy features",
              "The primary task is Emotion Recognition",
              "'Text Interference' can be effectively modeled by low-level acoustic features"
            ]
          }
        },
        "query": "Keywords: Audio-Text Large Models, Text-Dominant Bias, Robust Emotion Recognition, Modality Routing, Adversarial Settings, Cross-Modality Discrepancies, Dynamic Fusion Task: Robust Multimodal Emotion Recognition. Input: Audio and Text pairs. Output: Emotion labels. Settings: The study evaluates performance under three distinct conditions: Faithful (consistent modalities), Adversarial (conflicting modalities), and Irrelevant (unrelated modalities). The core problem is to mitigate the text-dominant bias by understanding the conditional dependency between audio and text, specifically when text acts as interference rather than evidence. Constraints: efficiency (lightweight gates/adapters), robustness (against text interference), inference speed (dynamic routing), dynamic modality selection 1. Construct 'Text Interference' representations utilizing audio energy and temporal evidence to quantify modality reliability and characterize the dependency structure between audio and text. 2. Train a lightweight gating mechanism or adapter to learn modality weights based on interference levels, inspired by approaches that handle arbitrary modality conditions. 3. Implement a dynamic routing strategy during inference to select between a 'Joint Expert' (Audio+Text) and an 'Audio-only Expert' depending on the reliability score, effectively reframing the fusion process as a conditional decision. 关键词: 音频-文本大模型，文本主导偏见，稳健情绪识别，模态路由，对抗设定，跨模态差异，动态融合"
      },
      {
        "pattern_id": "pattern_8",
        "pattern_name": "Reframing Speech Synthesis Efficiency",
        "score": 0.40278373611114954,
        "brief": {
          "motivation": "Current Audio-Text Large Models suffer from 'Text-Dominant Bias,' prioritizing textual semantics over acoustic cues, which leads to failures in real-world scenarios involving sarcasm or misleading text. Furthermore, addressing this bias often requires computationally expensive fusion mechanisms. Drawing inspiration from efficiency-focused speech synthesis research, there is a critical need for systems that are not only robust against misleading text but also maintain high inference efficiency through lightweight, dynamic architectures.",
          "problem_definition": "Task: Robust and Efficient Multimodal Emotion Recognition. Input: Audio and Text pairs. Output: Emotion labels. Settings: The study evaluates performance under three distinct conditions: Faithful (consistent modalities), Adversarial (conflicting modalities), and Irrelevant (unrelated modalities). The core challenge is to maintain high accuracy in adversarial settings while minimizing computational overhead, similar to latency constraints in speech synthesis.",
          "constraints": [
            "efficiency (lightweight gates/adapters)",
            "robustness (against text interference)",
            "inference speed (dynamic routing)",
            "low latency (real-time applicability)"
          ],
          "technical_plan": "1. Construct 'Text Interference' representations utilizing audio energy and temporal evidence to quantify modality reliability, analogous to residual prediction in synthesis. 2. Train a lightweight gating mechanism or adapter to learn modality weights based on interference levels, ensuring minimal parameter increase. 3. Implement a dynamic routing strategy during inference to select between a 'Joint Expert' (Audio+Text) and an 'Audio-only Expert' depending on the reliability score, optimizing for both accuracy and speed.",
          "expected_contributions": [
            "Identification and analysis of text-dominant bias in Audio-Text LMMs across faithful, adversarial, and irrelevant settings.",
            "Proposal of a modality reliability-based routing framework leveraging energy and temporal evidence, inspired by efficient model refinement techniques.",
            "Demonstration of improved robustness and inference efficiency in emotion recognition by mitigating misleading text through expert selection."
          ],
          "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., IEMOCAP, MELD) with constructed adversarial/irrelevant splits. Metrics: Accuracy, F1-score, Inference Latency, and FLOPs. Baselines: Standard Audio-Text LMMs, static fusion models, and efficient adaptation baselines. Ablations: Impact of the routing mechanism, impact of the Audio-only expert, and analysis of computational cost. Robustness Settings: Performance comparison specifically on Adversarial and Irrelevant subsets.",
          "keywords_en": [
            "Audio-Text Large Models",
            "Text-Dominant Bias",
            "Robust Emotion Recognition",
            "Modality Routing",
            "Adversarial Settings",
            "Efficient Inference",
            "Lightweight Adapters"
          ],
          "keywords_zh": [
            "音频-文本大模型",
            "文本主导偏见",
            "稳健情绪识别",
            "模态路由",
            "对抗设定",
            "高效推理",
            "轻量级适配器"
          ],
          "assumptions": {
            "explicit": [
              "Use of faithful, adversarial, and irrelevant settings",
              "Use of energy and temporal evidence for interference detection",
              "Implementation of lightweight gates/adapters",
              "Inference involves selecting between Joint or Audio experts"
            ],
            "inferred": [
              "The base model is a pre-trained Large Multimodal Model (LMM)",
              "Energy refers to acoustic signal energy features",
              "The primary task is Emotion Recognition",
              "The routing mechanism aims to reduce computational load in addition to improving accuracy"
            ]
          }
        },
        "query": "Keywords: Audio-Text Large Models, Text-Dominant Bias, Robust Emotion Recognition, Modality Routing, Adversarial Settings, Efficient Inference, Lightweight Adapters Task: Robust and Efficient Multimodal Emotion Recognition. Input: Audio and Text pairs. Output: Emotion labels. Settings: The study evaluates performance under three distinct conditions: Faithful (consistent modalities), Adversarial (conflicting modalities), and Irrelevant (unrelated modalities). The core challenge is to maintain high accuracy in adversarial settings while minimizing computational overhead, similar to latency constraints in speech synthesis. Constraints: efficiency (lightweight gates/adapters), robustness (against text interference), inference speed (dynamic routing), low latency (real-time applicability) 1. Construct 'Text Interference' representations utilizing audio energy and temporal evidence to quantify modality reliability, analogous to residual prediction in synthesis. 2. Train a lightweight gating mechanism or adapter to learn modality weights based on interference levels, ensuring minimal parameter increase. 3. Implement a dynamic routing strategy during inference to select between a 'Joint Expert' (Audio+Text) and an 'Audio-only Expert' depending on the reliability score, optimizing for both accuracy and speed. 关键词: 音频-文本大模型，文本主导偏见，稳健情绪识别，模态路由，对抗设定，高效推理，轻量级适配器"
      }
    ],
    "judge": {
      "best_index": 0,
      "rationale": "Candidate 1 (pattern_112) is the most faithful and complete representation of the raw idea. It accurately captures the core problem of 'Text-Dominant Bias' in Audio-Text Large Models and strictly adheres to the three specific settings (faithful, adversarial, irrelevant) mentioned in the input. The technical plan is highly actionable, detailing the construction of 'Text Interference' representations using energy and temporal evidence, and the implementation of a lightweight gating mechanism for dynamic routing between 'Joint' and 'Audio' experts. Unlike Candidate 3, it avoids introducing unrelated concepts like speech synthesis efficiency, and compared to Candidate 2, it frames the problem more concretely as a 'dynamic selection task' and 'modular composition,' which aligns perfectly with the user's description of routing and expert selection. The evaluation plan is robust, specifically targeting the adversarial and irrelevant splits to validate the solution's effectiveness."
    },
    "recall_scores": {
      "0": 0.3256862745098039,
      "1": 0.26970926301555104,
      "2": 0.44637592968221773
    },
    "chosen_index": 2,
    "query_best": "Keywords: Audio-Text Large Models, Text-Dominant Bias, Robust Emotion Recognition, Modality Routing, Adversarial Settings, Efficient Inference, Lightweight Adapters Task: Robust and Efficient Multimodal Emotion Recognition. Input: Audio and Text pairs. Output: Emotion labels. Settings: The study evaluates performance under three distinct conditions: Faithful (consistent modalities), Adversarial (conflicting modalities), and Irrelevant (unrelated modalities). The core challenge is to maintain high accuracy in adversarial settings while minimizing computational overhead, similar to latency constraints in speech synthesis. Constraints: efficiency (lightweight gates/adapters), robustness (against text interference), inference speed (dynamic routing), low latency (real-time applicability) 1. Construct 'Text Interference' representations utilizing audio energy and temporal evidence to quantify modality reliability, analogous to residual prediction in synthesis. 2. Train a lightweight gating mechanism or adapter to learn modality weights based on interference levels, ensuring minimal parameter increase. 3. Implement a dynamic routing strategy during inference to select between a 'Joint Expert' (Audio+Text) and an 'Audio-only Expert' depending on the reliability score, optimizing for both accuracy and speed. 关键词: 音频-文本大模型，文本主导偏见，稳健情绪识别，模态路由，对抗设定，高效推理，轻量级适配器",
    "brief_best": {
      "motivation": "Current Audio-Text Large Models suffer from 'Text-Dominant Bias,' prioritizing textual semantics over acoustic cues, which leads to failures in real-world scenarios involving sarcasm or misleading text. Furthermore, addressing this bias often requires computationally expensive fusion mechanisms. Drawing inspiration from efficiency-focused speech synthesis research, there is a critical need for systems that are not only robust against misleading text but also maintain high inference efficiency through lightweight, dynamic architectures.",
      "problem_definition": "Task: Robust and Efficient Multimodal Emotion Recognition. Input: Audio and Text pairs. Output: Emotion labels. Settings: The study evaluates performance under three distinct conditions: Faithful (consistent modalities), Adversarial (conflicting modalities), and Irrelevant (unrelated modalities). The core challenge is to maintain high accuracy in adversarial settings while minimizing computational overhead, similar to latency constraints in speech synthesis.",
      "constraints": [
        "efficiency (lightweight gates/adapters)",
        "robustness (against text interference)",
        "inference speed (dynamic routing)",
        "low latency (real-time applicability)"
      ],
      "technical_plan": "1. Construct 'Text Interference' representations utilizing audio energy and temporal evidence to quantify modality reliability, analogous to residual prediction in synthesis. 2. Train a lightweight gating mechanism or adapter to learn modality weights based on interference levels, ensuring minimal parameter increase. 3. Implement a dynamic routing strategy during inference to select between a 'Joint Expert' (Audio+Text) and an 'Audio-only Expert' depending on the reliability score, optimizing for both accuracy and speed.",
      "expected_contributions": [
        "Identification and analysis of text-dominant bias in Audio-Text LMMs across faithful, adversarial, and irrelevant settings.",
        "Proposal of a modality reliability-based routing framework leveraging energy and temporal evidence, inspired by efficient model refinement techniques.",
        "Demonstration of improved robustness and inference efficiency in emotion recognition by mitigating misleading text through expert selection."
      ],
      "evaluation_plan": "Datasets: Standard multimodal emotion datasets (e.g., IEMOCAP, MELD) with constructed adversarial/irrelevant splits. Metrics: Accuracy, F1-score, Inference Latency, and FLOPs. Baselines: Standard Audio-Text LMMs, static fusion models, and efficient adaptation baselines. Ablations: Impact of the routing mechanism, impact of the Audio-only expert, and analysis of computational cost. Robustness Settings: Performance comparison specifically on Adversarial and Irrelevant subsets.",
      "keywords_en": [
        "Audio-Text Large Models",
        "Text-Dominant Bias",
        "Robust Emotion Recognition",
        "Modality Routing",
        "Adversarial Settings",
        "Efficient Inference",
        "Lightweight Adapters"
      ],
      "keywords_zh": [
        "音频-文本大模型",
        "文本主导偏见",
        "稳健情绪识别",
        "模态路由",
        "对抗设定",
        "高效推理",
        "轻量级适配器"
      ],
      "assumptions": {
        "explicit": [
          "Use of faithful, adversarial, and irrelevant settings",
          "Use of energy and temporal evidence for interference detection",
          "Implementation of lightweight gates/adapters",
          "Inference involves selecting between Joint or Audio experts"
        ],
        "inferred": [
          "The base model is a pre-trained Large Multimodal Model (LMM)",
          "Energy refers to acoustic signal energy features",
          "The primary task is Emotion Recognition",
          "The routing mechanism aims to reduce computational load in addition to improving accuracy"
        ]
      }
    }
  }
}